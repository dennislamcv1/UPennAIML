{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc9489b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b599cdc4d67ef286c3a84c9d82faf39",
     "grade": false,
     "grade_id": "cell-assignment-1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 2: Neural Network in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2895bf9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1fe0b9ca78c693c567e6cb1a3761d1f1",
     "grade": false,
     "grade_id": "cell-169290abf3c740f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 1 Background and Instructions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1631246",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0199e21f9387e8ce9245411436bc3e8c",
     "grade": false,
     "grade_id": "cell-6764e64724eee683",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Background\n",
    "\n",
    "Neural Networks are a class of algorithms, loosely modeled after human biological neural networks in the brain. They are instrumental in extracting patterns out of the dataset provided. Most of the present day advancements in Artificial Intelligence are made possible by the different variants of neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612a244",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d7add9c8086df49fda9e34d3010da51",
     "grade": false,
     "grade_id": "cell-4a662a8e4dabeb68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment\n",
    "In this assignment, we will develop a Neural Network and write the entire training loop of that Neural Network using a library called Pytorch. Pytorch, released by Meta (also known as Facebook) is perhaps today the most popular package used to develop a neural network. We are going to implement the backpropagation algorithm in Pytorch. Backpropagation was first postulated roughly 40 years ago by Geoffrey Hinton - one of the authority figures in Deep Learning today. He was awarded the Turing Prize for his contributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8849b0c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f5870e379ee42df13811d794e0678e4",
     "grade": false,
     "grade_id": "cell-6cde643839b5d137",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Instructions\n",
    "\n",
    "Complete the starter code methods provided without changing their signatures or return values. \n",
    "\n",
    "### Note:\n",
    "You are expected to write code where you see **your code here**.  \n",
    "Make sure you delete the lines with **raise NotImplementedError** or your code may not run correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4f461",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f71de58398884072f936a4574d95055e",
     "grade": false,
     "grade_id": "cell-212345b5e873db74",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Import Libraries\n",
    "\n",
    "The code cell below contains the basic python packages you will need to complete this assignment. Do not update\n",
    "the python package or add new ones. Doing so will may cause the autograder to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b455da6d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3599aaa94ad1331206d5448dd65ef310",
     "grade": false,
     "grade_id": "cell-0ec972f6dbfa4bf8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Basic libraries for functionalities\n",
    "import numpy as np              #numpy library stands for numerical python - they create a datatype called arrays which speeds up calculation\n",
    "import matplotlib.pyplot as plt #matplotlib is a library used for drawing graphs and plotting\n",
    "import random                   #used to create random datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f177f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e7f511f308a5cf92d0bcecb33a29342",
     "grade": false,
     "grade_id": "cell-3d3b59daba42ad30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## About Pytorch\n",
    "Building a Deep Learning model is a customizable process; for example, someone may want to build a Deep Learning model to extract large features from an image, whereas someone else may only want to extract small features from it. In order to address this, packages were developed for creating Deep Learning models, such as tensorflow, keras and Pytorch. Pytorch is the perhaps the package most widely used by developers. This framework provides Deep Learning model building blocks that developers can assemble for their needs.\n",
    "\n",
    "## PyTorch Blitz\n",
    "Here is a quick introduction to PyTorch(60 minutes) provided by the developers of PyTorch itself. Please watch it if you are new to PyTorch. \n",
    "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "779b77ec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25671756383fc625dfc989272f2514f6",
     "grade": false,
     "grade_id": "cell-3db39efe970ac68d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#More libraries for implementing our neural network\n",
    "import torch                                        #pytorch library \n",
    "from torch.utils.data import DataLoader, Dataset    #importing the dataset and dotaloader class which we will use to input the data\n",
    "import torch.optim as optim                         #optimizer for pytorch\n",
    "import torch.nn as nn                               #neural network module to write the network architecture\n",
    "import torch.nn.functional as F                     #neural network module to write the network architecture\n",
    "import torchvision as thv                           #torch vision to work on images in the dataset\n",
    "from torchvision.datasets import MNIST              #library to import/download the MNIST datset\n",
    "import torchvision.transforms as T                  #torchvision module to do data transformation\n",
    "from copy import deepcopy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce22428",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5489b6b2d7ee72d11a0ccaaf7db10a3e",
     "grade": false,
     "grade_id": "cell-73023a2099b7e873",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 2: Data and it's Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1c970c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef5491e37fa182a1eca26b2e48b2cb62",
     "grade": false,
     "grade_id": "cell-b15342dfc6cde03c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Get Data \n",
    "\n",
    "We provide the dataset in the local MNIST directory. Run the following function `get_data()`to return the training and validation datasets as `train` and `val`. The training images and labels are coupled together and will be called `train` in the `get_data()` method. In the same way, the validation (testing) images and labels are coupled together and will be called `val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a68fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \n",
    "    \"\"\"\n",
    "    return - the training and validation datasets along with their lables. (output type: torchvision dataset)\n",
    "    \"\"\"\n",
    "    train = MNIST('./', download=False, train=True)\n",
    "    val = MNIST('./', download=False, train=False)\n",
    "\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b77dd19",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b9aba3a480b6f045af36358b5b5a286",
     "grade": false,
     "grade_id": "cell-0baaa2712d9270e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mnist_train, mnist_val = get_data()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6225efc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c1a56b1908a76aec0771c5e788f2039",
     "grade": false,
     "grade_id": "cell-37f23030b4333e16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Subsampling of Dataset\n",
    "The training dataset contains 60,000 images, and we will select only 30,000 of them. The testing dataset contains 10,000 images, and we will select 5,000 of them. The function `subsample` takes the input of how many samples we want. \n",
    "\n",
    "Along with subsampling, we will also normalize our dataset by dividing the feature data by 255 (maximum value) because it provides stability while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af17ac65",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5552694dfa738aca89689fd04dc6cd48",
     "grade": false,
     "grade_id": "cell-349ef7c33dcaa008",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def subsample(X, Y, num_samples, num_classes):\n",
    "    \"\"\"\n",
    "    X: Feature set, as a 2D numpy array\n",
    "    Y: Labels, as a 1D numpy array\n",
    "    num_samples: Total number of samples required after subsampling\n",
    "    num_classes: Number of unique classes in the dataset\n",
    "    \n",
    "    Return: subsampled_X, subsampled_Y subsampled dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "    ### Uncomment each line of code and complete as needed\n",
    "\n",
    "    # Flatten X and change type\n",
    "    X = X.reshape(X.shape[0], -1).astype(np.float32)                         #use the reshape and the shape function to flatten X data          \n",
    "    \n",
    "    # Normalize X\n",
    "    X = X / 255.0                                                               #Normalize with the value of 255\n",
    "    # Calculate the final size of each class\n",
    "    class_size = num_samples // num_classes                                         #Each class size is given by dividing the total samples by the number of classes\n",
    "    indices = []\n",
    "\n",
    "    for label in range(num_classes):  \n",
    "        label_indices = np.argwhere(Y == label)[:class_size].flatten()           # Find indices for the given label and select the first class_size elements           \n",
    "        indices.extend(label_indices)                                                   # Keep on extending the list based upon label indices\n",
    "    indices = np.array(indices)\n",
    "    subsampled_X = X[indices]                                                    # Get the subsampled values\n",
    "    subsampled_Y = Y[indices]                                                    # Get the subsampled values\n",
    "\n",
    "    return subsampled_X, subsampled_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b7bd0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a51c2e7e07c142e5bc7c5c99e404e4e",
     "grade": false,
     "grade_id": "cell-9044c3985e8af55d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Calling the subsample two times \n",
    "Notice that we have called the `subsample` function twice, once to create the train dataset and once to create the validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "209216cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f210a9d4be6d4f76d2591f59b43dd195",
     "grade": false,
     "grade_id": "cell-7522d4207e3a4d0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "trainX, trainY = subsample(X=mnist_train.data.numpy(), Y=mnist_train.targets.numpy(), num_samples=30000, num_classes=10)\n",
    "valX, valY = subsample(X=mnist_val.data.numpy(), Y=mnist_val.targets.numpy(), num_samples=5000, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b916efa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61d8614a257fce122c6cd55c4f892a93",
     "grade": true,
     "grade_id": "cell-e810e00d59b4594d",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape Test Passed for training data\n",
      "Output Shape Test Passed for Validation data\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "\n",
    "def test_subsample_output_shape(X, Y, num_samples, num_classes):\n",
    "\n",
    "    subsampled_X, subsampled_Y = subsample(X, Y, num_samples, num_classes)\n",
    "\n",
    "    assert subsampled_X.shape[0] == num_samples, f\"Expected {num_samples} samples, but got {subsampled_X.shape[0]}\"\n",
    "    assert len(subsampled_Y) == num_samples, f\"Expected {num_samples} labels, but got {len(subsampled_Y)}\"\n",
    "    \n",
    "test_subsample_output_shape(X=mnist_train.data.numpy(), Y=mnist_train.targets.numpy(), num_samples=30000, num_classes=10)\n",
    "print(\"Output Shape Test Passed for training data\")\n",
    "\n",
    "test_subsample_output_shape(X=mnist_val.data.numpy(), Y=mnist_val.targets.numpy(), num_samples=5000, num_classes=10)\n",
    "print(\"Output Shape Test Passed for Validation data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e130545e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94a7d81200515299a7350bc2e7c2c518",
     "grade": true,
     "grade_id": "cell-17d2e9d8c63ab8fc",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsample Class Distribution Passed for training data\n",
      "Subsample Class Distribution Passed for Validation data\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "def test_subsample_class_distribution(X, Y, num_samples, num_classes):\n",
    "\n",
    "    subsampled_X, subsampled_Y = subsample(X, Y, num_samples, num_classes)\n",
    "    \n",
    "    unique, counts = np.unique(subsampled_Y, return_counts=True)\n",
    "    expected_class_size = num_samples // num_classes\n",
    "\n",
    "    for i, label in enumerate(range(num_classes)):\n",
    "        assert counts[i] == expected_class_size, (\n",
    "            f\"Expected {expected_class_size} samples for class {label}, \"\n",
    "            f\"but got {counts[i]}\")\n",
    "        \n",
    "test_subsample_output_shape(X=mnist_train.data.numpy(), Y=mnist_train.targets.numpy(), num_samples=30000, num_classes=10)\n",
    "print(\"Subsample Class Distribution Passed for training data\")\n",
    "\n",
    "test_subsample_output_shape(X=mnist_val.data.numpy(), Y=mnist_val.targets.numpy(), num_samples=5000, num_classes=10)\n",
    "print(\"Subsample Class Distribution Passed for Validation data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c5f7e92",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "551b9161ec4b4f2860b68a01d372e41c",
     "grade": true,
     "grade_id": "cell-2a08077e310bfdc1",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization test is passed for training data\n",
      "Normalizaiton test passed for testing data\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "def test_subsample_normalization(X, Y, num_samples, num_classes):\n",
    "\n",
    "    subsampled_X, _ = subsample(X, Y, num_samples, num_classes)\n",
    "    \n",
    "    assert np.all(subsampled_X <= 1.0) and np.all(subsampled_X >= 0.0), (\n",
    "        \"Normalization check failed: Features are not in the range [0, 1]\")\n",
    "    \n",
    "test_subsample_output_shape(X=mnist_train.data.numpy(), Y=mnist_train.targets.numpy(), num_samples=30000, num_classes=10)\n",
    "print(\"Normalization test is passed for training data\")\n",
    "\n",
    "test_subsample_output_shape(X=mnist_val.data.numpy(), Y=mnist_val.targets.numpy(), num_samples=5000, num_classes=10)\n",
    "print(\"Normalizaiton test passed for testing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e823a70c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2dd6584df718d96ba52e447ec4544ae",
     "grade": false,
     "grade_id": "cell-4b046a7ceec35bcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## About MNIST dataset\n",
    "In this assignment, we are going to work with the MNIST digit dataset. It’s a classic dataset with images of handwritten digits from 0 to 9. It's great for testing and teaching image recognition. Each image is labeled with the correct digit, so it’s a great starting point for experimenting with how well models can recognize and classify numbers. Here we are going to apply this on the neural network we are going to develop using PyTorch. \n",
    "\n",
    "We will create a custom dataset using the `Dataset` class in PyTorch. It helps us create the dataset which can later be used with the the `DataLoader` class of PyTorch to train and test our models. In Machine Learning (ML) and Deep Learning (DL), the majority of the time is spent working on preparing the data to be used by the model. That involves cleaning the dataset, organizing it, removing missing items, performing some transforms and many other activities. In order to make that job simpler, PyTorch introduced the Dataset class for our purpose - which helps us in creating and transforming our dataset in a customizable way. After the dataset has been created, the next task is to provide the model with small batches of data from the dataset - here in comes the use of DataLoader class which helps us in doing this action quite effectively. \n",
    "\n",
    "These elements are introduced in the PyTorch Blitz video linked earlier in the notebook; however, for more in-depth reading, see these two tutorials: \n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/basics/data_tutorial.html. \n",
    "\n",
    "* https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4 - In this playlist, see video 09."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d169dccd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1c9e6a7420f613d9142c87ee3698f00",
     "grade": false,
     "grade_id": "cell-5036eb0d8be2c50d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "    -----------\n",
    "    x_data : numpy-array of subsampled Image Input\n",
    "    y_data : numpy-array of subsampled Labels Input\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    __len__():Returns the number of samples.\n",
    "\n",
    "    __getitem__(idx):Retrieves the image and label at the given index (Tensor).\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    ### Uncomment each line of code and complete as needed\n",
    "    \n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = torch.tensor(x_data, dtype=torch.float32)       # In the constructor define the images and the labels\n",
    "        self.y_data = torch.tensor(y_data, dtype=torch.long) \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)                                              #The length of the total dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x_data[idx]                                              #This would ensure the retrieval of new datapoints  \n",
    "        y = self.y_data[idx]\n",
    "        return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38d17f45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d474260688fd369eda66898fcceba3e",
     "grade": false,
     "grade_id": "cell-1f1ffb83f58b0369",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = MNISTDataset(trainX, trainY)  \n",
    "val_dataset = MNISTDataset(valX, valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cf285b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d6261c0eb5a3742e050c55253668992",
     "grade": true,
     "grade_id": "cell-b2a00ba7ccb8eef8",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the Train dataset is correct\n",
      "The length of the Val dataset is correct\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "\n",
    "def test_length(x_data, y_data, expected_length):\n",
    "    dataset = MNISTDataset(x_data, y_data)\n",
    "\n",
    "    dataset_length = len(dataset)\n",
    "    assert dataset_length == expected_length, f\"Expected dataset length {expected_length}, but got {dataset_length}\"\n",
    "    \n",
    "test_length(x_data= trainX, y_data=trainY, expected_length = 30000 )\n",
    "print(\"The length of the Train dataset is correct\")\n",
    "\n",
    "test_length(x_data= valX, y_data=valY, expected_length = 5000 )\n",
    "print(\"The length of the Val dataset is correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c9171d1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b20c2fdf3accc07ed953a4fe86478513",
     "grade": true,
     "grade_id": "cell-52a5c522fc0c934b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Retrieval of Train dataset is Passed\n",
      "Data Retrieval of Val dataset is Passed\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "def test_getitem(x_data, y_data):\n",
    "    dataset = MNISTDataset(x_data, y_data)\n",
    "\n",
    "    idx = 1\n",
    "    expected_x = x_data[idx]\n",
    "    expected_y = y_data[idx]\n",
    "    x, y = dataset[idx]\n",
    "\n",
    "    assert np.array_equal(x, expected_x), f\"Expected x at index {idx} to be {expected_x}, but got {x}\"\n",
    "    assert y == expected_y, f\"Expected y at index {idx} to be {expected_y}, but got {y}\"\n",
    "    \n",
    "    \n",
    "test_getitem(x_data = trainX, y_data=trainY)\n",
    "print(\"Data Retrieval of Train dataset is Passed\")\n",
    "test_getitem(x_data = valX, y_data=valY)\n",
    "print(\"Data Retrieval of Val dataset is Passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9de48e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae16d29c837ddb738629b2b5d783bc9c",
     "grade": false,
     "grade_id": "cell-21ed73ac4c4458b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 3 Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b884ccbb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d424000daeeb90791a4fd7c321e6be03",
     "grade": false,
     "grade_id": "cell-8ebe6308c7af7391",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Define Neural Network in Pytorch\n",
    "Neural networks in PyTorch are created using the `torch.nn` module. The `__init__` method specifies which layers are present in our network. You will notice that in the `__init__` method the linear layer has parameters 784 and 10. This is because we are taking our input dataset, composed of MNIST images of the size 28X28, and converting them to a linear vector of 784. The value 10 represents the number of possible outputs of the MNIST dataset. \n",
    "\n",
    "\n",
    "The `forward` method determines how the data will flow through the network. It shows which elements are applied upon the input data `x` and in what order to perform the forward pass of the datset. In the forward method we are applying `ReLU` upon our dataset after the fully connected layer `fc1`. This is the application of the activation function on our data to bring non-linearity into our model. Non-Linearity is at the heart of what makes neural networks capable of extracting features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ba25c3e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b006c84b8e19514959b2f1111533b51",
     "grade": false,
     "grade_id": "cell-d04445af0a20d126",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module): \n",
    "    \"\"\"\n",
    "    A neural network with a single fully connected layer.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    fc1 :Fully connected layer with input size 784 (28X28) and output size 10 (number of samples).\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    forward(x):Applies the forward pass using ReLU activation on the fully connected layer.\n",
    "    \n",
    "    Output:\n",
    "    --------\n",
    "    out: output of the application of the simple neural network on the dataset\n",
    "    \"\"\" \n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    ### Uncomment each line of code and get the answer\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 10)         #Define the sizes of the layer of the NN corresponding to the dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))           # write out how the data flows through the model\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da1b6a0d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df321224f21ed9bbf08851e9bdd1dc5f",
     "grade": false,
     "grade_id": "cell-eafc45687fca3252",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "net = Net() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "330e249a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa6d5a1d045e80de4688d5bfe0aef2f5",
     "grade": true,
     "grade_id": "cell-5fdc86c5be80b511",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization test passed.\n",
      "The Model is correctly initialized\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "def test_initialization(net):\n",
    "    assert isinstance(net.fc1, nn.Linear), \"fc1 should be an instance of nn.Linear\"\n",
    "    assert net.fc1.in_features == 784, \"fc1 input features should be 784\"\n",
    "    assert net.fc1.out_features == 10, \"fc1 output features should be 10\"\n",
    "    print(\"Initialization test passed.\")\n",
    "    \n",
    "test_initialization(net)\n",
    "print(\"The Model is correctly initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fdf3402",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05c260246ad991621819e37342d53007",
     "grade": true,
     "grade_id": "cell-372f27cd8dc6ab68",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass test passed.\n",
      "Forward pass is correctly initialized\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "def test_forward_pass(net):\n",
    "    dummy_input = torch.randn(1, 784)\n",
    "    output = net(dummy_input)\n",
    "    assert output.shape == (1, 10), \"Output shape should be (1, 10)\"\n",
    "    assert output.dtype == torch.float32, \"Output dtype should be float32\"\n",
    "    print(\"Forward pass test passed.\")\n",
    "    \n",
    "    \n",
    "test_forward_pass(net)\n",
    "print(\"Forward pass is correctly initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195451af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b390a1a7edcb3c0b2554433823431fd",
     "grade": false,
     "grade_id": "cell-727132540cd416ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 4 Training and Evaluation of the Neural Network\n",
    "After defining our dataset and our model above, we train and evaluate our model. If you look at the code in this section, you will notice that the validation method, `validate_pytorch_NN`, is defined first and then the training loop, `train_pytorch_nn`. This has been done purposefully, because we are calling the validation function from the training loop. A recommendation is that you start coding from the training loop and then circle back to validation loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa77a94",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "668aeacd4edb0e6dd3019635a73141e9",
     "grade": false,
     "grade_id": "cell-d957321df715a3b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Validation of Pytorch Neural Network\n",
    "Here we have written the `validate_pytorch_NN` method. It takes the trained model, implements it on the testing (validation) dataset, calculates the running loss, and finally outputs the average validation loss and average validation error. Here the key metric is the running loss. During the training and the evaluation (validation) of a neural network, it is run iteratively. The performance is tracked at each iteration, and the record keeping of that is called running loss. We will later plot the loss and the accuracy curves of the model. Those curves are characteristic of the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "953fc99f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae8ed7b269c68cebaf033d5f4670d4e6",
     "grade": false,
     "grade_id": "cell-0ff4deb962733383",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def validate_pytorch_NN(net, val_dataloader):\n",
    "    \"\"\"\n",
    "    Validate the PyTorch neural network with the given validation dataloader.\n",
    "    \n",
    "    net: The neural network model\n",
    "    criterion: The loss function\n",
    "    val_dataloader: DataLoader for the validation data\n",
    "    \n",
    "    Return: average validation loss and error\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "    ### Uncomment each line of code and complete as needed\n",
    "\n",
    "    # Initialize total validation loss and error\n",
    "    val_loss, val_error = 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Iterate over all mini-batches from the validation dataset\n",
    "    for x_val, y_val in val_dataloader:\n",
    "        x_val = torch.Tensor(x_val)                      # Convert the data to tensor\n",
    "        y_val = torch.Tensor(y_val).long()                     # Convert the data to tensor\n",
    "    #     \n",
    "    #     # Compute forward pass and error\n",
    "        with torch.no_grad():\n",
    "            val_out = net(x_val)  # Get model predictions\n",
    "            val_loss += criterion(val_out, y_val).item()  # Compute running loss\n",
    "            \n",
    "            _, val_pred = torch.max(val_out, 1)  # Get predicted class\n",
    "            val_error += np.sum(np.array(val_pred.cpu()) != y_val.cpu().numpy()) / y_val.shape[0]  # Compute running error\n",
    "    # \n",
    "    # # Compute average validation loss and error\n",
    "    avg_val_loss = val_loss / len(val_dataloader)  # Average loss\n",
    "    avg_val_error = val_error / len(val_dataloader)  # Average error\n",
    "    # \n",
    "    return avg_val_loss, avg_val_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b30d88e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66f543e524fb95b154ff78cd8d74c3e0",
     "grade": false,
     "grade_id": "cell-391b0c7230ed68c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Train Pytorch Neural Network\n",
    "\n",
    "The training loop in PyTorch teaches your model to make better predictions. Start by passing your data through the model to get predictions. Then calculate how far off these predictions are from the actual values using a loss function. Next, update the model’s parameters to reduce this error, and repeat this process many times. Essentially, the training loop helps the model learn from its mistakes and improve over time.\n",
    "\n",
    "### Steps of the Training Loop\n",
    "1. Perform the forward pass on the data where the `model` is applied on the training dataset to generate an ouptut.\n",
    "2. Perform the backpropagation where the gradients are set to zero using `optimizer.zero_grad()` and calculate the backward loss using `loss.backward()`\n",
    "3. Update the model parameters using the `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4917fbb2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e86c4f62b7eaf23a606bd70ba84d9e83",
     "grade": false,
     "grade_id": "cell-572bc733a3053ecd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_pytorch_nn(net, train_dataloader, val_dataloader):\n",
    "    \"\"\"\n",
    "    Train a PyTorch neural network with the given parameters.\n",
    "    \n",
    "    train_dataloader: DataLoader for the training data\n",
    "    val_dataloader: DataLoader for the validation data\n",
    "    lr: Learning rate for the optimizer\n",
    "    \n",
    "    Return: Lists of training and validation losses and errors\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "    ### Uncomment each line of code and complete as needed\n",
    "\n",
    "    # Setup (e.g., seed initialization, if any) - Do not change these values\n",
    "    torch.manual_seed(20)\n",
    "    random.seed(20)\n",
    "    np.random.seed(20)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    train_error_list = []\n",
    "    train_loss_list = []\n",
    "    val_error_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "#     # Calculate the total number of batches needed\n",
    "    num_iterations = 3050\n",
    "    iteration = 0\n",
    "\n",
    "    while iteration < num_iterations:  # Loop until we reach 30,000 iterations\n",
    "        for x, y in train_dataloader:\n",
    "            if iteration >= num_iterations:\n",
    "                break  # Stop if max iterations reached\n",
    "\n",
    "            x = x  # Training input\n",
    "            y = y  # Training labels\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = net(x)  # Apply model on input data\n",
    "            loss = criterion(outputs, y)  # Compute running loss\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class predictions\n",
    "            err = np.sum(predicted.cpu().numpy() != y.cpu().numpy()) / x.shape[0]  # Compute running error\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()  # Zero out gradients\n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "            # Logging\n",
    "            print(f\"Iteration [{iteration}], \"\n",
    "                  f\"Loss: {loss.item():.4f}, \"\n",
    "                  f\"Error: {err:.4f}\")\n",
    "\n",
    "            train_loss_list.append(loss.item())  # Log loss\n",
    "            train_error_list.append(err)  # Log error\n",
    "\n",
    "            # Validation step\n",
    "            if iteration % 1000 == 0:  # Validate every 1000 iterations\n",
    "                val_loss, val_err = validate_pytorch_NN(net, val_dataloader)\n",
    "                val_loss_list.append(val_loss)  # Log validation loss\n",
    "                val_error_list.append(val_err)  # Log validation error\n",
    "\n",
    "            iteration += 1  # Increment iteration\n",
    "\n",
    "    return train_loss_list, train_error_list, val_loss_list, val_error_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e33ea92c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6af5d983cf765ecf0d7d7cf313b6e0b2",
     "grade": false,
     "grade_id": "cell-d53ca14d3be40490",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = MNISTDataset(trainX, trainY) \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = MNISTDataset(valX, valY)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3522b4ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dec6b1b1ecd137e26817b4f6c71253af",
     "grade": false,
     "grade_id": "cell-7db9ffa85c87e271",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [0], Loss: 2.2953, Error: 0.8750\n",
      "Iteration [1], Loss: 2.2567, Error: 0.8125\n",
      "Iteration [2], Loss: 2.1947, Error: 0.7656\n",
      "Iteration [3], Loss: 2.1371, Error: 0.6719\n",
      "Iteration [4], Loss: 2.0197, Error: 0.5625\n",
      "Iteration [5], Loss: 2.0136, Error: 0.6406\n",
      "Iteration [6], Loss: 2.0783, Error: 0.7500\n",
      "Iteration [7], Loss: 2.0218, Error: 0.5625\n",
      "Iteration [8], Loss: 1.8402, Error: 0.4531\n",
      "Iteration [9], Loss: 2.0089, Error: 0.6406\n",
      "Iteration [10], Loss: 1.8555, Error: 0.5000\n",
      "Iteration [11], Loss: 1.7814, Error: 0.4844\n",
      "Iteration [12], Loss: 1.6638, Error: 0.4062\n",
      "Iteration [13], Loss: 1.7884, Error: 0.4688\n",
      "Iteration [14], Loss: 1.6407, Error: 0.3125\n",
      "Iteration [15], Loss: 1.8088, Error: 0.4688\n",
      "Iteration [16], Loss: 1.5031, Error: 0.2812\n",
      "Iteration [17], Loss: 1.8254, Error: 0.5312\n",
      "Iteration [18], Loss: 1.6637, Error: 0.4688\n",
      "Iteration [19], Loss: 1.4677, Error: 0.2812\n",
      "Iteration [20], Loss: 1.4207, Error: 0.2500\n",
      "Iteration [21], Loss: 1.5269, Error: 0.3750\n",
      "Iteration [22], Loss: 1.5246, Error: 0.3594\n",
      "Iteration [23], Loss: 1.4427, Error: 0.3438\n",
      "Iteration [24], Loss: 1.5101, Error: 0.3125\n",
      "Iteration [25], Loss: 1.3897, Error: 0.2500\n",
      "Iteration [26], Loss: 1.2971, Error: 0.2656\n",
      "Iteration [27], Loss: 1.3542, Error: 0.2812\n",
      "Iteration [28], Loss: 1.4009, Error: 0.3594\n",
      "Iteration [29], Loss: 1.1834, Error: 0.1719\n",
      "Iteration [30], Loss: 1.3107, Error: 0.2812\n",
      "Iteration [31], Loss: 1.1836, Error: 0.2344\n",
      "Iteration [32], Loss: 1.1480, Error: 0.2500\n",
      "Iteration [33], Loss: 1.1590, Error: 0.2188\n",
      "Iteration [34], Loss: 1.2723, Error: 0.2812\n",
      "Iteration [35], Loss: 1.1714, Error: 0.2812\n",
      "Iteration [36], Loss: 1.0857, Error: 0.2031\n",
      "Iteration [37], Loss: 1.1443, Error: 0.2656\n",
      "Iteration [38], Loss: 1.2336, Error: 0.2656\n",
      "Iteration [39], Loss: 1.0318, Error: 0.2188\n",
      "Iteration [40], Loss: 0.9565, Error: 0.1875\n",
      "Iteration [41], Loss: 1.1627, Error: 0.2656\n",
      "Iteration [42], Loss: 1.1323, Error: 0.2500\n",
      "Iteration [43], Loss: 1.3827, Error: 0.3125\n",
      "Iteration [44], Loss: 1.1349, Error: 0.2969\n",
      "Iteration [45], Loss: 1.1856, Error: 0.2656\n",
      "Iteration [46], Loss: 1.0297, Error: 0.2031\n",
      "Iteration [47], Loss: 1.0088, Error: 0.2344\n",
      "Iteration [48], Loss: 1.0940, Error: 0.2344\n",
      "Iteration [49], Loss: 1.1576, Error: 0.2656\n",
      "Iteration [50], Loss: 1.0183, Error: 0.2188\n",
      "Iteration [51], Loss: 0.8941, Error: 0.1562\n",
      "Iteration [52], Loss: 0.9863, Error: 0.2656\n",
      "Iteration [53], Loss: 1.0426, Error: 0.2656\n",
      "Iteration [54], Loss: 1.2589, Error: 0.3281\n",
      "Iteration [55], Loss: 1.1547, Error: 0.2812\n",
      "Iteration [56], Loss: 0.9846, Error: 0.2500\n",
      "Iteration [57], Loss: 0.9064, Error: 0.2031\n",
      "Iteration [58], Loss: 1.2003, Error: 0.2969\n",
      "Iteration [59], Loss: 1.1317, Error: 0.2969\n",
      "Iteration [60], Loss: 1.0634, Error: 0.2812\n",
      "Iteration [61], Loss: 0.8379, Error: 0.1094\n",
      "Iteration [62], Loss: 0.9926, Error: 0.2344\n",
      "Iteration [63], Loss: 0.9595, Error: 0.2656\n",
      "Iteration [64], Loss: 0.8293, Error: 0.1406\n",
      "Iteration [65], Loss: 0.9592, Error: 0.2188\n",
      "Iteration [66], Loss: 0.8516, Error: 0.2031\n",
      "Iteration [67], Loss: 1.1138, Error: 0.2969\n",
      "Iteration [68], Loss: 0.9004, Error: 0.1719\n",
      "Iteration [69], Loss: 0.9163, Error: 0.2031\n",
      "Iteration [70], Loss: 0.9777, Error: 0.2344\n",
      "Iteration [71], Loss: 0.7307, Error: 0.1562\n",
      "Iteration [72], Loss: 0.7725, Error: 0.1562\n",
      "Iteration [73], Loss: 1.0657, Error: 0.3438\n",
      "Iteration [74], Loss: 0.8155, Error: 0.1719\n",
      "Iteration [75], Loss: 0.9429, Error: 0.2812\n",
      "Iteration [76], Loss: 0.8257, Error: 0.2188\n",
      "Iteration [77], Loss: 0.8180, Error: 0.2031\n",
      "Iteration [78], Loss: 0.9180, Error: 0.2344\n",
      "Iteration [79], Loss: 1.0342, Error: 0.2500\n",
      "Iteration [80], Loss: 0.8117, Error: 0.2031\n",
      "Iteration [81], Loss: 0.8388, Error: 0.1719\n",
      "Iteration [82], Loss: 1.0160, Error: 0.2656\n",
      "Iteration [83], Loss: 0.8432, Error: 0.1875\n",
      "Iteration [84], Loss: 0.8679, Error: 0.2344\n",
      "Iteration [85], Loss: 1.0186, Error: 0.2812\n",
      "Iteration [86], Loss: 0.8307, Error: 0.1562\n",
      "Iteration [87], Loss: 0.8664, Error: 0.1875\n",
      "Iteration [88], Loss: 0.6630, Error: 0.1250\n",
      "Iteration [89], Loss: 0.8835, Error: 0.1719\n",
      "Iteration [90], Loss: 0.7202, Error: 0.1719\n",
      "Iteration [91], Loss: 1.1294, Error: 0.3438\n",
      "Iteration [92], Loss: 1.1730, Error: 0.3594\n",
      "Iteration [93], Loss: 1.1290, Error: 0.3594\n",
      "Iteration [94], Loss: 0.9075, Error: 0.2188\n",
      "Iteration [95], Loss: 0.9106, Error: 0.2656\n",
      "Iteration [96], Loss: 0.9138, Error: 0.2500\n",
      "Iteration [97], Loss: 1.0641, Error: 0.2188\n",
      "Iteration [98], Loss: 0.8823, Error: 0.2500\n",
      "Iteration [99], Loss: 0.9135, Error: 0.2500\n",
      "Iteration [100], Loss: 0.9181, Error: 0.2031\n",
      "Iteration [101], Loss: 0.8097, Error: 0.1875\n",
      "Iteration [102], Loss: 0.9241, Error: 0.2656\n",
      "Iteration [103], Loss: 0.7265, Error: 0.1406\n",
      "Iteration [104], Loss: 0.7391, Error: 0.1719\n",
      "Iteration [105], Loss: 0.9787, Error: 0.3125\n",
      "Iteration [106], Loss: 0.9642, Error: 0.2344\n",
      "Iteration [107], Loss: 0.9212, Error: 0.2812\n",
      "Iteration [108], Loss: 0.9789, Error: 0.2812\n",
      "Iteration [109], Loss: 0.9597, Error: 0.2500\n",
      "Iteration [110], Loss: 0.8874, Error: 0.2500\n",
      "Iteration [111], Loss: 0.5205, Error: 0.0938\n",
      "Iteration [112], Loss: 0.9960, Error: 0.2812\n",
      "Iteration [113], Loss: 0.7904, Error: 0.2500\n",
      "Iteration [114], Loss: 0.9109, Error: 0.2656\n",
      "Iteration [115], Loss: 0.9393, Error: 0.2812\n",
      "Iteration [116], Loss: 0.7787, Error: 0.2188\n",
      "Iteration [117], Loss: 0.7879, Error: 0.2031\n",
      "Iteration [118], Loss: 0.6290, Error: 0.1250\n",
      "Iteration [119], Loss: 0.9010, Error: 0.2188\n",
      "Iteration [120], Loss: 1.0332, Error: 0.2969\n",
      "Iteration [121], Loss: 0.5806, Error: 0.1094\n",
      "Iteration [122], Loss: 0.8588, Error: 0.2188\n",
      "Iteration [123], Loss: 0.8853, Error: 0.2500\n",
      "Iteration [124], Loss: 0.7475, Error: 0.2188\n",
      "Iteration [125], Loss: 0.8758, Error: 0.2188\n",
      "Iteration [126], Loss: 0.8405, Error: 0.2344\n",
      "Iteration [127], Loss: 0.6659, Error: 0.1719\n",
      "Iteration [128], Loss: 0.9352, Error: 0.2344\n",
      "Iteration [129], Loss: 1.0085, Error: 0.2969\n",
      "Iteration [130], Loss: 0.7141, Error: 0.1719\n",
      "Iteration [131], Loss: 0.8751, Error: 0.2344\n",
      "Iteration [132], Loss: 0.7899, Error: 0.1875\n",
      "Iteration [133], Loss: 0.8466, Error: 0.2344\n",
      "Iteration [134], Loss: 0.6629, Error: 0.1406\n",
      "Iteration [135], Loss: 0.6199, Error: 0.1562\n",
      "Iteration [136], Loss: 0.6210, Error: 0.1094\n",
      "Iteration [137], Loss: 0.9530, Error: 0.2656\n",
      "Iteration [138], Loss: 0.8801, Error: 0.2188\n",
      "Iteration [139], Loss: 0.6591, Error: 0.1094\n",
      "Iteration [140], Loss: 0.9302, Error: 0.2500\n",
      "Iteration [141], Loss: 0.8935, Error: 0.2188\n",
      "Iteration [142], Loss: 0.9562, Error: 0.2656\n",
      "Iteration [143], Loss: 0.6577, Error: 0.1250\n",
      "Iteration [144], Loss: 0.7174, Error: 0.1719\n",
      "Iteration [145], Loss: 0.7189, Error: 0.1875\n",
      "Iteration [146], Loss: 0.8303, Error: 0.2344\n",
      "Iteration [147], Loss: 0.7063, Error: 0.2500\n",
      "Iteration [148], Loss: 0.8905, Error: 0.2344\n",
      "Iteration [149], Loss: 0.6559, Error: 0.1250\n",
      "Iteration [150], Loss: 0.7719, Error: 0.2188\n",
      "Iteration [151], Loss: 0.6257, Error: 0.1406\n",
      "Iteration [152], Loss: 0.7975, Error: 0.1875\n",
      "Iteration [153], Loss: 0.6919, Error: 0.1562\n",
      "Iteration [154], Loss: 1.0545, Error: 0.2969\n",
      "Iteration [155], Loss: 0.9023, Error: 0.2500\n",
      "Iteration [156], Loss: 0.7697, Error: 0.2031\n",
      "Iteration [157], Loss: 0.7705, Error: 0.1719\n",
      "Iteration [158], Loss: 1.0411, Error: 0.2812\n",
      "Iteration [159], Loss: 0.7873, Error: 0.2188\n",
      "Iteration [160], Loss: 0.7408, Error: 0.1719\n",
      "Iteration [161], Loss: 0.6574, Error: 0.1094\n",
      "Iteration [162], Loss: 0.7869, Error: 0.2344\n",
      "Iteration [163], Loss: 0.6153, Error: 0.1406\n",
      "Iteration [164], Loss: 0.6092, Error: 0.1094\n",
      "Iteration [165], Loss: 0.7999, Error: 0.2500\n",
      "Iteration [166], Loss: 0.6533, Error: 0.1875\n",
      "Iteration [167], Loss: 0.6739, Error: 0.2031\n",
      "Iteration [168], Loss: 0.9273, Error: 0.2969\n",
      "Iteration [169], Loss: 0.8909, Error: 0.2656\n",
      "Iteration [170], Loss: 0.7060, Error: 0.1719\n",
      "Iteration [171], Loss: 0.8633, Error: 0.2656\n",
      "Iteration [172], Loss: 0.5620, Error: 0.1250\n",
      "Iteration [173], Loss: 0.9499, Error: 0.2969\n",
      "Iteration [174], Loss: 0.6231, Error: 0.2031\n",
      "Iteration [175], Loss: 0.7414, Error: 0.2344\n",
      "Iteration [176], Loss: 0.8312, Error: 0.2812\n",
      "Iteration [177], Loss: 0.6004, Error: 0.1875\n",
      "Iteration [178], Loss: 0.7554, Error: 0.2344\n",
      "Iteration [179], Loss: 0.5965, Error: 0.1406\n",
      "Iteration [180], Loss: 0.6476, Error: 0.1719\n",
      "Iteration [181], Loss: 0.6328, Error: 0.1719\n",
      "Iteration [182], Loss: 0.8157, Error: 0.2344\n",
      "Iteration [183], Loss: 0.8418, Error: 0.2500\n",
      "Iteration [184], Loss: 0.7525, Error: 0.2188\n",
      "Iteration [185], Loss: 0.7329, Error: 0.1875\n",
      "Iteration [186], Loss: 0.8285, Error: 0.2344\n",
      "Iteration [187], Loss: 0.6433, Error: 0.1562\n",
      "Iteration [188], Loss: 0.8250, Error: 0.2188\n",
      "Iteration [189], Loss: 0.7548, Error: 0.1875\n",
      "Iteration [190], Loss: 0.6218, Error: 0.1875\n",
      "Iteration [191], Loss: 0.7709, Error: 0.1719\n",
      "Iteration [192], Loss: 0.9348, Error: 0.2812\n",
      "Iteration [193], Loss: 0.6163, Error: 0.1719\n",
      "Iteration [194], Loss: 1.1288, Error: 0.3594\n",
      "Iteration [195], Loss: 0.6457, Error: 0.1562\n",
      "Iteration [196], Loss: 0.6331, Error: 0.1719\n",
      "Iteration [197], Loss: 0.8180, Error: 0.2188\n",
      "Iteration [198], Loss: 0.6385, Error: 0.1719\n",
      "Iteration [199], Loss: 0.4931, Error: 0.1094\n",
      "Iteration [200], Loss: 0.7156, Error: 0.1875\n",
      "Iteration [201], Loss: 0.7512, Error: 0.2188\n",
      "Iteration [202], Loss: 0.6885, Error: 0.1875\n",
      "Iteration [203], Loss: 0.7360, Error: 0.2188\n",
      "Iteration [204], Loss: 0.7020, Error: 0.2031\n",
      "Iteration [205], Loss: 0.7584, Error: 0.2188\n",
      "Iteration [206], Loss: 0.6947, Error: 0.2031\n",
      "Iteration [207], Loss: 0.5818, Error: 0.1875\n",
      "Iteration [208], Loss: 0.8585, Error: 0.2188\n",
      "Iteration [209], Loss: 0.7387, Error: 0.2344\n",
      "Iteration [210], Loss: 0.7285, Error: 0.2188\n",
      "Iteration [211], Loss: 0.8412, Error: 0.2812\n",
      "Iteration [212], Loss: 0.5619, Error: 0.1406\n",
      "Iteration [213], Loss: 0.8861, Error: 0.2500\n",
      "Iteration [214], Loss: 0.6807, Error: 0.1875\n",
      "Iteration [215], Loss: 0.5894, Error: 0.1406\n",
      "Iteration [216], Loss: 0.6777, Error: 0.1719\n",
      "Iteration [217], Loss: 0.6983, Error: 0.2031\n",
      "Iteration [218], Loss: 0.8714, Error: 0.2969\n",
      "Iteration [219], Loss: 0.7109, Error: 0.2031\n",
      "Iteration [220], Loss: 0.7297, Error: 0.2031\n",
      "Iteration [221], Loss: 0.6678, Error: 0.1562\n",
      "Iteration [222], Loss: 0.6763, Error: 0.2031\n",
      "Iteration [223], Loss: 0.6564, Error: 0.1875\n",
      "Iteration [224], Loss: 0.6944, Error: 0.1875\n",
      "Iteration [225], Loss: 0.7519, Error: 0.2188\n",
      "Iteration [226], Loss: 0.6998, Error: 0.2344\n",
      "Iteration [227], Loss: 0.8236, Error: 0.2188\n",
      "Iteration [228], Loss: 0.7123, Error: 0.2031\n",
      "Iteration [229], Loss: 0.8187, Error: 0.2344\n",
      "Iteration [230], Loss: 0.8258, Error: 0.2500\n",
      "Iteration [231], Loss: 0.6902, Error: 0.1875\n",
      "Iteration [232], Loss: 0.5431, Error: 0.1875\n",
      "Iteration [233], Loss: 0.7251, Error: 0.2188\n",
      "Iteration [234], Loss: 0.6412, Error: 0.1719\n",
      "Iteration [235], Loss: 0.6258, Error: 0.1719\n",
      "Iteration [236], Loss: 0.8007, Error: 0.2188\n",
      "Iteration [237], Loss: 0.7373, Error: 0.1719\n",
      "Iteration [238], Loss: 0.7479, Error: 0.2344\n",
      "Iteration [239], Loss: 0.6782, Error: 0.1875\n",
      "Iteration [240], Loss: 0.5923, Error: 0.1562\n",
      "Iteration [241], Loss: 0.8420, Error: 0.2500\n",
      "Iteration [242], Loss: 0.7817, Error: 0.2188\n",
      "Iteration [243], Loss: 0.7945, Error: 0.2344\n",
      "Iteration [244], Loss: 0.6929, Error: 0.2188\n",
      "Iteration [245], Loss: 0.6107, Error: 0.1562\n",
      "Iteration [246], Loss: 0.7383, Error: 0.2188\n",
      "Iteration [247], Loss: 0.8348, Error: 0.2500\n",
      "Iteration [248], Loss: 0.5501, Error: 0.1250\n",
      "Iteration [249], Loss: 0.6838, Error: 0.1875\n",
      "Iteration [250], Loss: 0.8033, Error: 0.2188\n",
      "Iteration [251], Loss: 0.5489, Error: 0.1406\n",
      "Iteration [252], Loss: 0.7438, Error: 0.2344\n",
      "Iteration [253], Loss: 0.8307, Error: 0.2188\n",
      "Iteration [254], Loss: 0.6988, Error: 0.1719\n",
      "Iteration [255], Loss: 0.7749, Error: 0.2031\n",
      "Iteration [256], Loss: 0.6645, Error: 0.1719\n",
      "Iteration [257], Loss: 0.7089, Error: 0.2344\n",
      "Iteration [258], Loss: 0.7682, Error: 0.2188\n",
      "Iteration [259], Loss: 0.8875, Error: 0.2656\n",
      "Iteration [260], Loss: 0.7822, Error: 0.2188\n",
      "Iteration [261], Loss: 0.8107, Error: 0.2656\n",
      "Iteration [262], Loss: 0.6009, Error: 0.1719\n",
      "Iteration [263], Loss: 0.6118, Error: 0.1719\n",
      "Iteration [264], Loss: 0.6331, Error: 0.2031\n",
      "Iteration [265], Loss: 0.4699, Error: 0.0781\n",
      "Iteration [266], Loss: 0.6018, Error: 0.1719\n",
      "Iteration [267], Loss: 0.5822, Error: 0.1562\n",
      "Iteration [268], Loss: 0.7043, Error: 0.1719\n",
      "Iteration [269], Loss: 0.7124, Error: 0.2344\n",
      "Iteration [270], Loss: 0.9279, Error: 0.2656\n",
      "Iteration [271], Loss: 0.5013, Error: 0.1094\n",
      "Iteration [272], Loss: 0.5731, Error: 0.1406\n",
      "Iteration [273], Loss: 0.6849, Error: 0.2344\n",
      "Iteration [274], Loss: 0.7605, Error: 0.2188\n",
      "Iteration [275], Loss: 0.4806, Error: 0.1250\n",
      "Iteration [276], Loss: 0.5038, Error: 0.1406\n",
      "Iteration [277], Loss: 0.6625, Error: 0.1875\n",
      "Iteration [278], Loss: 0.5844, Error: 0.1875\n",
      "Iteration [279], Loss: 0.6410, Error: 0.1250\n",
      "Iteration [280], Loss: 0.5914, Error: 0.1562\n",
      "Iteration [281], Loss: 0.7203, Error: 0.2344\n",
      "Iteration [282], Loss: 0.5648, Error: 0.1562\n",
      "Iteration [283], Loss: 0.7120, Error: 0.2344\n",
      "Iteration [284], Loss: 0.6292, Error: 0.1719\n",
      "Iteration [285], Loss: 0.5936, Error: 0.1719\n",
      "Iteration [286], Loss: 0.6797, Error: 0.2031\n",
      "Iteration [287], Loss: 0.7290, Error: 0.2188\n",
      "Iteration [288], Loss: 0.5250, Error: 0.1250\n",
      "Iteration [289], Loss: 0.7216, Error: 0.2188\n",
      "Iteration [290], Loss: 0.7800, Error: 0.2188\n",
      "Iteration [291], Loss: 0.5138, Error: 0.1406\n",
      "Iteration [292], Loss: 0.6815, Error: 0.2031\n",
      "Iteration [293], Loss: 0.8339, Error: 0.2500\n",
      "Iteration [294], Loss: 0.7181, Error: 0.1875\n",
      "Iteration [295], Loss: 0.7550, Error: 0.2031\n",
      "Iteration [296], Loss: 0.7040, Error: 0.2031\n",
      "Iteration [297], Loss: 0.5638, Error: 0.1406\n",
      "Iteration [298], Loss: 0.7383, Error: 0.2031\n",
      "Iteration [299], Loss: 0.7112, Error: 0.2188\n",
      "Iteration [300], Loss: 0.7292, Error: 0.2188\n",
      "Iteration [301], Loss: 0.6731, Error: 0.2031\n",
      "Iteration [302], Loss: 0.7172, Error: 0.2031\n",
      "Iteration [303], Loss: 0.8056, Error: 0.2188\n",
      "Iteration [304], Loss: 0.7573, Error: 0.2500\n",
      "Iteration [305], Loss: 0.9803, Error: 0.3438\n",
      "Iteration [306], Loss: 0.8147, Error: 0.2031\n",
      "Iteration [307], Loss: 0.5119, Error: 0.1406\n",
      "Iteration [308], Loss: 0.5929, Error: 0.1406\n",
      "Iteration [309], Loss: 0.7195, Error: 0.2031\n",
      "Iteration [310], Loss: 0.7573, Error: 0.2031\n",
      "Iteration [311], Loss: 0.4982, Error: 0.1094\n",
      "Iteration [312], Loss: 0.6644, Error: 0.1875\n",
      "Iteration [313], Loss: 0.6249, Error: 0.1719\n",
      "Iteration [314], Loss: 0.6048, Error: 0.1719\n",
      "Iteration [315], Loss: 0.8717, Error: 0.2656\n",
      "Iteration [316], Loss: 0.7301, Error: 0.2188\n",
      "Iteration [317], Loss: 0.5999, Error: 0.1562\n",
      "Iteration [318], Loss: 0.8124, Error: 0.2500\n",
      "Iteration [319], Loss: 0.4172, Error: 0.1094\n",
      "Iteration [320], Loss: 0.5082, Error: 0.1562\n",
      "Iteration [321], Loss: 0.4901, Error: 0.1250\n",
      "Iteration [322], Loss: 0.5437, Error: 0.1562\n",
      "Iteration [323], Loss: 0.6556, Error: 0.1406\n",
      "Iteration [324], Loss: 0.5352, Error: 0.1406\n",
      "Iteration [325], Loss: 0.5978, Error: 0.1562\n",
      "Iteration [326], Loss: 0.6875, Error: 0.1719\n",
      "Iteration [327], Loss: 0.6547, Error: 0.1875\n",
      "Iteration [328], Loss: 0.6095, Error: 0.1562\n",
      "Iteration [329], Loss: 0.7971, Error: 0.2500\n",
      "Iteration [330], Loss: 0.6985, Error: 0.2031\n",
      "Iteration [331], Loss: 0.5588, Error: 0.1875\n",
      "Iteration [332], Loss: 0.5877, Error: 0.1562\n",
      "Iteration [333], Loss: 0.5126, Error: 0.1250\n",
      "Iteration [334], Loss: 0.4773, Error: 0.1562\n",
      "Iteration [335], Loss: 0.8764, Error: 0.2031\n",
      "Iteration [336], Loss: 0.6861, Error: 0.2031\n",
      "Iteration [337], Loss: 0.6240, Error: 0.1719\n",
      "Iteration [338], Loss: 0.5035, Error: 0.1406\n",
      "Iteration [339], Loss: 0.7165, Error: 0.2188\n",
      "Iteration [340], Loss: 0.6930, Error: 0.2344\n",
      "Iteration [341], Loss: 0.5481, Error: 0.1562\n",
      "Iteration [342], Loss: 0.5727, Error: 0.1719\n",
      "Iteration [343], Loss: 0.7594, Error: 0.2344\n",
      "Iteration [344], Loss: 0.5463, Error: 0.1719\n",
      "Iteration [345], Loss: 0.7422, Error: 0.2500\n",
      "Iteration [346], Loss: 0.6723, Error: 0.2031\n",
      "Iteration [347], Loss: 0.5404, Error: 0.1562\n",
      "Iteration [348], Loss: 0.6674, Error: 0.1719\n",
      "Iteration [349], Loss: 0.4369, Error: 0.1250\n",
      "Iteration [350], Loss: 0.5118, Error: 0.1406\n",
      "Iteration [351], Loss: 0.6921, Error: 0.2188\n",
      "Iteration [352], Loss: 0.9143, Error: 0.2656\n",
      "Iteration [353], Loss: 0.7528, Error: 0.2812\n",
      "Iteration [354], Loss: 0.7435, Error: 0.2344\n",
      "Iteration [355], Loss: 0.5797, Error: 0.1719\n",
      "Iteration [356], Loss: 0.5486, Error: 0.1406\n",
      "Iteration [357], Loss: 0.4125, Error: 0.1250\n",
      "Iteration [358], Loss: 0.5217, Error: 0.1406\n",
      "Iteration [359], Loss: 0.8836, Error: 0.3438\n",
      "Iteration [360], Loss: 0.6726, Error: 0.2188\n",
      "Iteration [361], Loss: 0.8243, Error: 0.2812\n",
      "Iteration [362], Loss: 0.4869, Error: 0.1406\n",
      "Iteration [363], Loss: 0.6085, Error: 0.2031\n",
      "Iteration [364], Loss: 0.6313, Error: 0.2031\n",
      "Iteration [365], Loss: 0.7677, Error: 0.2500\n",
      "Iteration [366], Loss: 0.6363, Error: 0.1719\n",
      "Iteration [367], Loss: 0.5448, Error: 0.1562\n",
      "Iteration [368], Loss: 0.6943, Error: 0.1875\n",
      "Iteration [369], Loss: 0.8086, Error: 0.2188\n",
      "Iteration [370], Loss: 0.5861, Error: 0.1406\n",
      "Iteration [371], Loss: 0.6507, Error: 0.1719\n",
      "Iteration [372], Loss: 0.8791, Error: 0.2969\n",
      "Iteration [373], Loss: 0.5556, Error: 0.1562\n",
      "Iteration [374], Loss: 0.5696, Error: 0.2188\n",
      "Iteration [375], Loss: 0.5128, Error: 0.1562\n",
      "Iteration [376], Loss: 0.4301, Error: 0.1250\n",
      "Iteration [377], Loss: 0.5709, Error: 0.1875\n",
      "Iteration [378], Loss: 0.7890, Error: 0.2188\n",
      "Iteration [379], Loss: 0.5648, Error: 0.1406\n",
      "Iteration [380], Loss: 0.7023, Error: 0.2188\n",
      "Iteration [381], Loss: 0.7091, Error: 0.2344\n",
      "Iteration [382], Loss: 0.7801, Error: 0.2500\n",
      "Iteration [383], Loss: 0.6988, Error: 0.1875\n",
      "Iteration [384], Loss: 0.7779, Error: 0.2344\n",
      "Iteration [385], Loss: 0.5438, Error: 0.1406\n",
      "Iteration [386], Loss: 0.5641, Error: 0.1719\n",
      "Iteration [387], Loss: 0.7307, Error: 0.2500\n",
      "Iteration [388], Loss: 0.6515, Error: 0.2188\n",
      "Iteration [389], Loss: 0.5034, Error: 0.1406\n",
      "Iteration [390], Loss: 0.4943, Error: 0.1719\n",
      "Iteration [391], Loss: 0.6424, Error: 0.1562\n",
      "Iteration [392], Loss: 0.8350, Error: 0.2656\n",
      "Iteration [393], Loss: 0.8927, Error: 0.2188\n",
      "Iteration [394], Loss: 0.8272, Error: 0.2500\n",
      "Iteration [395], Loss: 0.5984, Error: 0.1875\n",
      "Iteration [396], Loss: 0.7915, Error: 0.2500\n",
      "Iteration [397], Loss: 0.9011, Error: 0.3125\n",
      "Iteration [398], Loss: 0.5850, Error: 0.1719\n",
      "Iteration [399], Loss: 0.7531, Error: 0.2031\n",
      "Iteration [400], Loss: 0.7547, Error: 0.2188\n",
      "Iteration [401], Loss: 0.6436, Error: 0.2031\n",
      "Iteration [402], Loss: 0.7680, Error: 0.2344\n",
      "Iteration [403], Loss: 0.6019, Error: 0.1875\n",
      "Iteration [404], Loss: 0.5843, Error: 0.1719\n",
      "Iteration [405], Loss: 0.8952, Error: 0.2656\n",
      "Iteration [406], Loss: 0.5552, Error: 0.1875\n",
      "Iteration [407], Loss: 0.6833, Error: 0.1875\n",
      "Iteration [408], Loss: 0.5173, Error: 0.1406\n",
      "Iteration [409], Loss: 0.8155, Error: 0.2500\n",
      "Iteration [410], Loss: 0.7995, Error: 0.2500\n",
      "Iteration [411], Loss: 0.6493, Error: 0.1875\n",
      "Iteration [412], Loss: 0.6302, Error: 0.1719\n",
      "Iteration [413], Loss: 0.4606, Error: 0.1250\n",
      "Iteration [414], Loss: 0.5529, Error: 0.1562\n",
      "Iteration [415], Loss: 0.6132, Error: 0.1875\n",
      "Iteration [416], Loss: 0.6303, Error: 0.1562\n",
      "Iteration [417], Loss: 0.4971, Error: 0.1562\n",
      "Iteration [418], Loss: 0.7355, Error: 0.2031\n",
      "Iteration [419], Loss: 0.7004, Error: 0.1875\n",
      "Iteration [420], Loss: 0.4901, Error: 0.1250\n",
      "Iteration [421], Loss: 0.6951, Error: 0.2031\n",
      "Iteration [422], Loss: 0.7522, Error: 0.2656\n",
      "Iteration [423], Loss: 0.6440, Error: 0.1875\n",
      "Iteration [424], Loss: 0.9542, Error: 0.2969\n",
      "Iteration [425], Loss: 0.4129, Error: 0.0781\n",
      "Iteration [426], Loss: 0.5143, Error: 0.1406\n",
      "Iteration [427], Loss: 0.6874, Error: 0.2031\n",
      "Iteration [428], Loss: 0.7425, Error: 0.2188\n",
      "Iteration [429], Loss: 0.6868, Error: 0.2188\n",
      "Iteration [430], Loss: 0.7532, Error: 0.2344\n",
      "Iteration [431], Loss: 0.6442, Error: 0.1719\n",
      "Iteration [432], Loss: 0.7042, Error: 0.2188\n",
      "Iteration [433], Loss: 0.4610, Error: 0.0938\n",
      "Iteration [434], Loss: 0.6682, Error: 0.2344\n",
      "Iteration [435], Loss: 0.6490, Error: 0.1875\n",
      "Iteration [436], Loss: 0.6776, Error: 0.2344\n",
      "Iteration [437], Loss: 0.4518, Error: 0.1094\n",
      "Iteration [438], Loss: 0.6404, Error: 0.1875\n",
      "Iteration [439], Loss: 0.6425, Error: 0.2344\n",
      "Iteration [440], Loss: 0.9166, Error: 0.2969\n",
      "Iteration [441], Loss: 0.5274, Error: 0.1406\n",
      "Iteration [442], Loss: 0.7459, Error: 0.2500\n",
      "Iteration [443], Loss: 0.5074, Error: 0.1406\n",
      "Iteration [444], Loss: 0.5755, Error: 0.1719\n",
      "Iteration [445], Loss: 0.6114, Error: 0.1875\n",
      "Iteration [446], Loss: 0.9242, Error: 0.3125\n",
      "Iteration [447], Loss: 0.7584, Error: 0.2188\n",
      "Iteration [448], Loss: 0.7078, Error: 0.2031\n",
      "Iteration [449], Loss: 0.6138, Error: 0.1875\n",
      "Iteration [450], Loss: 0.7091, Error: 0.1875\n",
      "Iteration [451], Loss: 0.5507, Error: 0.1406\n",
      "Iteration [452], Loss: 0.6253, Error: 0.1562\n",
      "Iteration [453], Loss: 0.8177, Error: 0.2500\n",
      "Iteration [454], Loss: 0.6551, Error: 0.2031\n",
      "Iteration [455], Loss: 0.6202, Error: 0.2344\n",
      "Iteration [456], Loss: 0.6439, Error: 0.1719\n",
      "Iteration [457], Loss: 0.7781, Error: 0.2344\n",
      "Iteration [458], Loss: 0.8162, Error: 0.2500\n",
      "Iteration [459], Loss: 0.5780, Error: 0.1406\n",
      "Iteration [460], Loss: 0.8668, Error: 0.2500\n",
      "Iteration [461], Loss: 0.5530, Error: 0.1406\n",
      "Iteration [462], Loss: 0.6068, Error: 0.2031\n",
      "Iteration [463], Loss: 0.6254, Error: 0.1875\n",
      "Iteration [464], Loss: 0.4263, Error: 0.1250\n",
      "Iteration [465], Loss: 0.6536, Error: 0.1875\n",
      "Iteration [466], Loss: 0.7823, Error: 0.2500\n",
      "Iteration [467], Loss: 0.5449, Error: 0.1562\n",
      "Iteration [468], Loss: 0.6655, Error: 0.2083\n",
      "Iteration [469], Loss: 0.7650, Error: 0.2500\n",
      "Iteration [470], Loss: 0.8392, Error: 0.2656\n",
      "Iteration [471], Loss: 0.7294, Error: 0.2188\n",
      "Iteration [472], Loss: 0.6142, Error: 0.2031\n",
      "Iteration [473], Loss: 0.6588, Error: 0.1875\n",
      "Iteration [474], Loss: 0.4602, Error: 0.1406\n",
      "Iteration [475], Loss: 0.6492, Error: 0.2188\n",
      "Iteration [476], Loss: 0.5573, Error: 0.1875\n",
      "Iteration [477], Loss: 0.8781, Error: 0.2500\n",
      "Iteration [478], Loss: 0.6784, Error: 0.1719\n",
      "Iteration [479], Loss: 0.4784, Error: 0.1094\n",
      "Iteration [480], Loss: 0.3942, Error: 0.1250\n",
      "Iteration [481], Loss: 0.5288, Error: 0.1719\n",
      "Iteration [482], Loss: 0.7478, Error: 0.1875\n",
      "Iteration [483], Loss: 0.7665, Error: 0.2344\n",
      "Iteration [484], Loss: 0.5401, Error: 0.1875\n",
      "Iteration [485], Loss: 0.7434, Error: 0.2656\n",
      "Iteration [486], Loss: 0.6163, Error: 0.1875\n",
      "Iteration [487], Loss: 0.5580, Error: 0.1719\n",
      "Iteration [488], Loss: 0.6013, Error: 0.2031\n",
      "Iteration [489], Loss: 0.4956, Error: 0.1250\n",
      "Iteration [490], Loss: 0.6249, Error: 0.1875\n",
      "Iteration [491], Loss: 0.6688, Error: 0.2031\n",
      "Iteration [492], Loss: 0.6442, Error: 0.1562\n",
      "Iteration [493], Loss: 0.7086, Error: 0.2188\n",
      "Iteration [494], Loss: 0.6438, Error: 0.1875\n",
      "Iteration [495], Loss: 0.6025, Error: 0.2188\n",
      "Iteration [496], Loss: 0.4411, Error: 0.1250\n",
      "Iteration [497], Loss: 0.5265, Error: 0.1562\n",
      "Iteration [498], Loss: 0.7501, Error: 0.2344\n",
      "Iteration [499], Loss: 0.5939, Error: 0.1406\n",
      "Iteration [500], Loss: 0.7156, Error: 0.2031\n",
      "Iteration [501], Loss: 0.5499, Error: 0.1562\n",
      "Iteration [502], Loss: 0.6628, Error: 0.2344\n",
      "Iteration [503], Loss: 0.6669, Error: 0.2031\n",
      "Iteration [504], Loss: 0.6567, Error: 0.2031\n",
      "Iteration [505], Loss: 0.6982, Error: 0.2031\n",
      "Iteration [506], Loss: 0.5630, Error: 0.1562\n",
      "Iteration [507], Loss: 0.7484, Error: 0.2188\n",
      "Iteration [508], Loss: 0.9216, Error: 0.2344\n",
      "Iteration [509], Loss: 0.5463, Error: 0.1562\n",
      "Iteration [510], Loss: 0.3929, Error: 0.1094\n",
      "Iteration [511], Loss: 0.4329, Error: 0.1094\n",
      "Iteration [512], Loss: 0.4325, Error: 0.1250\n",
      "Iteration [513], Loss: 0.4192, Error: 0.1406\n",
      "Iteration [514], Loss: 0.5688, Error: 0.1406\n",
      "Iteration [515], Loss: 0.4048, Error: 0.0938\n",
      "Iteration [516], Loss: 0.5166, Error: 0.1250\n",
      "Iteration [517], Loss: 0.8224, Error: 0.2969\n",
      "Iteration [518], Loss: 0.7075, Error: 0.2188\n",
      "Iteration [519], Loss: 0.4781, Error: 0.1406\n",
      "Iteration [520], Loss: 0.4947, Error: 0.0938\n",
      "Iteration [521], Loss: 0.5150, Error: 0.1719\n",
      "Iteration [522], Loss: 0.6330, Error: 0.1719\n",
      "Iteration [523], Loss: 0.3783, Error: 0.0781\n",
      "Iteration [524], Loss: 0.8520, Error: 0.2188\n",
      "Iteration [525], Loss: 0.5786, Error: 0.1719\n",
      "Iteration [526], Loss: 0.5432, Error: 0.1562\n",
      "Iteration [527], Loss: 0.6631, Error: 0.1875\n",
      "Iteration [528], Loss: 0.7136, Error: 0.2500\n",
      "Iteration [529], Loss: 0.6345, Error: 0.2188\n",
      "Iteration [530], Loss: 0.6520, Error: 0.2031\n",
      "Iteration [531], Loss: 0.6264, Error: 0.1875\n",
      "Iteration [532], Loss: 0.5195, Error: 0.1562\n",
      "Iteration [533], Loss: 0.7050, Error: 0.2500\n",
      "Iteration [534], Loss: 0.7147, Error: 0.2500\n",
      "Iteration [535], Loss: 0.6064, Error: 0.2031\n",
      "Iteration [536], Loss: 0.4521, Error: 0.1406\n",
      "Iteration [537], Loss: 0.7530, Error: 0.2344\n",
      "Iteration [538], Loss: 0.6766, Error: 0.2344\n",
      "Iteration [539], Loss: 0.6665, Error: 0.2188\n",
      "Iteration [540], Loss: 0.6967, Error: 0.2188\n",
      "Iteration [541], Loss: 0.5780, Error: 0.1875\n",
      "Iteration [542], Loss: 0.5984, Error: 0.1562\n",
      "Iteration [543], Loss: 0.6960, Error: 0.2188\n",
      "Iteration [544], Loss: 0.6369, Error: 0.2031\n",
      "Iteration [545], Loss: 0.6754, Error: 0.2031\n",
      "Iteration [546], Loss: 0.5921, Error: 0.2031\n",
      "Iteration [547], Loss: 0.6470, Error: 0.2188\n",
      "Iteration [548], Loss: 0.5498, Error: 0.1719\n",
      "Iteration [549], Loss: 0.6438, Error: 0.1875\n",
      "Iteration [550], Loss: 0.5728, Error: 0.1719\n",
      "Iteration [551], Loss: 0.5560, Error: 0.1719\n",
      "Iteration [552], Loss: 0.5225, Error: 0.1562\n",
      "Iteration [553], Loss: 0.5603, Error: 0.1406\n",
      "Iteration [554], Loss: 0.4913, Error: 0.1719\n",
      "Iteration [555], Loss: 0.5525, Error: 0.1719\n",
      "Iteration [556], Loss: 0.5543, Error: 0.1719\n",
      "Iteration [557], Loss: 0.5167, Error: 0.1406\n",
      "Iteration [558], Loss: 0.4452, Error: 0.1719\n",
      "Iteration [559], Loss: 0.6795, Error: 0.1875\n",
      "Iteration [560], Loss: 0.7317, Error: 0.2344\n",
      "Iteration [561], Loss: 0.5432, Error: 0.1562\n",
      "Iteration [562], Loss: 0.5908, Error: 0.1719\n",
      "Iteration [563], Loss: 0.8639, Error: 0.2500\n",
      "Iteration [564], Loss: 0.5763, Error: 0.1719\n",
      "Iteration [565], Loss: 0.6369, Error: 0.1875\n",
      "Iteration [566], Loss: 0.4447, Error: 0.0938\n",
      "Iteration [567], Loss: 0.7067, Error: 0.2188\n",
      "Iteration [568], Loss: 0.5478, Error: 0.1719\n",
      "Iteration [569], Loss: 0.7071, Error: 0.2188\n",
      "Iteration [570], Loss: 0.5603, Error: 0.1719\n",
      "Iteration [571], Loss: 0.3994, Error: 0.1094\n",
      "Iteration [572], Loss: 0.5019, Error: 0.1719\n",
      "Iteration [573], Loss: 0.5767, Error: 0.1562\n",
      "Iteration [574], Loss: 0.6301, Error: 0.1719\n",
      "Iteration [575], Loss: 0.4691, Error: 0.1719\n",
      "Iteration [576], Loss: 0.7902, Error: 0.2188\n",
      "Iteration [577], Loss: 0.6336, Error: 0.2031\n",
      "Iteration [578], Loss: 0.5835, Error: 0.1406\n",
      "Iteration [579], Loss: 0.5716, Error: 0.1562\n",
      "Iteration [580], Loss: 0.4878, Error: 0.1250\n",
      "Iteration [581], Loss: 0.6518, Error: 0.2031\n",
      "Iteration [582], Loss: 0.7256, Error: 0.2500\n",
      "Iteration [583], Loss: 0.5692, Error: 0.1562\n",
      "Iteration [584], Loss: 0.5414, Error: 0.1875\n",
      "Iteration [585], Loss: 0.4726, Error: 0.0938\n",
      "Iteration [586], Loss: 0.7367, Error: 0.2188\n",
      "Iteration [587], Loss: 0.6685, Error: 0.2500\n",
      "Iteration [588], Loss: 0.4850, Error: 0.1250\n",
      "Iteration [589], Loss: 0.7048, Error: 0.2031\n",
      "Iteration [590], Loss: 0.6811, Error: 0.2344\n",
      "Iteration [591], Loss: 0.5939, Error: 0.2188\n",
      "Iteration [592], Loss: 0.7256, Error: 0.2031\n",
      "Iteration [593], Loss: 0.7006, Error: 0.2031\n",
      "Iteration [594], Loss: 0.7244, Error: 0.2812\n",
      "Iteration [595], Loss: 0.7053, Error: 0.2344\n",
      "Iteration [596], Loss: 0.5460, Error: 0.1562\n",
      "Iteration [597], Loss: 0.7311, Error: 0.2188\n",
      "Iteration [598], Loss: 0.6517, Error: 0.2188\n",
      "Iteration [599], Loss: 0.4499, Error: 0.1250\n",
      "Iteration [600], Loss: 0.5497, Error: 0.2188\n",
      "Iteration [601], Loss: 0.7840, Error: 0.2812\n",
      "Iteration [602], Loss: 0.4484, Error: 0.0781\n",
      "Iteration [603], Loss: 0.5006, Error: 0.1250\n",
      "Iteration [604], Loss: 0.5891, Error: 0.1719\n",
      "Iteration [605], Loss: 0.5935, Error: 0.1562\n",
      "Iteration [606], Loss: 0.5265, Error: 0.1562\n",
      "Iteration [607], Loss: 0.5537, Error: 0.1875\n",
      "Iteration [608], Loss: 0.7035, Error: 0.2344\n",
      "Iteration [609], Loss: 0.6450, Error: 0.1562\n",
      "Iteration [610], Loss: 0.8920, Error: 0.3125\n",
      "Iteration [611], Loss: 0.6258, Error: 0.2031\n",
      "Iteration [612], Loss: 0.5436, Error: 0.1250\n",
      "Iteration [613], Loss: 0.6149, Error: 0.1562\n",
      "Iteration [614], Loss: 0.7588, Error: 0.2656\n",
      "Iteration [615], Loss: 0.3612, Error: 0.1094\n",
      "Iteration [616], Loss: 0.8141, Error: 0.2656\n",
      "Iteration [617], Loss: 0.4768, Error: 0.1562\n",
      "Iteration [618], Loss: 0.5066, Error: 0.0938\n",
      "Iteration [619], Loss: 0.5240, Error: 0.1562\n",
      "Iteration [620], Loss: 0.3672, Error: 0.0625\n",
      "Iteration [621], Loss: 0.5783, Error: 0.1875\n",
      "Iteration [622], Loss: 0.5247, Error: 0.1562\n",
      "Iteration [623], Loss: 0.6491, Error: 0.1875\n",
      "Iteration [624], Loss: 0.6925, Error: 0.2344\n",
      "Iteration [625], Loss: 0.6223, Error: 0.1875\n",
      "Iteration [626], Loss: 0.5185, Error: 0.1562\n",
      "Iteration [627], Loss: 0.5695, Error: 0.1719\n",
      "Iteration [628], Loss: 0.5509, Error: 0.2031\n",
      "Iteration [629], Loss: 0.4007, Error: 0.0938\n",
      "Iteration [630], Loss: 0.5066, Error: 0.1250\n",
      "Iteration [631], Loss: 0.6090, Error: 0.1719\n",
      "Iteration [632], Loss: 0.5526, Error: 0.1719\n",
      "Iteration [633], Loss: 0.8009, Error: 0.2656\n",
      "Iteration [634], Loss: 0.6924, Error: 0.2344\n",
      "Iteration [635], Loss: 0.4736, Error: 0.1562\n",
      "Iteration [636], Loss: 0.9024, Error: 0.3125\n",
      "Iteration [637], Loss: 0.3950, Error: 0.1250\n",
      "Iteration [638], Loss: 0.6270, Error: 0.1562\n",
      "Iteration [639], Loss: 0.4984, Error: 0.1875\n",
      "Iteration [640], Loss: 0.8476, Error: 0.3125\n",
      "Iteration [641], Loss: 0.6289, Error: 0.2031\n",
      "Iteration [642], Loss: 0.4733, Error: 0.1719\n",
      "Iteration [643], Loss: 0.7962, Error: 0.2812\n",
      "Iteration [644], Loss: 0.5631, Error: 0.1562\n",
      "Iteration [645], Loss: 0.5172, Error: 0.1406\n",
      "Iteration [646], Loss: 0.7174, Error: 0.2344\n",
      "Iteration [647], Loss: 0.3529, Error: 0.0625\n",
      "Iteration [648], Loss: 0.6363, Error: 0.2344\n",
      "Iteration [649], Loss: 0.7929, Error: 0.2500\n",
      "Iteration [650], Loss: 0.4404, Error: 0.1250\n",
      "Iteration [651], Loss: 0.5423, Error: 0.1719\n",
      "Iteration [652], Loss: 0.4926, Error: 0.1562\n",
      "Iteration [653], Loss: 0.5414, Error: 0.1719\n",
      "Iteration [654], Loss: 0.8779, Error: 0.2500\n",
      "Iteration [655], Loss: 0.3105, Error: 0.0625\n",
      "Iteration [656], Loss: 0.4977, Error: 0.1406\n",
      "Iteration [657], Loss: 0.5546, Error: 0.1719\n",
      "Iteration [658], Loss: 0.6991, Error: 0.2500\n",
      "Iteration [659], Loss: 0.7053, Error: 0.1719\n",
      "Iteration [660], Loss: 0.6286, Error: 0.1562\n",
      "Iteration [661], Loss: 0.6037, Error: 0.1875\n",
      "Iteration [662], Loss: 0.5333, Error: 0.1719\n",
      "Iteration [663], Loss: 0.6269, Error: 0.2031\n",
      "Iteration [664], Loss: 0.6995, Error: 0.2344\n",
      "Iteration [665], Loss: 0.4842, Error: 0.1719\n",
      "Iteration [666], Loss: 0.4913, Error: 0.1094\n",
      "Iteration [667], Loss: 0.4984, Error: 0.1094\n",
      "Iteration [668], Loss: 0.4271, Error: 0.1250\n",
      "Iteration [669], Loss: 0.8835, Error: 0.2969\n",
      "Iteration [670], Loss: 0.6918, Error: 0.2344\n",
      "Iteration [671], Loss: 0.5739, Error: 0.1719\n",
      "Iteration [672], Loss: 0.5184, Error: 0.1719\n",
      "Iteration [673], Loss: 0.5632, Error: 0.1406\n",
      "Iteration [674], Loss: 0.4913, Error: 0.1406\n",
      "Iteration [675], Loss: 0.3852, Error: 0.0938\n",
      "Iteration [676], Loss: 0.5922, Error: 0.1719\n",
      "Iteration [677], Loss: 0.5667, Error: 0.1719\n",
      "Iteration [678], Loss: 0.7873, Error: 0.2344\n",
      "Iteration [679], Loss: 0.4801, Error: 0.1250\n",
      "Iteration [680], Loss: 0.7581, Error: 0.2500\n",
      "Iteration [681], Loss: 0.6025, Error: 0.1875\n",
      "Iteration [682], Loss: 0.6967, Error: 0.2188\n",
      "Iteration [683], Loss: 0.5070, Error: 0.1875\n",
      "Iteration [684], Loss: 0.4464, Error: 0.1250\n",
      "Iteration [685], Loss: 0.7133, Error: 0.2500\n",
      "Iteration [686], Loss: 0.8115, Error: 0.2344\n",
      "Iteration [687], Loss: 0.5437, Error: 0.1875\n",
      "Iteration [688], Loss: 0.5716, Error: 0.1719\n",
      "Iteration [689], Loss: 0.8284, Error: 0.2812\n",
      "Iteration [690], Loss: 0.7680, Error: 0.2344\n",
      "Iteration [691], Loss: 0.5708, Error: 0.1562\n",
      "Iteration [692], Loss: 0.5396, Error: 0.1719\n",
      "Iteration [693], Loss: 0.5090, Error: 0.1562\n",
      "Iteration [694], Loss: 0.7003, Error: 0.1875\n",
      "Iteration [695], Loss: 0.5895, Error: 0.1719\n",
      "Iteration [696], Loss: 0.7285, Error: 0.2500\n",
      "Iteration [697], Loss: 0.5736, Error: 0.1719\n",
      "Iteration [698], Loss: 0.6988, Error: 0.2344\n",
      "Iteration [699], Loss: 0.6517, Error: 0.1875\n",
      "Iteration [700], Loss: 0.5982, Error: 0.2188\n",
      "Iteration [701], Loss: 0.8036, Error: 0.2344\n",
      "Iteration [702], Loss: 0.4793, Error: 0.1719\n",
      "Iteration [703], Loss: 0.5387, Error: 0.1719\n",
      "Iteration [704], Loss: 0.4914, Error: 0.1562\n",
      "Iteration [705], Loss: 0.6508, Error: 0.2031\n",
      "Iteration [706], Loss: 0.5603, Error: 0.1875\n",
      "Iteration [707], Loss: 0.7424, Error: 0.2344\n",
      "Iteration [708], Loss: 0.4733, Error: 0.1250\n",
      "Iteration [709], Loss: 0.6021, Error: 0.1719\n",
      "Iteration [710], Loss: 0.7167, Error: 0.2344\n",
      "Iteration [711], Loss: 0.5526, Error: 0.2031\n",
      "Iteration [712], Loss: 0.5036, Error: 0.1406\n",
      "Iteration [713], Loss: 0.6848, Error: 0.2344\n",
      "Iteration [714], Loss: 0.5286, Error: 0.1875\n",
      "Iteration [715], Loss: 0.5633, Error: 0.1719\n",
      "Iteration [716], Loss: 0.3763, Error: 0.1094\n",
      "Iteration [717], Loss: 0.5061, Error: 0.1250\n",
      "Iteration [718], Loss: 0.8742, Error: 0.2656\n",
      "Iteration [719], Loss: 0.4418, Error: 0.1250\n",
      "Iteration [720], Loss: 0.7389, Error: 0.2344\n",
      "Iteration [721], Loss: 0.6340, Error: 0.2188\n",
      "Iteration [722], Loss: 0.5993, Error: 0.1719\n",
      "Iteration [723], Loss: 0.5621, Error: 0.1719\n",
      "Iteration [724], Loss: 0.5974, Error: 0.1719\n",
      "Iteration [725], Loss: 0.5385, Error: 0.1562\n",
      "Iteration [726], Loss: 0.6705, Error: 0.1406\n",
      "Iteration [727], Loss: 0.9011, Error: 0.3281\n",
      "Iteration [728], Loss: 0.6228, Error: 0.2188\n",
      "Iteration [729], Loss: 0.5723, Error: 0.1562\n",
      "Iteration [730], Loss: 0.5803, Error: 0.1562\n",
      "Iteration [731], Loss: 0.3561, Error: 0.0938\n",
      "Iteration [732], Loss: 0.5467, Error: 0.1562\n",
      "Iteration [733], Loss: 0.5596, Error: 0.1562\n",
      "Iteration [734], Loss: 0.5361, Error: 0.1719\n",
      "Iteration [735], Loss: 0.6155, Error: 0.2031\n",
      "Iteration [736], Loss: 0.6018, Error: 0.1875\n",
      "Iteration [737], Loss: 0.6116, Error: 0.2031\n",
      "Iteration [738], Loss: 0.7568, Error: 0.2031\n",
      "Iteration [739], Loss: 0.5878, Error: 0.1719\n",
      "Iteration [740], Loss: 0.5074, Error: 0.1719\n",
      "Iteration [741], Loss: 0.4414, Error: 0.1250\n",
      "Iteration [742], Loss: 0.6626, Error: 0.2188\n",
      "Iteration [743], Loss: 0.7535, Error: 0.2656\n",
      "Iteration [744], Loss: 0.5936, Error: 0.1875\n",
      "Iteration [745], Loss: 0.7774, Error: 0.2344\n",
      "Iteration [746], Loss: 0.5954, Error: 0.1562\n",
      "Iteration [747], Loss: 0.8165, Error: 0.2812\n",
      "Iteration [748], Loss: 0.5141, Error: 0.1562\n",
      "Iteration [749], Loss: 0.5916, Error: 0.1875\n",
      "Iteration [750], Loss: 0.7127, Error: 0.1875\n",
      "Iteration [751], Loss: 0.5405, Error: 0.1562\n",
      "Iteration [752], Loss: 0.6571, Error: 0.1875\n",
      "Iteration [753], Loss: 0.5768, Error: 0.1719\n",
      "Iteration [754], Loss: 0.6323, Error: 0.2031\n",
      "Iteration [755], Loss: 0.4585, Error: 0.1406\n",
      "Iteration [756], Loss: 0.3606, Error: 0.0938\n",
      "Iteration [757], Loss: 0.4610, Error: 0.1406\n",
      "Iteration [758], Loss: 0.5620, Error: 0.1719\n",
      "Iteration [759], Loss: 0.5327, Error: 0.1719\n",
      "Iteration [760], Loss: 0.6237, Error: 0.2188\n",
      "Iteration [761], Loss: 0.5528, Error: 0.1719\n",
      "Iteration [762], Loss: 0.4653, Error: 0.1562\n",
      "Iteration [763], Loss: 0.5000, Error: 0.1406\n",
      "Iteration [764], Loss: 0.5039, Error: 0.1406\n",
      "Iteration [765], Loss: 0.7237, Error: 0.2031\n",
      "Iteration [766], Loss: 0.4714, Error: 0.1562\n",
      "Iteration [767], Loss: 0.8788, Error: 0.3125\n",
      "Iteration [768], Loss: 0.4957, Error: 0.1250\n",
      "Iteration [769], Loss: 0.5448, Error: 0.1719\n",
      "Iteration [770], Loss: 0.5282, Error: 0.2031\n",
      "Iteration [771], Loss: 0.5158, Error: 0.1406\n",
      "Iteration [772], Loss: 0.4979, Error: 0.1250\n",
      "Iteration [773], Loss: 0.5742, Error: 0.1562\n",
      "Iteration [774], Loss: 0.6666, Error: 0.2344\n",
      "Iteration [775], Loss: 0.5626, Error: 0.1719\n",
      "Iteration [776], Loss: 0.7811, Error: 0.2188\n",
      "Iteration [777], Loss: 0.4305, Error: 0.1406\n",
      "Iteration [778], Loss: 0.5215, Error: 0.1875\n",
      "Iteration [779], Loss: 0.7159, Error: 0.2344\n",
      "Iteration [780], Loss: 0.4162, Error: 0.1406\n",
      "Iteration [781], Loss: 0.7289, Error: 0.2500\n",
      "Iteration [782], Loss: 0.4411, Error: 0.1406\n",
      "Iteration [783], Loss: 0.4938, Error: 0.1406\n",
      "Iteration [784], Loss: 0.7330, Error: 0.2188\n",
      "Iteration [785], Loss: 0.5704, Error: 0.1719\n",
      "Iteration [786], Loss: 0.7882, Error: 0.2500\n",
      "Iteration [787], Loss: 0.4452, Error: 0.1250\n",
      "Iteration [788], Loss: 0.6706, Error: 0.2188\n",
      "Iteration [789], Loss: 0.4785, Error: 0.1094\n",
      "Iteration [790], Loss: 0.3225, Error: 0.1250\n",
      "Iteration [791], Loss: 0.7204, Error: 0.2656\n",
      "Iteration [792], Loss: 0.5278, Error: 0.1406\n",
      "Iteration [793], Loss: 0.3718, Error: 0.0938\n",
      "Iteration [794], Loss: 0.5540, Error: 0.1719\n",
      "Iteration [795], Loss: 0.5297, Error: 0.1719\n",
      "Iteration [796], Loss: 0.4064, Error: 0.1250\n",
      "Iteration [797], Loss: 0.6362, Error: 0.1406\n",
      "Iteration [798], Loss: 0.7568, Error: 0.2656\n",
      "Iteration [799], Loss: 0.4912, Error: 0.1562\n",
      "Iteration [800], Loss: 0.7304, Error: 0.2969\n",
      "Iteration [801], Loss: 0.6784, Error: 0.2344\n",
      "Iteration [802], Loss: 0.5325, Error: 0.1875\n",
      "Iteration [803], Loss: 0.7565, Error: 0.2500\n",
      "Iteration [804], Loss: 0.5327, Error: 0.1562\n",
      "Iteration [805], Loss: 0.6430, Error: 0.2031\n",
      "Iteration [806], Loss: 0.5413, Error: 0.1562\n",
      "Iteration [807], Loss: 0.5484, Error: 0.1875\n",
      "Iteration [808], Loss: 0.6379, Error: 0.1875\n",
      "Iteration [809], Loss: 0.5696, Error: 0.1875\n",
      "Iteration [810], Loss: 0.5211, Error: 0.1875\n",
      "Iteration [811], Loss: 0.7137, Error: 0.2656\n",
      "Iteration [812], Loss: 0.6570, Error: 0.1719\n",
      "Iteration [813], Loss: 0.6720, Error: 0.2344\n",
      "Iteration [814], Loss: 0.5145, Error: 0.1406\n",
      "Iteration [815], Loss: 0.6384, Error: 0.1719\n",
      "Iteration [816], Loss: 0.5960, Error: 0.2344\n",
      "Iteration [817], Loss: 0.6023, Error: 0.2031\n",
      "Iteration [818], Loss: 0.4761, Error: 0.1250\n",
      "Iteration [819], Loss: 0.5798, Error: 0.1875\n",
      "Iteration [820], Loss: 0.4196, Error: 0.1406\n",
      "Iteration [821], Loss: 0.7145, Error: 0.3125\n",
      "Iteration [822], Loss: 0.6567, Error: 0.2031\n",
      "Iteration [823], Loss: 0.5028, Error: 0.1562\n",
      "Iteration [824], Loss: 0.7179, Error: 0.2188\n",
      "Iteration [825], Loss: 0.5518, Error: 0.1250\n",
      "Iteration [826], Loss: 0.5432, Error: 0.1719\n",
      "Iteration [827], Loss: 0.5707, Error: 0.1875\n",
      "Iteration [828], Loss: 0.5918, Error: 0.2188\n",
      "Iteration [829], Loss: 0.5507, Error: 0.1406\n",
      "Iteration [830], Loss: 0.5436, Error: 0.1562\n",
      "Iteration [831], Loss: 0.5579, Error: 0.1875\n",
      "Iteration [832], Loss: 0.5692, Error: 0.2031\n",
      "Iteration [833], Loss: 0.4854, Error: 0.1250\n",
      "Iteration [834], Loss: 0.4232, Error: 0.1250\n",
      "Iteration [835], Loss: 0.5785, Error: 0.2344\n",
      "Iteration [836], Loss: 0.6729, Error: 0.2188\n",
      "Iteration [837], Loss: 0.6995, Error: 0.2500\n",
      "Iteration [838], Loss: 0.3959, Error: 0.1250\n",
      "Iteration [839], Loss: 0.5726, Error: 0.1406\n",
      "Iteration [840], Loss: 0.7179, Error: 0.2188\n",
      "Iteration [841], Loss: 0.5804, Error: 0.2031\n",
      "Iteration [842], Loss: 0.8115, Error: 0.2656\n",
      "Iteration [843], Loss: 0.6234, Error: 0.2188\n",
      "Iteration [844], Loss: 0.6169, Error: 0.2031\n",
      "Iteration [845], Loss: 0.4399, Error: 0.1094\n",
      "Iteration [846], Loss: 0.8183, Error: 0.2812\n",
      "Iteration [847], Loss: 0.6329, Error: 0.1719\n",
      "Iteration [848], Loss: 0.5621, Error: 0.2188\n",
      "Iteration [849], Loss: 0.5916, Error: 0.1719\n",
      "Iteration [850], Loss: 0.6923, Error: 0.2344\n",
      "Iteration [851], Loss: 0.6012, Error: 0.1875\n",
      "Iteration [852], Loss: 0.4902, Error: 0.1250\n",
      "Iteration [853], Loss: 0.7255, Error: 0.2500\n",
      "Iteration [854], Loss: 0.5885, Error: 0.1719\n",
      "Iteration [855], Loss: 0.6709, Error: 0.2188\n",
      "Iteration [856], Loss: 0.4260, Error: 0.1406\n",
      "Iteration [857], Loss: 0.8708, Error: 0.2656\n",
      "Iteration [858], Loss: 0.5745, Error: 0.2031\n",
      "Iteration [859], Loss: 0.4299, Error: 0.1094\n",
      "Iteration [860], Loss: 0.6525, Error: 0.2188\n",
      "Iteration [861], Loss: 0.5251, Error: 0.1719\n",
      "Iteration [862], Loss: 0.7035, Error: 0.2188\n",
      "Iteration [863], Loss: 0.7044, Error: 0.2031\n",
      "Iteration [864], Loss: 0.5440, Error: 0.2031\n",
      "Iteration [865], Loss: 0.7393, Error: 0.2500\n",
      "Iteration [866], Loss: 0.7607, Error: 0.2344\n",
      "Iteration [867], Loss: 0.6561, Error: 0.2188\n",
      "Iteration [868], Loss: 0.7515, Error: 0.2031\n",
      "Iteration [869], Loss: 0.5961, Error: 0.1875\n",
      "Iteration [870], Loss: 0.5660, Error: 0.1875\n",
      "Iteration [871], Loss: 0.4127, Error: 0.1094\n",
      "Iteration [872], Loss: 0.6913, Error: 0.2344\n",
      "Iteration [873], Loss: 0.5543, Error: 0.1875\n",
      "Iteration [874], Loss: 0.3949, Error: 0.0938\n",
      "Iteration [875], Loss: 0.6250, Error: 0.2031\n",
      "Iteration [876], Loss: 0.3391, Error: 0.0938\n",
      "Iteration [877], Loss: 0.8049, Error: 0.2188\n",
      "Iteration [878], Loss: 0.3474, Error: 0.0781\n",
      "Iteration [879], Loss: 0.4795, Error: 0.0938\n",
      "Iteration [880], Loss: 0.5684, Error: 0.1719\n",
      "Iteration [881], Loss: 0.5338, Error: 0.1719\n",
      "Iteration [882], Loss: 0.5785, Error: 0.1719\n",
      "Iteration [883], Loss: 0.6524, Error: 0.2344\n",
      "Iteration [884], Loss: 0.5268, Error: 0.1094\n",
      "Iteration [885], Loss: 0.5625, Error: 0.1562\n",
      "Iteration [886], Loss: 0.5861, Error: 0.1719\n",
      "Iteration [887], Loss: 0.4786, Error: 0.1094\n",
      "Iteration [888], Loss: 0.7620, Error: 0.2500\n",
      "Iteration [889], Loss: 0.3213, Error: 0.0938\n",
      "Iteration [890], Loss: 0.7315, Error: 0.2031\n",
      "Iteration [891], Loss: 0.5810, Error: 0.1719\n",
      "Iteration [892], Loss: 0.7270, Error: 0.1875\n",
      "Iteration [893], Loss: 0.5383, Error: 0.1719\n",
      "Iteration [894], Loss: 0.4584, Error: 0.1250\n",
      "Iteration [895], Loss: 0.5430, Error: 0.1875\n",
      "Iteration [896], Loss: 0.6194, Error: 0.2188\n",
      "Iteration [897], Loss: 0.5102, Error: 0.1250\n",
      "Iteration [898], Loss: 0.6390, Error: 0.2031\n",
      "Iteration [899], Loss: 0.8407, Error: 0.2500\n",
      "Iteration [900], Loss: 0.5295, Error: 0.2188\n",
      "Iteration [901], Loss: 0.3466, Error: 0.1094\n",
      "Iteration [902], Loss: 0.4995, Error: 0.1562\n",
      "Iteration [903], Loss: 0.6745, Error: 0.2188\n",
      "Iteration [904], Loss: 0.6878, Error: 0.2031\n",
      "Iteration [905], Loss: 0.4367, Error: 0.1250\n",
      "Iteration [906], Loss: 0.6642, Error: 0.2031\n",
      "Iteration [907], Loss: 0.3212, Error: 0.0781\n",
      "Iteration [908], Loss: 0.6216, Error: 0.2031\n",
      "Iteration [909], Loss: 0.6434, Error: 0.2031\n",
      "Iteration [910], Loss: 0.5252, Error: 0.2031\n",
      "Iteration [911], Loss: 0.5765, Error: 0.1562\n",
      "Iteration [912], Loss: 0.5756, Error: 0.1562\n",
      "Iteration [913], Loss: 0.5514, Error: 0.2031\n",
      "Iteration [914], Loss: 0.5620, Error: 0.2031\n",
      "Iteration [915], Loss: 0.6833, Error: 0.2188\n",
      "Iteration [916], Loss: 0.3885, Error: 0.0781\n",
      "Iteration [917], Loss: 0.5538, Error: 0.1875\n",
      "Iteration [918], Loss: 0.5203, Error: 0.1406\n",
      "Iteration [919], Loss: 0.7096, Error: 0.2188\n",
      "Iteration [920], Loss: 0.4182, Error: 0.1250\n",
      "Iteration [921], Loss: 0.6620, Error: 0.2031\n",
      "Iteration [922], Loss: 0.5801, Error: 0.1875\n",
      "Iteration [923], Loss: 0.8109, Error: 0.2656\n",
      "Iteration [924], Loss: 0.6952, Error: 0.1719\n",
      "Iteration [925], Loss: 0.5408, Error: 0.1562\n",
      "Iteration [926], Loss: 0.8030, Error: 0.2188\n",
      "Iteration [927], Loss: 0.5966, Error: 0.1875\n",
      "Iteration [928], Loss: 0.6451, Error: 0.2188\n",
      "Iteration [929], Loss: 0.5966, Error: 0.1719\n",
      "Iteration [930], Loss: 0.4926, Error: 0.1719\n",
      "Iteration [931], Loss: 0.5964, Error: 0.2031\n",
      "Iteration [932], Loss: 0.6924, Error: 0.2188\n",
      "Iteration [933], Loss: 0.5459, Error: 0.1719\n",
      "Iteration [934], Loss: 0.5159, Error: 0.1406\n",
      "Iteration [935], Loss: 0.5044, Error: 0.1875\n",
      "Iteration [936], Loss: 0.6741, Error: 0.2188\n",
      "Iteration [937], Loss: 0.6419, Error: 0.1667\n",
      "Iteration [938], Loss: 0.7042, Error: 0.2656\n",
      "Iteration [939], Loss: 0.3921, Error: 0.0938\n",
      "Iteration [940], Loss: 0.6222, Error: 0.1719\n",
      "Iteration [941], Loss: 0.4353, Error: 0.1406\n",
      "Iteration [942], Loss: 0.2670, Error: 0.0625\n",
      "Iteration [943], Loss: 0.6215, Error: 0.2031\n",
      "Iteration [944], Loss: 0.8714, Error: 0.2812\n",
      "Iteration [945], Loss: 0.5424, Error: 0.1250\n",
      "Iteration [946], Loss: 0.6011, Error: 0.2031\n",
      "Iteration [947], Loss: 0.4843, Error: 0.1562\n",
      "Iteration [948], Loss: 0.5491, Error: 0.2031\n",
      "Iteration [949], Loss: 0.5585, Error: 0.2031\n",
      "Iteration [950], Loss: 0.5631, Error: 0.1875\n",
      "Iteration [951], Loss: 0.3587, Error: 0.0938\n",
      "Iteration [952], Loss: 0.5660, Error: 0.1719\n",
      "Iteration [953], Loss: 0.6332, Error: 0.1719\n",
      "Iteration [954], Loss: 0.5877, Error: 0.1875\n",
      "Iteration [955], Loss: 0.6969, Error: 0.2188\n",
      "Iteration [956], Loss: 0.6135, Error: 0.2188\n",
      "Iteration [957], Loss: 0.5901, Error: 0.1719\n",
      "Iteration [958], Loss: 0.5010, Error: 0.1875\n",
      "Iteration [959], Loss: 0.6035, Error: 0.2344\n",
      "Iteration [960], Loss: 0.4826, Error: 0.1562\n",
      "Iteration [961], Loss: 0.7122, Error: 0.2500\n",
      "Iteration [962], Loss: 0.5410, Error: 0.1562\n",
      "Iteration [963], Loss: 0.6041, Error: 0.1406\n",
      "Iteration [964], Loss: 0.7978, Error: 0.2656\n",
      "Iteration [965], Loss: 0.8025, Error: 0.2188\n",
      "Iteration [966], Loss: 0.7499, Error: 0.2500\n",
      "Iteration [967], Loss: 0.4043, Error: 0.1094\n",
      "Iteration [968], Loss: 0.5422, Error: 0.1562\n",
      "Iteration [969], Loss: 0.5692, Error: 0.1875\n",
      "Iteration [970], Loss: 0.6739, Error: 0.2344\n",
      "Iteration [971], Loss: 0.5889, Error: 0.1875\n",
      "Iteration [972], Loss: 0.5335, Error: 0.1562\n",
      "Iteration [973], Loss: 0.6018, Error: 0.1719\n",
      "Iteration [974], Loss: 0.8000, Error: 0.2500\n",
      "Iteration [975], Loss: 0.5491, Error: 0.1406\n",
      "Iteration [976], Loss: 0.7042, Error: 0.2188\n",
      "Iteration [977], Loss: 0.7710, Error: 0.2656\n",
      "Iteration [978], Loss: 0.7865, Error: 0.2812\n",
      "Iteration [979], Loss: 0.3427, Error: 0.1094\n",
      "Iteration [980], Loss: 0.6322, Error: 0.2031\n",
      "Iteration [981], Loss: 0.7450, Error: 0.2188\n",
      "Iteration [982], Loss: 0.5890, Error: 0.1719\n",
      "Iteration [983], Loss: 0.4372, Error: 0.1094\n",
      "Iteration [984], Loss: 0.3798, Error: 0.1094\n",
      "Iteration [985], Loss: 1.0481, Error: 0.2969\n",
      "Iteration [986], Loss: 0.5713, Error: 0.1875\n",
      "Iteration [987], Loss: 0.5172, Error: 0.1719\n",
      "Iteration [988], Loss: 0.6763, Error: 0.2031\n",
      "Iteration [989], Loss: 0.4419, Error: 0.1406\n",
      "Iteration [990], Loss: 0.4328, Error: 0.1250\n",
      "Iteration [991], Loss: 0.6098, Error: 0.1875\n",
      "Iteration [992], Loss: 0.5044, Error: 0.1719\n",
      "Iteration [993], Loss: 0.7384, Error: 0.2812\n",
      "Iteration [994], Loss: 0.6127, Error: 0.2031\n",
      "Iteration [995], Loss: 0.3881, Error: 0.1094\n",
      "Iteration [996], Loss: 0.5541, Error: 0.1719\n",
      "Iteration [997], Loss: 0.3565, Error: 0.0938\n",
      "Iteration [998], Loss: 0.6072, Error: 0.1875\n",
      "Iteration [999], Loss: 0.5193, Error: 0.1875\n",
      "Iteration [1000], Loss: 0.5088, Error: 0.1406\n",
      "Iteration [1001], Loss: 0.5336, Error: 0.1562\n",
      "Iteration [1002], Loss: 0.6009, Error: 0.2031\n",
      "Iteration [1003], Loss: 0.3667, Error: 0.1250\n",
      "Iteration [1004], Loss: 0.6896, Error: 0.2188\n",
      "Iteration [1005], Loss: 0.6526, Error: 0.2031\n",
      "Iteration [1006], Loss: 0.9159, Error: 0.3125\n",
      "Iteration [1007], Loss: 0.4395, Error: 0.1875\n",
      "Iteration [1008], Loss: 0.4083, Error: 0.1094\n",
      "Iteration [1009], Loss: 0.5261, Error: 0.1406\n",
      "Iteration [1010], Loss: 0.6762, Error: 0.2031\n",
      "Iteration [1011], Loss: 0.5051, Error: 0.1562\n",
      "Iteration [1012], Loss: 0.5591, Error: 0.1875\n",
      "Iteration [1013], Loss: 0.4136, Error: 0.1250\n",
      "Iteration [1014], Loss: 0.2940, Error: 0.0781\n",
      "Iteration [1015], Loss: 0.4739, Error: 0.1406\n",
      "Iteration [1016], Loss: 0.6043, Error: 0.1875\n",
      "Iteration [1017], Loss: 0.6440, Error: 0.2188\n",
      "Iteration [1018], Loss: 0.4752, Error: 0.1719\n",
      "Iteration [1019], Loss: 0.5495, Error: 0.1406\n",
      "Iteration [1020], Loss: 0.5961, Error: 0.1406\n",
      "Iteration [1021], Loss: 0.4813, Error: 0.1250\n",
      "Iteration [1022], Loss: 0.5335, Error: 0.1875\n",
      "Iteration [1023], Loss: 0.5057, Error: 0.1875\n",
      "Iteration [1024], Loss: 0.5777, Error: 0.1562\n",
      "Iteration [1025], Loss: 0.5502, Error: 0.1719\n",
      "Iteration [1026], Loss: 0.8455, Error: 0.2031\n",
      "Iteration [1027], Loss: 0.6129, Error: 0.2188\n",
      "Iteration [1028], Loss: 0.4731, Error: 0.1406\n",
      "Iteration [1029], Loss: 0.4277, Error: 0.1094\n",
      "Iteration [1030], Loss: 0.5405, Error: 0.1562\n",
      "Iteration [1031], Loss: 0.2794, Error: 0.0781\n",
      "Iteration [1032], Loss: 0.4460, Error: 0.1094\n",
      "Iteration [1033], Loss: 0.2653, Error: 0.0625\n",
      "Iteration [1034], Loss: 0.4072, Error: 0.1094\n",
      "Iteration [1035], Loss: 0.5076, Error: 0.1875\n",
      "Iteration [1036], Loss: 0.6387, Error: 0.2344\n",
      "Iteration [1037], Loss: 0.5488, Error: 0.1406\n",
      "Iteration [1038], Loss: 0.6876, Error: 0.1875\n",
      "Iteration [1039], Loss: 0.7118, Error: 0.2344\n",
      "Iteration [1040], Loss: 0.6735, Error: 0.2188\n",
      "Iteration [1041], Loss: 0.4404, Error: 0.1562\n",
      "Iteration [1042], Loss: 0.7626, Error: 0.2344\n",
      "Iteration [1043], Loss: 0.6268, Error: 0.2031\n",
      "Iteration [1044], Loss: 0.5707, Error: 0.1875\n",
      "Iteration [1045], Loss: 0.5663, Error: 0.1719\n",
      "Iteration [1046], Loss: 0.5989, Error: 0.1719\n",
      "Iteration [1047], Loss: 0.3927, Error: 0.0938\n",
      "Iteration [1048], Loss: 0.4538, Error: 0.1250\n",
      "Iteration [1049], Loss: 0.7449, Error: 0.1875\n",
      "Iteration [1050], Loss: 0.5385, Error: 0.1719\n",
      "Iteration [1051], Loss: 0.6747, Error: 0.2031\n",
      "Iteration [1052], Loss: 0.2113, Error: 0.0781\n",
      "Iteration [1053], Loss: 0.5726, Error: 0.1719\n",
      "Iteration [1054], Loss: 0.5408, Error: 0.1719\n",
      "Iteration [1055], Loss: 0.6603, Error: 0.1719\n",
      "Iteration [1056], Loss: 0.5906, Error: 0.1875\n",
      "Iteration [1057], Loss: 0.6701, Error: 0.2188\n",
      "Iteration [1058], Loss: 0.4553, Error: 0.1250\n",
      "Iteration [1059], Loss: 0.7741, Error: 0.2344\n",
      "Iteration [1060], Loss: 0.8088, Error: 0.2500\n",
      "Iteration [1061], Loss: 0.5944, Error: 0.2031\n",
      "Iteration [1062], Loss: 0.5026, Error: 0.1562\n",
      "Iteration [1063], Loss: 0.6642, Error: 0.2188\n",
      "Iteration [1064], Loss: 0.5571, Error: 0.1719\n",
      "Iteration [1065], Loss: 0.5299, Error: 0.1719\n",
      "Iteration [1066], Loss: 0.4921, Error: 0.1562\n",
      "Iteration [1067], Loss: 0.6754, Error: 0.2031\n",
      "Iteration [1068], Loss: 0.6088, Error: 0.2344\n",
      "Iteration [1069], Loss: 0.6243, Error: 0.2344\n",
      "Iteration [1070], Loss: 0.6067, Error: 0.1875\n",
      "Iteration [1071], Loss: 0.6268, Error: 0.2031\n",
      "Iteration [1072], Loss: 0.4690, Error: 0.1406\n",
      "Iteration [1073], Loss: 0.7270, Error: 0.2031\n",
      "Iteration [1074], Loss: 0.5508, Error: 0.2031\n",
      "Iteration [1075], Loss: 0.8770, Error: 0.3281\n",
      "Iteration [1076], Loss: 0.8900, Error: 0.2969\n",
      "Iteration [1077], Loss: 0.5619, Error: 0.1875\n",
      "Iteration [1078], Loss: 0.4122, Error: 0.1094\n",
      "Iteration [1079], Loss: 0.5562, Error: 0.1406\n",
      "Iteration [1080], Loss: 0.5144, Error: 0.1562\n",
      "Iteration [1081], Loss: 0.4916, Error: 0.1562\n",
      "Iteration [1082], Loss: 0.5054, Error: 0.1250\n",
      "Iteration [1083], Loss: 0.4387, Error: 0.1562\n",
      "Iteration [1084], Loss: 0.5915, Error: 0.2188\n",
      "Iteration [1085], Loss: 0.5420, Error: 0.2188\n",
      "Iteration [1086], Loss: 0.5515, Error: 0.2031\n",
      "Iteration [1087], Loss: 0.5859, Error: 0.1719\n",
      "Iteration [1088], Loss: 0.4962, Error: 0.1406\n",
      "Iteration [1089], Loss: 0.6716, Error: 0.1875\n",
      "Iteration [1090], Loss: 0.4941, Error: 0.1562\n",
      "Iteration [1091], Loss: 0.3831, Error: 0.1250\n",
      "Iteration [1092], Loss: 0.5376, Error: 0.1562\n",
      "Iteration [1093], Loss: 0.6646, Error: 0.2188\n",
      "Iteration [1094], Loss: 0.6354, Error: 0.2031\n",
      "Iteration [1095], Loss: 0.6248, Error: 0.2031\n",
      "Iteration [1096], Loss: 0.7657, Error: 0.2500\n",
      "Iteration [1097], Loss: 0.5737, Error: 0.1875\n",
      "Iteration [1098], Loss: 0.4432, Error: 0.1094\n",
      "Iteration [1099], Loss: 0.5354, Error: 0.2031\n",
      "Iteration [1100], Loss: 0.5238, Error: 0.1719\n",
      "Iteration [1101], Loss: 0.5454, Error: 0.1875\n",
      "Iteration [1102], Loss: 0.5899, Error: 0.1875\n",
      "Iteration [1103], Loss: 0.3157, Error: 0.0938\n",
      "Iteration [1104], Loss: 0.5755, Error: 0.1719\n",
      "Iteration [1105], Loss: 0.5431, Error: 0.1719\n",
      "Iteration [1106], Loss: 0.5610, Error: 0.1562\n",
      "Iteration [1107], Loss: 0.4900, Error: 0.1719\n",
      "Iteration [1108], Loss: 0.4818, Error: 0.1406\n",
      "Iteration [1109], Loss: 0.4509, Error: 0.1562\n",
      "Iteration [1110], Loss: 0.6594, Error: 0.2188\n",
      "Iteration [1111], Loss: 0.4151, Error: 0.1250\n",
      "Iteration [1112], Loss: 0.4204, Error: 0.1406\n",
      "Iteration [1113], Loss: 0.6267, Error: 0.2031\n",
      "Iteration [1114], Loss: 0.6421, Error: 0.2344\n",
      "Iteration [1115], Loss: 0.4667, Error: 0.1719\n",
      "Iteration [1116], Loss: 0.7364, Error: 0.2344\n",
      "Iteration [1117], Loss: 0.5568, Error: 0.1562\n",
      "Iteration [1118], Loss: 0.4909, Error: 0.1719\n",
      "Iteration [1119], Loss: 0.5442, Error: 0.1562\n",
      "Iteration [1120], Loss: 0.7303, Error: 0.2344\n",
      "Iteration [1121], Loss: 0.5569, Error: 0.1719\n",
      "Iteration [1122], Loss: 0.5202, Error: 0.1562\n",
      "Iteration [1123], Loss: 0.3135, Error: 0.0781\n",
      "Iteration [1124], Loss: 0.4381, Error: 0.1250\n",
      "Iteration [1125], Loss: 0.5734, Error: 0.1875\n",
      "Iteration [1126], Loss: 0.8963, Error: 0.2969\n",
      "Iteration [1127], Loss: 0.6440, Error: 0.1875\n",
      "Iteration [1128], Loss: 0.4622, Error: 0.1562\n",
      "Iteration [1129], Loss: 0.4436, Error: 0.1406\n",
      "Iteration [1130], Loss: 0.5858, Error: 0.1875\n",
      "Iteration [1131], Loss: 0.3259, Error: 0.0625\n",
      "Iteration [1132], Loss: 0.6162, Error: 0.2188\n",
      "Iteration [1133], Loss: 0.5275, Error: 0.1875\n",
      "Iteration [1134], Loss: 0.6822, Error: 0.2031\n",
      "Iteration [1135], Loss: 0.5316, Error: 0.1719\n",
      "Iteration [1136], Loss: 0.6549, Error: 0.2344\n",
      "Iteration [1137], Loss: 0.5821, Error: 0.1875\n",
      "Iteration [1138], Loss: 0.4440, Error: 0.1250\n",
      "Iteration [1139], Loss: 0.6190, Error: 0.1875\n",
      "Iteration [1140], Loss: 0.6079, Error: 0.1875\n",
      "Iteration [1141], Loss: 0.5834, Error: 0.1719\n",
      "Iteration [1142], Loss: 0.6314, Error: 0.1875\n",
      "Iteration [1143], Loss: 0.6227, Error: 0.2344\n",
      "Iteration [1144], Loss: 0.4954, Error: 0.1719\n",
      "Iteration [1145], Loss: 0.4192, Error: 0.1406\n",
      "Iteration [1146], Loss: 0.6588, Error: 0.2031\n",
      "Iteration [1147], Loss: 0.5606, Error: 0.1875\n",
      "Iteration [1148], Loss: 0.4615, Error: 0.1875\n",
      "Iteration [1149], Loss: 0.4701, Error: 0.1250\n",
      "Iteration [1150], Loss: 0.5898, Error: 0.1875\n",
      "Iteration [1151], Loss: 0.3998, Error: 0.1250\n",
      "Iteration [1152], Loss: 0.6304, Error: 0.2031\n",
      "Iteration [1153], Loss: 0.4532, Error: 0.1719\n",
      "Iteration [1154], Loss: 0.4976, Error: 0.1562\n",
      "Iteration [1155], Loss: 0.5856, Error: 0.2031\n",
      "Iteration [1156], Loss: 0.4427, Error: 0.1406\n",
      "Iteration [1157], Loss: 0.7128, Error: 0.1719\n",
      "Iteration [1158], Loss: 0.5001, Error: 0.1562\n",
      "Iteration [1159], Loss: 0.3712, Error: 0.0938\n",
      "Iteration [1160], Loss: 0.5639, Error: 0.2031\n",
      "Iteration [1161], Loss: 0.4642, Error: 0.1406\n",
      "Iteration [1162], Loss: 0.5218, Error: 0.1406\n",
      "Iteration [1163], Loss: 0.7711, Error: 0.2344\n",
      "Iteration [1164], Loss: 0.5811, Error: 0.1875\n",
      "Iteration [1165], Loss: 0.4876, Error: 0.1875\n",
      "Iteration [1166], Loss: 0.6609, Error: 0.2188\n",
      "Iteration [1167], Loss: 0.5551, Error: 0.1719\n",
      "Iteration [1168], Loss: 0.3760, Error: 0.0938\n",
      "Iteration [1169], Loss: 0.4948, Error: 0.1562\n",
      "Iteration [1170], Loss: 0.4399, Error: 0.1562\n",
      "Iteration [1171], Loss: 0.5277, Error: 0.1562\n",
      "Iteration [1172], Loss: 0.4964, Error: 0.1406\n",
      "Iteration [1173], Loss: 0.5025, Error: 0.1406\n",
      "Iteration [1174], Loss: 0.5300, Error: 0.1562\n",
      "Iteration [1175], Loss: 0.6780, Error: 0.2500\n",
      "Iteration [1176], Loss: 0.4617, Error: 0.1406\n",
      "Iteration [1177], Loss: 0.4276, Error: 0.1250\n",
      "Iteration [1178], Loss: 0.3784, Error: 0.0938\n",
      "Iteration [1179], Loss: 0.4673, Error: 0.1719\n",
      "Iteration [1180], Loss: 0.6592, Error: 0.2188\n",
      "Iteration [1181], Loss: 0.5256, Error: 0.1875\n",
      "Iteration [1182], Loss: 0.3886, Error: 0.1094\n",
      "Iteration [1183], Loss: 0.5614, Error: 0.2188\n",
      "Iteration [1184], Loss: 0.6549, Error: 0.2031\n",
      "Iteration [1185], Loss: 0.4844, Error: 0.1250\n",
      "Iteration [1186], Loss: 0.3384, Error: 0.0938\n",
      "Iteration [1187], Loss: 0.4338, Error: 0.1250\n",
      "Iteration [1188], Loss: 0.4387, Error: 0.1406\n",
      "Iteration [1189], Loss: 0.6027, Error: 0.1875\n",
      "Iteration [1190], Loss: 0.6240, Error: 0.2188\n",
      "Iteration [1191], Loss: 0.2450, Error: 0.0469\n",
      "Iteration [1192], Loss: 0.4353, Error: 0.0938\n",
      "Iteration [1193], Loss: 0.5341, Error: 0.1875\n",
      "Iteration [1194], Loss: 0.6282, Error: 0.2188\n",
      "Iteration [1195], Loss: 0.5561, Error: 0.2031\n",
      "Iteration [1196], Loss: 0.5071, Error: 0.1562\n",
      "Iteration [1197], Loss: 0.4976, Error: 0.1562\n",
      "Iteration [1198], Loss: 0.5974, Error: 0.2031\n",
      "Iteration [1199], Loss: 0.4930, Error: 0.1406\n",
      "Iteration [1200], Loss: 0.4237, Error: 0.1250\n",
      "Iteration [1201], Loss: 0.4852, Error: 0.1875\n",
      "Iteration [1202], Loss: 0.5671, Error: 0.1875\n",
      "Iteration [1203], Loss: 0.6650, Error: 0.2031\n",
      "Iteration [1204], Loss: 0.5768, Error: 0.1406\n",
      "Iteration [1205], Loss: 0.4592, Error: 0.1406\n",
      "Iteration [1206], Loss: 0.5462, Error: 0.1875\n",
      "Iteration [1207], Loss: 0.4782, Error: 0.1719\n",
      "Iteration [1208], Loss: 0.5912, Error: 0.1719\n",
      "Iteration [1209], Loss: 0.5669, Error: 0.1719\n",
      "Iteration [1210], Loss: 0.6222, Error: 0.1406\n",
      "Iteration [1211], Loss: 0.4587, Error: 0.1250\n",
      "Iteration [1212], Loss: 0.5791, Error: 0.1719\n",
      "Iteration [1213], Loss: 0.4744, Error: 0.1406\n",
      "Iteration [1214], Loss: 0.5002, Error: 0.1406\n",
      "Iteration [1215], Loss: 0.5184, Error: 0.1562\n",
      "Iteration [1216], Loss: 0.5492, Error: 0.1875\n",
      "Iteration [1217], Loss: 0.3978, Error: 0.1250\n",
      "Iteration [1218], Loss: 0.6611, Error: 0.2031\n",
      "Iteration [1219], Loss: 0.5486, Error: 0.1719\n",
      "Iteration [1220], Loss: 0.5842, Error: 0.1875\n",
      "Iteration [1221], Loss: 0.5741, Error: 0.1562\n",
      "Iteration [1222], Loss: 0.4414, Error: 0.1094\n",
      "Iteration [1223], Loss: 0.6069, Error: 0.2188\n",
      "Iteration [1224], Loss: 0.7098, Error: 0.2344\n",
      "Iteration [1225], Loss: 0.4005, Error: 0.1562\n",
      "Iteration [1226], Loss: 0.4826, Error: 0.1875\n",
      "Iteration [1227], Loss: 0.6647, Error: 0.2188\n",
      "Iteration [1228], Loss: 0.4852, Error: 0.1406\n",
      "Iteration [1229], Loss: 0.6990, Error: 0.1875\n",
      "Iteration [1230], Loss: 0.4978, Error: 0.1562\n",
      "Iteration [1231], Loss: 0.6322, Error: 0.2500\n",
      "Iteration [1232], Loss: 0.5070, Error: 0.1562\n",
      "Iteration [1233], Loss: 0.6230, Error: 0.2344\n",
      "Iteration [1234], Loss: 0.4085, Error: 0.1094\n",
      "Iteration [1235], Loss: 0.5588, Error: 0.1719\n",
      "Iteration [1236], Loss: 0.5678, Error: 0.1875\n",
      "Iteration [1237], Loss: 0.6179, Error: 0.2344\n",
      "Iteration [1238], Loss: 0.6543, Error: 0.2344\n",
      "Iteration [1239], Loss: 0.4899, Error: 0.1719\n",
      "Iteration [1240], Loss: 0.5762, Error: 0.1875\n",
      "Iteration [1241], Loss: 0.3770, Error: 0.1094\n",
      "Iteration [1242], Loss: 0.5613, Error: 0.1875\n",
      "Iteration [1243], Loss: 0.5930, Error: 0.2188\n",
      "Iteration [1244], Loss: 0.5641, Error: 0.2031\n",
      "Iteration [1245], Loss: 0.6036, Error: 0.2031\n",
      "Iteration [1246], Loss: 0.4099, Error: 0.0938\n",
      "Iteration [1247], Loss: 0.5601, Error: 0.1875\n",
      "Iteration [1248], Loss: 0.5212, Error: 0.1875\n",
      "Iteration [1249], Loss: 0.5270, Error: 0.1875\n",
      "Iteration [1250], Loss: 0.4169, Error: 0.1094\n",
      "Iteration [1251], Loss: 0.4485, Error: 0.1406\n",
      "Iteration [1252], Loss: 0.4579, Error: 0.1250\n",
      "Iteration [1253], Loss: 0.5985, Error: 0.1875\n",
      "Iteration [1254], Loss: 0.5802, Error: 0.1875\n",
      "Iteration [1255], Loss: 0.6944, Error: 0.2500\n",
      "Iteration [1256], Loss: 0.5947, Error: 0.1875\n",
      "Iteration [1257], Loss: 0.4328, Error: 0.1719\n",
      "Iteration [1258], Loss: 0.4691, Error: 0.1562\n",
      "Iteration [1259], Loss: 0.6080, Error: 0.1875\n",
      "Iteration [1260], Loss: 0.5199, Error: 0.1875\n",
      "Iteration [1261], Loss: 0.6632, Error: 0.1719\n",
      "Iteration [1262], Loss: 0.5444, Error: 0.1562\n",
      "Iteration [1263], Loss: 0.5391, Error: 0.1719\n",
      "Iteration [1264], Loss: 0.3309, Error: 0.1094\n",
      "Iteration [1265], Loss: 0.5236, Error: 0.1562\n",
      "Iteration [1266], Loss: 0.7544, Error: 0.2344\n",
      "Iteration [1267], Loss: 0.4852, Error: 0.1719\n",
      "Iteration [1268], Loss: 0.9175, Error: 0.2656\n",
      "Iteration [1269], Loss: 0.5395, Error: 0.1875\n",
      "Iteration [1270], Loss: 0.4375, Error: 0.1406\n",
      "Iteration [1271], Loss: 0.5353, Error: 0.1562\n",
      "Iteration [1272], Loss: 0.6545, Error: 0.1875\n",
      "Iteration [1273], Loss: 0.5983, Error: 0.2188\n",
      "Iteration [1274], Loss: 0.5191, Error: 0.1719\n",
      "Iteration [1275], Loss: 0.6326, Error: 0.2188\n",
      "Iteration [1276], Loss: 0.6302, Error: 0.2500\n",
      "Iteration [1277], Loss: 0.5728, Error: 0.1719\n",
      "Iteration [1278], Loss: 0.5391, Error: 0.2188\n",
      "Iteration [1279], Loss: 0.5904, Error: 0.1406\n",
      "Iteration [1280], Loss: 0.4820, Error: 0.1562\n",
      "Iteration [1281], Loss: 0.5188, Error: 0.1562\n",
      "Iteration [1282], Loss: 0.7052, Error: 0.2188\n",
      "Iteration [1283], Loss: 0.6143, Error: 0.2344\n",
      "Iteration [1284], Loss: 0.6191, Error: 0.1719\n",
      "Iteration [1285], Loss: 0.6250, Error: 0.2031\n",
      "Iteration [1286], Loss: 0.7826, Error: 0.2656\n",
      "Iteration [1287], Loss: 0.7533, Error: 0.2344\n",
      "Iteration [1288], Loss: 0.5932, Error: 0.1562\n",
      "Iteration [1289], Loss: 0.4587, Error: 0.1406\n",
      "Iteration [1290], Loss: 0.5286, Error: 0.1719\n",
      "Iteration [1291], Loss: 0.8516, Error: 0.2656\n",
      "Iteration [1292], Loss: 0.5445, Error: 0.1562\n",
      "Iteration [1293], Loss: 0.4305, Error: 0.1250\n",
      "Iteration [1294], Loss: 0.5944, Error: 0.1875\n",
      "Iteration [1295], Loss: 0.5473, Error: 0.1562\n",
      "Iteration [1296], Loss: 0.6891, Error: 0.2812\n",
      "Iteration [1297], Loss: 0.6444, Error: 0.1875\n",
      "Iteration [1298], Loss: 0.7172, Error: 0.2188\n",
      "Iteration [1299], Loss: 0.7151, Error: 0.2500\n",
      "Iteration [1300], Loss: 0.6384, Error: 0.2500\n",
      "Iteration [1301], Loss: 0.5355, Error: 0.1406\n",
      "Iteration [1302], Loss: 0.5748, Error: 0.1406\n",
      "Iteration [1303], Loss: 0.3174, Error: 0.1094\n",
      "Iteration [1304], Loss: 0.4035, Error: 0.1562\n",
      "Iteration [1305], Loss: 0.3614, Error: 0.0781\n",
      "Iteration [1306], Loss: 0.7122, Error: 0.2188\n",
      "Iteration [1307], Loss: 0.5142, Error: 0.1406\n",
      "Iteration [1308], Loss: 0.5787, Error: 0.2188\n",
      "Iteration [1309], Loss: 0.4874, Error: 0.1406\n",
      "Iteration [1310], Loss: 0.6627, Error: 0.2344\n",
      "Iteration [1311], Loss: 0.5038, Error: 0.1406\n",
      "Iteration [1312], Loss: 0.5996, Error: 0.1875\n",
      "Iteration [1313], Loss: 0.6060, Error: 0.1719\n",
      "Iteration [1314], Loss: 0.4832, Error: 0.1406\n",
      "Iteration [1315], Loss: 0.8658, Error: 0.2188\n",
      "Iteration [1316], Loss: 0.7945, Error: 0.2500\n",
      "Iteration [1317], Loss: 0.3345, Error: 0.0938\n",
      "Iteration [1318], Loss: 0.5912, Error: 0.1562\n",
      "Iteration [1319], Loss: 0.5262, Error: 0.1250\n",
      "Iteration [1320], Loss: 0.7128, Error: 0.2656\n",
      "Iteration [1321], Loss: 0.6854, Error: 0.2500\n",
      "Iteration [1322], Loss: 0.6011, Error: 0.1875\n",
      "Iteration [1323], Loss: 0.5852, Error: 0.1875\n",
      "Iteration [1324], Loss: 0.6249, Error: 0.1875\n",
      "Iteration [1325], Loss: 0.6512, Error: 0.2344\n",
      "Iteration [1326], Loss: 0.4289, Error: 0.1562\n",
      "Iteration [1327], Loss: 0.4248, Error: 0.1562\n",
      "Iteration [1328], Loss: 0.4693, Error: 0.1406\n",
      "Iteration [1329], Loss: 0.7463, Error: 0.2344\n",
      "Iteration [1330], Loss: 0.5683, Error: 0.1719\n",
      "Iteration [1331], Loss: 0.2875, Error: 0.0625\n",
      "Iteration [1332], Loss: 0.5106, Error: 0.1719\n",
      "Iteration [1333], Loss: 0.8577, Error: 0.2500\n",
      "Iteration [1334], Loss: 0.5571, Error: 0.1406\n",
      "Iteration [1335], Loss: 0.4971, Error: 0.1562\n",
      "Iteration [1336], Loss: 0.4454, Error: 0.1562\n",
      "Iteration [1337], Loss: 0.5521, Error: 0.1875\n",
      "Iteration [1338], Loss: 0.5593, Error: 0.1719\n",
      "Iteration [1339], Loss: 0.5296, Error: 0.1875\n",
      "Iteration [1340], Loss: 0.7453, Error: 0.2656\n",
      "Iteration [1341], Loss: 0.7207, Error: 0.2344\n",
      "Iteration [1342], Loss: 0.4885, Error: 0.1406\n",
      "Iteration [1343], Loss: 0.4708, Error: 0.1406\n",
      "Iteration [1344], Loss: 0.6496, Error: 0.2500\n",
      "Iteration [1345], Loss: 0.5489, Error: 0.1719\n",
      "Iteration [1346], Loss: 0.5888, Error: 0.2031\n",
      "Iteration [1347], Loss: 0.7652, Error: 0.3125\n",
      "Iteration [1348], Loss: 0.4554, Error: 0.1562\n",
      "Iteration [1349], Loss: 0.5896, Error: 0.1875\n",
      "Iteration [1350], Loss: 0.5463, Error: 0.1562\n",
      "Iteration [1351], Loss: 0.6056, Error: 0.1875\n",
      "Iteration [1352], Loss: 0.6202, Error: 0.2188\n",
      "Iteration [1353], Loss: 0.4492, Error: 0.1406\n",
      "Iteration [1354], Loss: 0.7103, Error: 0.2344\n",
      "Iteration [1355], Loss: 0.5927, Error: 0.2031\n",
      "Iteration [1356], Loss: 0.7223, Error: 0.2188\n",
      "Iteration [1357], Loss: 0.5954, Error: 0.1719\n",
      "Iteration [1358], Loss: 0.4561, Error: 0.1562\n",
      "Iteration [1359], Loss: 0.4853, Error: 0.1406\n",
      "Iteration [1360], Loss: 0.3431, Error: 0.0938\n",
      "Iteration [1361], Loss: 0.5546, Error: 0.1719\n",
      "Iteration [1362], Loss: 0.4291, Error: 0.1250\n",
      "Iteration [1363], Loss: 0.4139, Error: 0.1250\n",
      "Iteration [1364], Loss: 0.4733, Error: 0.1094\n",
      "Iteration [1365], Loss: 0.3583, Error: 0.1094\n",
      "Iteration [1366], Loss: 0.6856, Error: 0.2188\n",
      "Iteration [1367], Loss: 0.8495, Error: 0.2969\n",
      "Iteration [1368], Loss: 0.5403, Error: 0.1719\n",
      "Iteration [1369], Loss: 0.5087, Error: 0.1406\n",
      "Iteration [1370], Loss: 0.4947, Error: 0.1250\n",
      "Iteration [1371], Loss: 0.8394, Error: 0.2812\n",
      "Iteration [1372], Loss: 0.3320, Error: 0.0781\n",
      "Iteration [1373], Loss: 0.5247, Error: 0.1719\n",
      "Iteration [1374], Loss: 0.8510, Error: 0.2969\n",
      "Iteration [1375], Loss: 0.5850, Error: 0.2188\n",
      "Iteration [1376], Loss: 0.5885, Error: 0.1875\n",
      "Iteration [1377], Loss: 0.6121, Error: 0.1875\n",
      "Iteration [1378], Loss: 0.5732, Error: 0.1875\n",
      "Iteration [1379], Loss: 0.5408, Error: 0.2188\n",
      "Iteration [1380], Loss: 0.6662, Error: 0.1875\n",
      "Iteration [1381], Loss: 0.4718, Error: 0.1719\n",
      "Iteration [1382], Loss: 0.6191, Error: 0.2188\n",
      "Iteration [1383], Loss: 0.7656, Error: 0.2344\n",
      "Iteration [1384], Loss: 0.3954, Error: 0.1094\n",
      "Iteration [1385], Loss: 0.3950, Error: 0.1094\n",
      "Iteration [1386], Loss: 0.6502, Error: 0.1719\n",
      "Iteration [1387], Loss: 0.7336, Error: 0.2656\n",
      "Iteration [1388], Loss: 0.9615, Error: 0.2812\n",
      "Iteration [1389], Loss: 0.4922, Error: 0.2188\n",
      "Iteration [1390], Loss: 0.4240, Error: 0.1406\n",
      "Iteration [1391], Loss: 0.6004, Error: 0.2188\n",
      "Iteration [1392], Loss: 0.6342, Error: 0.2188\n",
      "Iteration [1393], Loss: 0.7636, Error: 0.2500\n",
      "Iteration [1394], Loss: 0.5579, Error: 0.1562\n",
      "Iteration [1395], Loss: 0.6610, Error: 0.2031\n",
      "Iteration [1396], Loss: 0.6643, Error: 0.2031\n",
      "Iteration [1397], Loss: 0.2818, Error: 0.0625\n",
      "Iteration [1398], Loss: 0.4750, Error: 0.1562\n",
      "Iteration [1399], Loss: 0.6447, Error: 0.2188\n",
      "Iteration [1400], Loss: 0.5964, Error: 0.2188\n",
      "Iteration [1401], Loss: 0.5665, Error: 0.1875\n",
      "Iteration [1402], Loss: 0.4878, Error: 0.1406\n",
      "Iteration [1403], Loss: 0.4827, Error: 0.1406\n",
      "Iteration [1404], Loss: 0.6249, Error: 0.2031\n",
      "Iteration [1405], Loss: 0.4572, Error: 0.1719\n",
      "Iteration [1406], Loss: 0.4179, Error: 0.1042\n",
      "Iteration [1407], Loss: 0.4662, Error: 0.1562\n",
      "Iteration [1408], Loss: 0.8525, Error: 0.3125\n",
      "Iteration [1409], Loss: 0.3552, Error: 0.0938\n",
      "Iteration [1410], Loss: 0.5043, Error: 0.1406\n",
      "Iteration [1411], Loss: 0.6067, Error: 0.2188\n",
      "Iteration [1412], Loss: 0.6553, Error: 0.2344\n",
      "Iteration [1413], Loss: 0.4194, Error: 0.1094\n",
      "Iteration [1414], Loss: 0.6754, Error: 0.2188\n",
      "Iteration [1415], Loss: 0.4133, Error: 0.1562\n",
      "Iteration [1416], Loss: 0.6241, Error: 0.2188\n",
      "Iteration [1417], Loss: 0.4303, Error: 0.1562\n",
      "Iteration [1418], Loss: 0.5924, Error: 0.1719\n",
      "Iteration [1419], Loss: 0.5960, Error: 0.1406\n",
      "Iteration [1420], Loss: 0.5903, Error: 0.2031\n",
      "Iteration [1421], Loss: 0.6302, Error: 0.2031\n",
      "Iteration [1422], Loss: 0.3718, Error: 0.0938\n",
      "Iteration [1423], Loss: 0.5058, Error: 0.1562\n",
      "Iteration [1424], Loss: 0.5242, Error: 0.1562\n",
      "Iteration [1425], Loss: 0.4986, Error: 0.1562\n",
      "Iteration [1426], Loss: 0.5965, Error: 0.2031\n",
      "Iteration [1427], Loss: 0.6028, Error: 0.2031\n",
      "Iteration [1428], Loss: 0.5336, Error: 0.1406\n",
      "Iteration [1429], Loss: 0.6664, Error: 0.2031\n",
      "Iteration [1430], Loss: 0.5243, Error: 0.1719\n",
      "Iteration [1431], Loss: 0.6241, Error: 0.1719\n",
      "Iteration [1432], Loss: 0.4850, Error: 0.1562\n",
      "Iteration [1433], Loss: 0.5240, Error: 0.1562\n",
      "Iteration [1434], Loss: 0.6042, Error: 0.2031\n",
      "Iteration [1435], Loss: 0.4135, Error: 0.1250\n",
      "Iteration [1436], Loss: 0.7158, Error: 0.2344\n",
      "Iteration [1437], Loss: 0.5063, Error: 0.1406\n",
      "Iteration [1438], Loss: 0.3321, Error: 0.0781\n",
      "Iteration [1439], Loss: 0.5055, Error: 0.1719\n",
      "Iteration [1440], Loss: 0.7640, Error: 0.2656\n",
      "Iteration [1441], Loss: 0.5354, Error: 0.1719\n",
      "Iteration [1442], Loss: 0.6596, Error: 0.2031\n",
      "Iteration [1443], Loss: 0.7355, Error: 0.2344\n",
      "Iteration [1444], Loss: 0.6100, Error: 0.2031\n",
      "Iteration [1445], Loss: 0.5041, Error: 0.1562\n",
      "Iteration [1446], Loss: 0.5487, Error: 0.1406\n",
      "Iteration [1447], Loss: 0.5706, Error: 0.1875\n",
      "Iteration [1448], Loss: 0.4941, Error: 0.1406\n",
      "Iteration [1449], Loss: 0.7925, Error: 0.2344\n",
      "Iteration [1450], Loss: 0.4292, Error: 0.1250\n",
      "Iteration [1451], Loss: 0.5742, Error: 0.1562\n",
      "Iteration [1452], Loss: 0.4069, Error: 0.1094\n",
      "Iteration [1453], Loss: 0.6398, Error: 0.2188\n",
      "Iteration [1454], Loss: 0.6194, Error: 0.2344\n",
      "Iteration [1455], Loss: 0.5553, Error: 0.2031\n",
      "Iteration [1456], Loss: 0.3355, Error: 0.1094\n",
      "Iteration [1457], Loss: 0.5016, Error: 0.1875\n",
      "Iteration [1458], Loss: 0.5405, Error: 0.2031\n",
      "Iteration [1459], Loss: 0.4514, Error: 0.1250\n",
      "Iteration [1460], Loss: 0.5756, Error: 0.2031\n",
      "Iteration [1461], Loss: 0.4208, Error: 0.1250\n",
      "Iteration [1462], Loss: 0.3580, Error: 0.1094\n",
      "Iteration [1463], Loss: 0.7273, Error: 0.2500\n",
      "Iteration [1464], Loss: 0.7037, Error: 0.2031\n",
      "Iteration [1465], Loss: 0.7796, Error: 0.2500\n",
      "Iteration [1466], Loss: 0.3562, Error: 0.1250\n",
      "Iteration [1467], Loss: 0.4347, Error: 0.1406\n",
      "Iteration [1468], Loss: 0.3748, Error: 0.1094\n",
      "Iteration [1469], Loss: 0.4525, Error: 0.1719\n",
      "Iteration [1470], Loss: 0.4490, Error: 0.1250\n",
      "Iteration [1471], Loss: 0.8767, Error: 0.2656\n",
      "Iteration [1472], Loss: 0.4781, Error: 0.1406\n",
      "Iteration [1473], Loss: 0.7322, Error: 0.2344\n",
      "Iteration [1474], Loss: 0.4289, Error: 0.1250\n",
      "Iteration [1475], Loss: 0.5227, Error: 0.1719\n",
      "Iteration [1476], Loss: 0.5983, Error: 0.1875\n",
      "Iteration [1477], Loss: 0.6527, Error: 0.2344\n",
      "Iteration [1478], Loss: 0.5460, Error: 0.1719\n",
      "Iteration [1479], Loss: 0.5874, Error: 0.1875\n",
      "Iteration [1480], Loss: 0.8875, Error: 0.2500\n",
      "Iteration [1481], Loss: 0.7048, Error: 0.2656\n",
      "Iteration [1482], Loss: 0.4745, Error: 0.1406\n",
      "Iteration [1483], Loss: 0.5866, Error: 0.2188\n",
      "Iteration [1484], Loss: 0.5313, Error: 0.1719\n",
      "Iteration [1485], Loss: 0.5248, Error: 0.1875\n",
      "Iteration [1486], Loss: 0.4962, Error: 0.1562\n",
      "Iteration [1487], Loss: 0.5424, Error: 0.1875\n",
      "Iteration [1488], Loss: 0.3841, Error: 0.1406\n",
      "Iteration [1489], Loss: 0.6504, Error: 0.2188\n",
      "Iteration [1490], Loss: 0.4140, Error: 0.1562\n",
      "Iteration [1491], Loss: 0.5363, Error: 0.1719\n",
      "Iteration [1492], Loss: 0.4723, Error: 0.1719\n",
      "Iteration [1493], Loss: 0.5457, Error: 0.2188\n",
      "Iteration [1494], Loss: 0.6212, Error: 0.2344\n",
      "Iteration [1495], Loss: 0.5990, Error: 0.2031\n",
      "Iteration [1496], Loss: 0.5768, Error: 0.2188\n",
      "Iteration [1497], Loss: 0.6222, Error: 0.1875\n",
      "Iteration [1498], Loss: 0.4986, Error: 0.1406\n",
      "Iteration [1499], Loss: 0.4370, Error: 0.1406\n",
      "Iteration [1500], Loss: 0.5201, Error: 0.1719\n",
      "Iteration [1501], Loss: 0.4068, Error: 0.1406\n",
      "Iteration [1502], Loss: 0.5404, Error: 0.1719\n",
      "Iteration [1503], Loss: 0.7250, Error: 0.2344\n",
      "Iteration [1504], Loss: 0.4450, Error: 0.1406\n",
      "Iteration [1505], Loss: 0.3566, Error: 0.0938\n",
      "Iteration [1506], Loss: 0.4736, Error: 0.1406\n",
      "Iteration [1507], Loss: 0.4570, Error: 0.1562\n",
      "Iteration [1508], Loss: 0.5749, Error: 0.2031\n",
      "Iteration [1509], Loss: 0.3632, Error: 0.1250\n",
      "Iteration [1510], Loss: 0.5960, Error: 0.2188\n",
      "Iteration [1511], Loss: 0.3958, Error: 0.1406\n",
      "Iteration [1512], Loss: 0.7131, Error: 0.2188\n",
      "Iteration [1513], Loss: 0.5091, Error: 0.2031\n",
      "Iteration [1514], Loss: 0.2720, Error: 0.0938\n",
      "Iteration [1515], Loss: 0.5040, Error: 0.1406\n",
      "Iteration [1516], Loss: 0.4433, Error: 0.1406\n",
      "Iteration [1517], Loss: 0.6324, Error: 0.2031\n",
      "Iteration [1518], Loss: 0.5280, Error: 0.1719\n",
      "Iteration [1519], Loss: 0.4501, Error: 0.1875\n",
      "Iteration [1520], Loss: 0.6544, Error: 0.2344\n",
      "Iteration [1521], Loss: 0.3167, Error: 0.0938\n",
      "Iteration [1522], Loss: 0.4286, Error: 0.1406\n",
      "Iteration [1523], Loss: 0.8189, Error: 0.2812\n",
      "Iteration [1524], Loss: 0.5978, Error: 0.1719\n",
      "Iteration [1525], Loss: 0.6299, Error: 0.2031\n",
      "Iteration [1526], Loss: 0.4760, Error: 0.1250\n",
      "Iteration [1527], Loss: 0.7970, Error: 0.2188\n",
      "Iteration [1528], Loss: 0.5575, Error: 0.1719\n",
      "Iteration [1529], Loss: 0.6573, Error: 0.2031\n",
      "Iteration [1530], Loss: 0.6213, Error: 0.1875\n",
      "Iteration [1531], Loss: 0.5418, Error: 0.1719\n",
      "Iteration [1532], Loss: 0.2789, Error: 0.0625\n",
      "Iteration [1533], Loss: 0.4037, Error: 0.1406\n",
      "Iteration [1534], Loss: 0.6601, Error: 0.2344\n",
      "Iteration [1535], Loss: 0.7408, Error: 0.2344\n",
      "Iteration [1536], Loss: 0.6332, Error: 0.1875\n",
      "Iteration [1537], Loss: 0.3588, Error: 0.1094\n",
      "Iteration [1538], Loss: 0.6027, Error: 0.2188\n",
      "Iteration [1539], Loss: 0.4758, Error: 0.2188\n",
      "Iteration [1540], Loss: 0.5581, Error: 0.2031\n",
      "Iteration [1541], Loss: 0.4706, Error: 0.1719\n",
      "Iteration [1542], Loss: 0.6137, Error: 0.2344\n",
      "Iteration [1543], Loss: 0.4984, Error: 0.1406\n",
      "Iteration [1544], Loss: 0.3354, Error: 0.0625\n",
      "Iteration [1545], Loss: 0.5710, Error: 0.2031\n",
      "Iteration [1546], Loss: 0.7500, Error: 0.2656\n",
      "Iteration [1547], Loss: 0.5891, Error: 0.2031\n",
      "Iteration [1548], Loss: 0.5678, Error: 0.2188\n",
      "Iteration [1549], Loss: 0.5484, Error: 0.1719\n",
      "Iteration [1550], Loss: 0.4852, Error: 0.1250\n",
      "Iteration [1551], Loss: 0.5400, Error: 0.1719\n",
      "Iteration [1552], Loss: 0.6626, Error: 0.1875\n",
      "Iteration [1553], Loss: 0.6534, Error: 0.2344\n",
      "Iteration [1554], Loss: 0.4449, Error: 0.1406\n",
      "Iteration [1555], Loss: 0.7012, Error: 0.1875\n",
      "Iteration [1556], Loss: 0.6795, Error: 0.2188\n",
      "Iteration [1557], Loss: 0.5574, Error: 0.1719\n",
      "Iteration [1558], Loss: 0.5590, Error: 0.1875\n",
      "Iteration [1559], Loss: 0.4863, Error: 0.1406\n",
      "Iteration [1560], Loss: 0.7068, Error: 0.2500\n",
      "Iteration [1561], Loss: 0.3906, Error: 0.1094\n",
      "Iteration [1562], Loss: 0.5805, Error: 0.1875\n",
      "Iteration [1563], Loss: 0.3869, Error: 0.1250\n",
      "Iteration [1564], Loss: 0.6149, Error: 0.2031\n",
      "Iteration [1565], Loss: 0.3110, Error: 0.0781\n",
      "Iteration [1566], Loss: 0.7889, Error: 0.2500\n",
      "Iteration [1567], Loss: 0.4188, Error: 0.1250\n",
      "Iteration [1568], Loss: 0.5676, Error: 0.1562\n",
      "Iteration [1569], Loss: 0.3930, Error: 0.1250\n",
      "Iteration [1570], Loss: 0.3725, Error: 0.0781\n",
      "Iteration [1571], Loss: 0.4058, Error: 0.1406\n",
      "Iteration [1572], Loss: 0.4718, Error: 0.1406\n",
      "Iteration [1573], Loss: 0.5816, Error: 0.1719\n",
      "Iteration [1574], Loss: 0.3482, Error: 0.0938\n",
      "Iteration [1575], Loss: 0.6398, Error: 0.2031\n",
      "Iteration [1576], Loss: 0.4814, Error: 0.1250\n",
      "Iteration [1577], Loss: 0.4897, Error: 0.1562\n",
      "Iteration [1578], Loss: 0.5002, Error: 0.1406\n",
      "Iteration [1579], Loss: 0.4961, Error: 0.1875\n",
      "Iteration [1580], Loss: 0.7604, Error: 0.2500\n",
      "Iteration [1581], Loss: 0.5427, Error: 0.1875\n",
      "Iteration [1582], Loss: 0.4271, Error: 0.1406\n",
      "Iteration [1583], Loss: 0.3704, Error: 0.1406\n",
      "Iteration [1584], Loss: 0.4035, Error: 0.1250\n",
      "Iteration [1585], Loss: 0.6517, Error: 0.2344\n",
      "Iteration [1586], Loss: 0.5051, Error: 0.1406\n",
      "Iteration [1587], Loss: 0.4318, Error: 0.1250\n",
      "Iteration [1588], Loss: 0.4524, Error: 0.1562\n",
      "Iteration [1589], Loss: 0.4970, Error: 0.1250\n",
      "Iteration [1590], Loss: 0.4350, Error: 0.1406\n",
      "Iteration [1591], Loss: 0.6381, Error: 0.2188\n",
      "Iteration [1592], Loss: 0.5395, Error: 0.2031\n",
      "Iteration [1593], Loss: 0.7149, Error: 0.2656\n",
      "Iteration [1594], Loss: 0.3038, Error: 0.0781\n",
      "Iteration [1595], Loss: 0.5613, Error: 0.1719\n",
      "Iteration [1596], Loss: 0.4786, Error: 0.1875\n",
      "Iteration [1597], Loss: 0.5349, Error: 0.1562\n",
      "Iteration [1598], Loss: 0.5577, Error: 0.1875\n",
      "Iteration [1599], Loss: 0.7423, Error: 0.2188\n",
      "Iteration [1600], Loss: 0.4666, Error: 0.1562\n",
      "Iteration [1601], Loss: 0.5036, Error: 0.1250\n",
      "Iteration [1602], Loss: 0.6035, Error: 0.1875\n",
      "Iteration [1603], Loss: 0.6030, Error: 0.2188\n",
      "Iteration [1604], Loss: 0.4138, Error: 0.1406\n",
      "Iteration [1605], Loss: 0.4540, Error: 0.1250\n",
      "Iteration [1606], Loss: 0.7370, Error: 0.2656\n",
      "Iteration [1607], Loss: 0.7001, Error: 0.2031\n",
      "Iteration [1608], Loss: 0.4947, Error: 0.1875\n",
      "Iteration [1609], Loss: 0.6480, Error: 0.2188\n",
      "Iteration [1610], Loss: 0.4985, Error: 0.1562\n",
      "Iteration [1611], Loss: 0.7425, Error: 0.2188\n",
      "Iteration [1612], Loss: 0.4903, Error: 0.1562\n",
      "Iteration [1613], Loss: 0.4294, Error: 0.1250\n",
      "Iteration [1614], Loss: 0.6231, Error: 0.2188\n",
      "Iteration [1615], Loss: 0.4511, Error: 0.1562\n",
      "Iteration [1616], Loss: 0.6467, Error: 0.1719\n",
      "Iteration [1617], Loss: 0.5976, Error: 0.1719\n",
      "Iteration [1618], Loss: 0.5999, Error: 0.1719\n",
      "Iteration [1619], Loss: 0.4674, Error: 0.1562\n",
      "Iteration [1620], Loss: 0.2979, Error: 0.1094\n",
      "Iteration [1621], Loss: 0.7089, Error: 0.2500\n",
      "Iteration [1622], Loss: 0.5633, Error: 0.2031\n",
      "Iteration [1623], Loss: 0.5772, Error: 0.2031\n",
      "Iteration [1624], Loss: 0.7153, Error: 0.2188\n",
      "Iteration [1625], Loss: 0.4923, Error: 0.1719\n",
      "Iteration [1626], Loss: 0.6076, Error: 0.2031\n",
      "Iteration [1627], Loss: 0.6444, Error: 0.1875\n",
      "Iteration [1628], Loss: 0.6095, Error: 0.2031\n",
      "Iteration [1629], Loss: 0.4889, Error: 0.1719\n",
      "Iteration [1630], Loss: 0.3281, Error: 0.0938\n",
      "Iteration [1631], Loss: 0.3979, Error: 0.0938\n",
      "Iteration [1632], Loss: 0.4561, Error: 0.1250\n",
      "Iteration [1633], Loss: 0.7235, Error: 0.2500\n",
      "Iteration [1634], Loss: 0.7382, Error: 0.2344\n",
      "Iteration [1635], Loss: 0.3428, Error: 0.0938\n",
      "Iteration [1636], Loss: 0.5212, Error: 0.1562\n",
      "Iteration [1637], Loss: 0.5022, Error: 0.1562\n",
      "Iteration [1638], Loss: 0.5755, Error: 0.1875\n",
      "Iteration [1639], Loss: 0.6864, Error: 0.2188\n",
      "Iteration [1640], Loss: 0.5969, Error: 0.1875\n",
      "Iteration [1641], Loss: 0.5778, Error: 0.2031\n",
      "Iteration [1642], Loss: 0.3672, Error: 0.1094\n",
      "Iteration [1643], Loss: 0.6339, Error: 0.1562\n",
      "Iteration [1644], Loss: 0.6059, Error: 0.2031\n",
      "Iteration [1645], Loss: 0.6677, Error: 0.2500\n",
      "Iteration [1646], Loss: 0.3166, Error: 0.0781\n",
      "Iteration [1647], Loss: 0.5306, Error: 0.1719\n",
      "Iteration [1648], Loss: 0.4595, Error: 0.1406\n",
      "Iteration [1649], Loss: 0.6853, Error: 0.2344\n",
      "Iteration [1650], Loss: 0.9428, Error: 0.2969\n",
      "Iteration [1651], Loss: 0.4448, Error: 0.1875\n",
      "Iteration [1652], Loss: 0.3808, Error: 0.1094\n",
      "Iteration [1653], Loss: 0.7368, Error: 0.2188\n",
      "Iteration [1654], Loss: 0.6592, Error: 0.1875\n",
      "Iteration [1655], Loss: 0.6463, Error: 0.1875\n",
      "Iteration [1656], Loss: 0.4556, Error: 0.1562\n",
      "Iteration [1657], Loss: 0.5822, Error: 0.1875\n",
      "Iteration [1658], Loss: 0.5173, Error: 0.1875\n",
      "Iteration [1659], Loss: 0.5149, Error: 0.1250\n",
      "Iteration [1660], Loss: 0.3105, Error: 0.0625\n",
      "Iteration [1661], Loss: 0.5422, Error: 0.1875\n",
      "Iteration [1662], Loss: 0.7041, Error: 0.2812\n",
      "Iteration [1663], Loss: 0.3378, Error: 0.1250\n",
      "Iteration [1664], Loss: 0.3460, Error: 0.1094\n",
      "Iteration [1665], Loss: 0.7620, Error: 0.3125\n",
      "Iteration [1666], Loss: 0.5007, Error: 0.1719\n",
      "Iteration [1667], Loss: 0.6844, Error: 0.2656\n",
      "Iteration [1668], Loss: 0.4354, Error: 0.1094\n",
      "Iteration [1669], Loss: 0.4428, Error: 0.1562\n",
      "Iteration [1670], Loss: 0.7780, Error: 0.2500\n",
      "Iteration [1671], Loss: 0.6754, Error: 0.2188\n",
      "Iteration [1672], Loss: 0.4419, Error: 0.1562\n",
      "Iteration [1673], Loss: 0.7102, Error: 0.2188\n",
      "Iteration [1674], Loss: 0.4216, Error: 0.0781\n",
      "Iteration [1675], Loss: 0.4694, Error: 0.1406\n",
      "Iteration [1676], Loss: 0.4541, Error: 0.1562\n",
      "Iteration [1677], Loss: 0.2594, Error: 0.0781\n",
      "Iteration [1678], Loss: 0.5271, Error: 0.1719\n",
      "Iteration [1679], Loss: 0.6796, Error: 0.2188\n",
      "Iteration [1680], Loss: 0.8001, Error: 0.3125\n",
      "Iteration [1681], Loss: 0.4341, Error: 0.1562\n",
      "Iteration [1682], Loss: 0.5555, Error: 0.1719\n",
      "Iteration [1683], Loss: 0.6614, Error: 0.2188\n",
      "Iteration [1684], Loss: 0.5231, Error: 0.1406\n",
      "Iteration [1685], Loss: 0.5801, Error: 0.1562\n",
      "Iteration [1686], Loss: 0.5890, Error: 0.2031\n",
      "Iteration [1687], Loss: 0.5409, Error: 0.1875\n",
      "Iteration [1688], Loss: 0.5301, Error: 0.1719\n",
      "Iteration [1689], Loss: 0.4944, Error: 0.1562\n",
      "Iteration [1690], Loss: 0.4293, Error: 0.1250\n",
      "Iteration [1691], Loss: 0.6728, Error: 0.1875\n",
      "Iteration [1692], Loss: 0.6084, Error: 0.1719\n",
      "Iteration [1693], Loss: 0.5269, Error: 0.1719\n",
      "Iteration [1694], Loss: 0.5671, Error: 0.1562\n",
      "Iteration [1695], Loss: 0.5862, Error: 0.2031\n",
      "Iteration [1696], Loss: 0.5417, Error: 0.1719\n",
      "Iteration [1697], Loss: 0.6893, Error: 0.2500\n",
      "Iteration [1698], Loss: 0.2637, Error: 0.0625\n",
      "Iteration [1699], Loss: 0.5708, Error: 0.1875\n",
      "Iteration [1700], Loss: 0.5828, Error: 0.1562\n",
      "Iteration [1701], Loss: 0.5062, Error: 0.1719\n",
      "Iteration [1702], Loss: 0.6565, Error: 0.2344\n",
      "Iteration [1703], Loss: 0.5445, Error: 0.1719\n",
      "Iteration [1704], Loss: 0.5312, Error: 0.1719\n",
      "Iteration [1705], Loss: 0.6390, Error: 0.2188\n",
      "Iteration [1706], Loss: 0.4552, Error: 0.0938\n",
      "Iteration [1707], Loss: 0.6656, Error: 0.2188\n",
      "Iteration [1708], Loss: 0.4121, Error: 0.1094\n",
      "Iteration [1709], Loss: 0.4607, Error: 0.1406\n",
      "Iteration [1710], Loss: 0.4979, Error: 0.1094\n",
      "Iteration [1711], Loss: 0.5912, Error: 0.1875\n",
      "Iteration [1712], Loss: 0.5885, Error: 0.1719\n",
      "Iteration [1713], Loss: 0.4155, Error: 0.1406\n",
      "Iteration [1714], Loss: 0.4387, Error: 0.1406\n",
      "Iteration [1715], Loss: 0.4986, Error: 0.1562\n",
      "Iteration [1716], Loss: 0.5888, Error: 0.1719\n",
      "Iteration [1717], Loss: 0.6571, Error: 0.2031\n",
      "Iteration [1718], Loss: 0.4106, Error: 0.1562\n",
      "Iteration [1719], Loss: 0.4964, Error: 0.1562\n",
      "Iteration [1720], Loss: 0.6505, Error: 0.2188\n",
      "Iteration [1721], Loss: 0.6648, Error: 0.2188\n",
      "Iteration [1722], Loss: 0.5807, Error: 0.2344\n",
      "Iteration [1723], Loss: 0.6151, Error: 0.2031\n",
      "Iteration [1724], Loss: 0.5445, Error: 0.1562\n",
      "Iteration [1725], Loss: 0.4716, Error: 0.1719\n",
      "Iteration [1726], Loss: 0.3974, Error: 0.0938\n",
      "Iteration [1727], Loss: 0.5039, Error: 0.1875\n",
      "Iteration [1728], Loss: 0.8766, Error: 0.2656\n",
      "Iteration [1729], Loss: 0.6401, Error: 0.2031\n",
      "Iteration [1730], Loss: 0.5838, Error: 0.1719\n",
      "Iteration [1731], Loss: 0.5536, Error: 0.1875\n",
      "Iteration [1732], Loss: 0.6270, Error: 0.2344\n",
      "Iteration [1733], Loss: 0.6387, Error: 0.1719\n",
      "Iteration [1734], Loss: 0.4615, Error: 0.1719\n",
      "Iteration [1735], Loss: 0.5756, Error: 0.2031\n",
      "Iteration [1736], Loss: 0.6515, Error: 0.2031\n",
      "Iteration [1737], Loss: 0.5021, Error: 0.1406\n",
      "Iteration [1738], Loss: 0.4934, Error: 0.1719\n",
      "Iteration [1739], Loss: 0.6643, Error: 0.1875\n",
      "Iteration [1740], Loss: 0.6623, Error: 0.1875\n",
      "Iteration [1741], Loss: 0.6076, Error: 0.2188\n",
      "Iteration [1742], Loss: 0.5240, Error: 0.1719\n",
      "Iteration [1743], Loss: 0.6418, Error: 0.1875\n",
      "Iteration [1744], Loss: 0.4383, Error: 0.1875\n",
      "Iteration [1745], Loss: 0.7473, Error: 0.2188\n",
      "Iteration [1746], Loss: 0.5704, Error: 0.2188\n",
      "Iteration [1747], Loss: 0.4899, Error: 0.1406\n",
      "Iteration [1748], Loss: 0.5260, Error: 0.1875\n",
      "Iteration [1749], Loss: 0.5817, Error: 0.1719\n",
      "Iteration [1750], Loss: 0.4836, Error: 0.1406\n",
      "Iteration [1751], Loss: 0.4939, Error: 0.1875\n",
      "Iteration [1752], Loss: 0.4407, Error: 0.1875\n",
      "Iteration [1753], Loss: 0.4736, Error: 0.1250\n",
      "Iteration [1754], Loss: 0.4575, Error: 0.1719\n",
      "Iteration [1755], Loss: 0.4702, Error: 0.1719\n",
      "Iteration [1756], Loss: 0.7546, Error: 0.2500\n",
      "Iteration [1757], Loss: 0.6080, Error: 0.1406\n",
      "Iteration [1758], Loss: 0.5969, Error: 0.1875\n",
      "Iteration [1759], Loss: 0.3918, Error: 0.1094\n",
      "Iteration [1760], Loss: 0.4614, Error: 0.1719\n",
      "Iteration [1761], Loss: 0.6077, Error: 0.2031\n",
      "Iteration [1762], Loss: 0.5552, Error: 0.1719\n",
      "Iteration [1763], Loss: 0.4978, Error: 0.1250\n",
      "Iteration [1764], Loss: 0.7625, Error: 0.2031\n",
      "Iteration [1765], Loss: 0.5030, Error: 0.1406\n",
      "Iteration [1766], Loss: 0.3821, Error: 0.1250\n",
      "Iteration [1767], Loss: 0.6316, Error: 0.1875\n",
      "Iteration [1768], Loss: 0.6300, Error: 0.1562\n",
      "Iteration [1769], Loss: 0.5366, Error: 0.1562\n",
      "Iteration [1770], Loss: 0.9243, Error: 0.2812\n",
      "Iteration [1771], Loss: 0.3512, Error: 0.0938\n",
      "Iteration [1772], Loss: 0.3738, Error: 0.1250\n",
      "Iteration [1773], Loss: 0.6922, Error: 0.2344\n",
      "Iteration [1774], Loss: 0.4675, Error: 0.1875\n",
      "Iteration [1775], Loss: 0.4930, Error: 0.1719\n",
      "Iteration [1776], Loss: 0.6669, Error: 0.1875\n",
      "Iteration [1777], Loss: 0.5677, Error: 0.1719\n",
      "Iteration [1778], Loss: 0.3119, Error: 0.1094\n",
      "Iteration [1779], Loss: 0.3697, Error: 0.1094\n",
      "Iteration [1780], Loss: 0.4420, Error: 0.1562\n",
      "Iteration [1781], Loss: 0.4929, Error: 0.1562\n",
      "Iteration [1782], Loss: 0.4880, Error: 0.1562\n",
      "Iteration [1783], Loss: 0.5128, Error: 0.1406\n",
      "Iteration [1784], Loss: 0.3815, Error: 0.1250\n",
      "Iteration [1785], Loss: 0.4096, Error: 0.1094\n",
      "Iteration [1786], Loss: 0.7553, Error: 0.2656\n",
      "Iteration [1787], Loss: 0.6518, Error: 0.2031\n",
      "Iteration [1788], Loss: 0.7037, Error: 0.2344\n",
      "Iteration [1789], Loss: 0.6536, Error: 0.2031\n",
      "Iteration [1790], Loss: 0.3737, Error: 0.0938\n",
      "Iteration [1791], Loss: 0.6985, Error: 0.2812\n",
      "Iteration [1792], Loss: 0.3070, Error: 0.0938\n",
      "Iteration [1793], Loss: 0.4013, Error: 0.1562\n",
      "Iteration [1794], Loss: 0.4783, Error: 0.1719\n",
      "Iteration [1795], Loss: 0.5777, Error: 0.2031\n",
      "Iteration [1796], Loss: 0.6355, Error: 0.1875\n",
      "Iteration [1797], Loss: 0.3836, Error: 0.1406\n",
      "Iteration [1798], Loss: 0.6811, Error: 0.2031\n",
      "Iteration [1799], Loss: 0.4863, Error: 0.1719\n",
      "Iteration [1800], Loss: 0.4561, Error: 0.1562\n",
      "Iteration [1801], Loss: 0.7677, Error: 0.2031\n",
      "Iteration [1802], Loss: 0.3908, Error: 0.1406\n",
      "Iteration [1803], Loss: 0.4986, Error: 0.1562\n",
      "Iteration [1804], Loss: 0.5667, Error: 0.1875\n",
      "Iteration [1805], Loss: 0.4356, Error: 0.1250\n",
      "Iteration [1806], Loss: 0.7018, Error: 0.2344\n",
      "Iteration [1807], Loss: 0.6417, Error: 0.1875\n",
      "Iteration [1808], Loss: 0.3755, Error: 0.0938\n",
      "Iteration [1809], Loss: 0.4139, Error: 0.1406\n",
      "Iteration [1810], Loss: 0.3807, Error: 0.1406\n",
      "Iteration [1811], Loss: 0.5190, Error: 0.2031\n",
      "Iteration [1812], Loss: 0.5718, Error: 0.2031\n",
      "Iteration [1813], Loss: 0.2953, Error: 0.0938\n",
      "Iteration [1814], Loss: 0.6908, Error: 0.2188\n",
      "Iteration [1815], Loss: 0.4138, Error: 0.1094\n",
      "Iteration [1816], Loss: 0.5642, Error: 0.1406\n",
      "Iteration [1817], Loss: 0.5279, Error: 0.1875\n",
      "Iteration [1818], Loss: 0.4271, Error: 0.1719\n",
      "Iteration [1819], Loss: 0.4525, Error: 0.1250\n",
      "Iteration [1820], Loss: 0.4869, Error: 0.1406\n",
      "Iteration [1821], Loss: 0.5230, Error: 0.1875\n",
      "Iteration [1822], Loss: 0.4687, Error: 0.1719\n",
      "Iteration [1823], Loss: 0.7136, Error: 0.2344\n",
      "Iteration [1824], Loss: 0.5273, Error: 0.1719\n",
      "Iteration [1825], Loss: 0.4733, Error: 0.1406\n",
      "Iteration [1826], Loss: 0.4176, Error: 0.1094\n",
      "Iteration [1827], Loss: 0.4042, Error: 0.0781\n",
      "Iteration [1828], Loss: 0.6421, Error: 0.2031\n",
      "Iteration [1829], Loss: 0.8818, Error: 0.2969\n",
      "Iteration [1830], Loss: 0.5548, Error: 0.1875\n",
      "Iteration [1831], Loss: 0.4005, Error: 0.1406\n",
      "Iteration [1832], Loss: 0.5441, Error: 0.2188\n",
      "Iteration [1833], Loss: 0.2684, Error: 0.0781\n",
      "Iteration [1834], Loss: 0.5791, Error: 0.1875\n",
      "Iteration [1835], Loss: 0.5076, Error: 0.2031\n",
      "Iteration [1836], Loss: 0.4233, Error: 0.1562\n",
      "Iteration [1837], Loss: 0.7389, Error: 0.2500\n",
      "Iteration [1838], Loss: 0.4983, Error: 0.1406\n",
      "Iteration [1839], Loss: 0.6042, Error: 0.1719\n",
      "Iteration [1840], Loss: 0.4976, Error: 0.1250\n",
      "Iteration [1841], Loss: 0.4463, Error: 0.1406\n",
      "Iteration [1842], Loss: 0.5829, Error: 0.1875\n",
      "Iteration [1843], Loss: 0.4423, Error: 0.1250\n",
      "Iteration [1844], Loss: 0.4397, Error: 0.1406\n",
      "Iteration [1845], Loss: 0.4012, Error: 0.1094\n",
      "Iteration [1846], Loss: 0.6715, Error: 0.2031\n",
      "Iteration [1847], Loss: 0.6555, Error: 0.2188\n",
      "Iteration [1848], Loss: 0.5826, Error: 0.1719\n",
      "Iteration [1849], Loss: 0.6118, Error: 0.2031\n",
      "Iteration [1850], Loss: 0.4892, Error: 0.1406\n",
      "Iteration [1851], Loss: 0.5899, Error: 0.1562\n",
      "Iteration [1852], Loss: 0.3729, Error: 0.1562\n",
      "Iteration [1853], Loss: 0.5908, Error: 0.1875\n",
      "Iteration [1854], Loss: 0.6474, Error: 0.2188\n",
      "Iteration [1855], Loss: 0.5900, Error: 0.1875\n",
      "Iteration [1856], Loss: 0.3042, Error: 0.1094\n",
      "Iteration [1857], Loss: 0.4022, Error: 0.1250\n",
      "Iteration [1858], Loss: 0.8069, Error: 0.2500\n",
      "Iteration [1859], Loss: 0.5418, Error: 0.1875\n",
      "Iteration [1860], Loss: 0.5352, Error: 0.2031\n",
      "Iteration [1861], Loss: 0.5010, Error: 0.1719\n",
      "Iteration [1862], Loss: 0.6549, Error: 0.2031\n",
      "Iteration [1863], Loss: 0.3946, Error: 0.1562\n",
      "Iteration [1864], Loss: 0.4826, Error: 0.1562\n",
      "Iteration [1865], Loss: 0.5580, Error: 0.1875\n",
      "Iteration [1866], Loss: 0.3642, Error: 0.1094\n",
      "Iteration [1867], Loss: 0.4131, Error: 0.1406\n",
      "Iteration [1868], Loss: 0.7098, Error: 0.2188\n",
      "Iteration [1869], Loss: 0.6000, Error: 0.1875\n",
      "Iteration [1870], Loss: 0.7423, Error: 0.2344\n",
      "Iteration [1871], Loss: 0.3083, Error: 0.1094\n",
      "Iteration [1872], Loss: 0.5000, Error: 0.1562\n",
      "Iteration [1873], Loss: 0.5367, Error: 0.1562\n",
      "Iteration [1874], Loss: 0.5174, Error: 0.1719\n",
      "Iteration [1875], Loss: 0.7331, Error: 0.2500\n",
      "Iteration [1876], Loss: 0.6332, Error: 0.2031\n",
      "Iteration [1877], Loss: 0.3956, Error: 0.0781\n",
      "Iteration [1878], Loss: 0.4317, Error: 0.1250\n",
      "Iteration [1879], Loss: 0.4981, Error: 0.1875\n",
      "Iteration [1880], Loss: 0.4858, Error: 0.1875\n",
      "Iteration [1881], Loss: 0.6895, Error: 0.2031\n",
      "Iteration [1882], Loss: 0.5022, Error: 0.1562\n",
      "Iteration [1883], Loss: 0.5881, Error: 0.1719\n",
      "Iteration [1884], Loss: 0.5880, Error: 0.1875\n",
      "Iteration [1885], Loss: 0.4318, Error: 0.1562\n",
      "Iteration [1886], Loss: 0.3855, Error: 0.1250\n",
      "Iteration [1887], Loss: 0.2886, Error: 0.0781\n",
      "Iteration [1888], Loss: 0.4374, Error: 0.1250\n",
      "Iteration [1889], Loss: 0.5663, Error: 0.2031\n",
      "Iteration [1890], Loss: 0.5622, Error: 0.1562\n",
      "Iteration [1891], Loss: 0.4575, Error: 0.1719\n",
      "Iteration [1892], Loss: 0.5298, Error: 0.1562\n",
      "Iteration [1893], Loss: 0.4606, Error: 0.1406\n",
      "Iteration [1894], Loss: 0.5868, Error: 0.1719\n",
      "Iteration [1895], Loss: 0.4670, Error: 0.1406\n",
      "Iteration [1896], Loss: 0.5691, Error: 0.1719\n",
      "Iteration [1897], Loss: 0.7389, Error: 0.2812\n",
      "Iteration [1898], Loss: 0.4882, Error: 0.1406\n",
      "Iteration [1899], Loss: 0.6631, Error: 0.2031\n",
      "Iteration [1900], Loss: 0.5390, Error: 0.1875\n",
      "Iteration [1901], Loss: 0.4554, Error: 0.1562\n",
      "Iteration [1902], Loss: 0.4104, Error: 0.1406\n",
      "Iteration [1903], Loss: 0.7243, Error: 0.2031\n",
      "Iteration [1904], Loss: 0.4072, Error: 0.1406\n",
      "Iteration [1905], Loss: 0.7694, Error: 0.2344\n",
      "Iteration [1906], Loss: 0.4814, Error: 0.1562\n",
      "Iteration [1907], Loss: 0.4694, Error: 0.1562\n",
      "Iteration [1908], Loss: 0.4932, Error: 0.1406\n",
      "Iteration [1909], Loss: 0.4451, Error: 0.1406\n",
      "Iteration [1910], Loss: 0.5113, Error: 0.1875\n",
      "Iteration [1911], Loss: 0.4785, Error: 0.1719\n",
      "Iteration [1912], Loss: 0.6630, Error: 0.2031\n",
      "Iteration [1913], Loss: 0.6517, Error: 0.2031\n",
      "Iteration [1914], Loss: 0.5537, Error: 0.1875\n",
      "Iteration [1915], Loss: 0.6902, Error: 0.2344\n",
      "Iteration [1916], Loss: 0.7188, Error: 0.2188\n",
      "Iteration [1917], Loss: 0.4608, Error: 0.1562\n",
      "Iteration [1918], Loss: 0.4608, Error: 0.1562\n",
      "Iteration [1919], Loss: 0.5340, Error: 0.1406\n",
      "Iteration [1920], Loss: 0.4858, Error: 0.1719\n",
      "Iteration [1921], Loss: 0.3038, Error: 0.0781\n",
      "Iteration [1922], Loss: 0.5755, Error: 0.1719\n",
      "Iteration [1923], Loss: 0.5707, Error: 0.2188\n",
      "Iteration [1924], Loss: 0.6100, Error: 0.1719\n",
      "Iteration [1925], Loss: 0.7043, Error: 0.2500\n",
      "Iteration [1926], Loss: 0.5973, Error: 0.1562\n",
      "Iteration [1927], Loss: 0.4398, Error: 0.1250\n",
      "Iteration [1928], Loss: 0.5903, Error: 0.2188\n",
      "Iteration [1929], Loss: 0.4578, Error: 0.1406\n",
      "Iteration [1930], Loss: 0.7285, Error: 0.2656\n",
      "Iteration [1931], Loss: 0.5159, Error: 0.1875\n",
      "Iteration [1932], Loss: 0.6751, Error: 0.2656\n",
      "Iteration [1933], Loss: 0.5967, Error: 0.1875\n",
      "Iteration [1934], Loss: 0.5401, Error: 0.1406\n",
      "Iteration [1935], Loss: 0.5088, Error: 0.1562\n",
      "Iteration [1936], Loss: 0.4118, Error: 0.1094\n",
      "Iteration [1937], Loss: 0.5127, Error: 0.1719\n",
      "Iteration [1938], Loss: 0.5780, Error: 0.1719\n",
      "Iteration [1939], Loss: 0.6084, Error: 0.2188\n",
      "Iteration [1940], Loss: 0.4271, Error: 0.1406\n",
      "Iteration [1941], Loss: 0.4226, Error: 0.1250\n",
      "Iteration [1942], Loss: 0.4820, Error: 0.1406\n",
      "Iteration [1943], Loss: 0.7067, Error: 0.2500\n",
      "Iteration [1944], Loss: 0.4196, Error: 0.1094\n",
      "Iteration [1945], Loss: 0.3484, Error: 0.1250\n",
      "Iteration [1946], Loss: 0.5086, Error: 0.1406\n",
      "Iteration [1947], Loss: 0.5786, Error: 0.2500\n",
      "Iteration [1948], Loss: 0.6989, Error: 0.2344\n",
      "Iteration [1949], Loss: 0.4553, Error: 0.1406\n",
      "Iteration [1950], Loss: 0.5535, Error: 0.1875\n",
      "Iteration [1951], Loss: 0.6494, Error: 0.1719\n",
      "Iteration [1952], Loss: 0.5597, Error: 0.1875\n",
      "Iteration [1953], Loss: 0.5328, Error: 0.1719\n",
      "Iteration [1954], Loss: 0.6747, Error: 0.2344\n",
      "Iteration [1955], Loss: 0.5667, Error: 0.1875\n",
      "Iteration [1956], Loss: 0.7114, Error: 0.2344\n",
      "Iteration [1957], Loss: 0.4279, Error: 0.1406\n",
      "Iteration [1958], Loss: 0.5239, Error: 0.1406\n",
      "Iteration [1959], Loss: 0.6121, Error: 0.2188\n",
      "Iteration [1960], Loss: 0.4662, Error: 0.1562\n",
      "Iteration [1961], Loss: 0.5606, Error: 0.1875\n",
      "Iteration [1962], Loss: 0.4147, Error: 0.1406\n",
      "Iteration [1963], Loss: 0.6136, Error: 0.1875\n",
      "Iteration [1964], Loss: 0.4365, Error: 0.1250\n",
      "Iteration [1965], Loss: 0.5955, Error: 0.1875\n",
      "Iteration [1966], Loss: 0.6002, Error: 0.2031\n",
      "Iteration [1967], Loss: 0.2912, Error: 0.0781\n",
      "Iteration [1968], Loss: 0.4553, Error: 0.1250\n",
      "Iteration [1969], Loss: 0.3619, Error: 0.1094\n",
      "Iteration [1970], Loss: 0.6040, Error: 0.2031\n",
      "Iteration [1971], Loss: 0.5567, Error: 0.1875\n",
      "Iteration [1972], Loss: 0.4435, Error: 0.1719\n",
      "Iteration [1973], Loss: 0.5078, Error: 0.1719\n",
      "Iteration [1974], Loss: 0.4948, Error: 0.2031\n",
      "Iteration [1975], Loss: 0.4220, Error: 0.1406\n",
      "Iteration [1976], Loss: 0.5783, Error: 0.1719\n",
      "Iteration [1977], Loss: 0.6415, Error: 0.2031\n",
      "Iteration [1978], Loss: 0.6386, Error: 0.1875\n",
      "Iteration [1979], Loss: 0.5312, Error: 0.1719\n",
      "Iteration [1980], Loss: 0.5583, Error: 0.1719\n",
      "Iteration [1981], Loss: 0.5365, Error: 0.1719\n",
      "Iteration [1982], Loss: 0.3143, Error: 0.0781\n",
      "Iteration [1983], Loss: 0.4579, Error: 0.1875\n",
      "Iteration [1984], Loss: 0.5600, Error: 0.2031\n",
      "Iteration [1985], Loss: 0.7665, Error: 0.2344\n",
      "Iteration [1986], Loss: 0.4950, Error: 0.1250\n",
      "Iteration [1987], Loss: 0.3139, Error: 0.0938\n",
      "Iteration [1988], Loss: 0.5506, Error: 0.1562\n",
      "Iteration [1989], Loss: 0.6364, Error: 0.2188\n",
      "Iteration [1990], Loss: 0.4031, Error: 0.1406\n",
      "Iteration [1991], Loss: 1.0135, Error: 0.3125\n",
      "Iteration [1992], Loss: 0.6025, Error: 0.2031\n",
      "Iteration [1993], Loss: 0.4565, Error: 0.1562\n",
      "Iteration [1994], Loss: 0.4710, Error: 0.1406\n",
      "Iteration [1995], Loss: 0.4277, Error: 0.1250\n",
      "Iteration [1996], Loss: 0.5933, Error: 0.1875\n",
      "Iteration [1997], Loss: 0.2886, Error: 0.0938\n",
      "Iteration [1998], Loss: 0.6406, Error: 0.1875\n",
      "Iteration [1999], Loss: 0.4477, Error: 0.1562\n",
      "Iteration [2000], Loss: 0.4872, Error: 0.1250\n",
      "Iteration [2001], Loss: 0.3644, Error: 0.1406\n",
      "Iteration [2002], Loss: 0.4369, Error: 0.1406\n",
      "Iteration [2003], Loss: 0.3820, Error: 0.1250\n",
      "Iteration [2004], Loss: 0.8971, Error: 0.3125\n",
      "Iteration [2005], Loss: 0.5982, Error: 0.1406\n",
      "Iteration [2006], Loss: 0.6290, Error: 0.2031\n",
      "Iteration [2007], Loss: 0.2885, Error: 0.0625\n",
      "Iteration [2008], Loss: 0.4655, Error: 0.1250\n",
      "Iteration [2009], Loss: 0.3378, Error: 0.0312\n",
      "Iteration [2010], Loss: 0.3296, Error: 0.0469\n",
      "Iteration [2011], Loss: 0.3140, Error: 0.0625\n",
      "Iteration [2012], Loss: 0.3538, Error: 0.1094\n",
      "Iteration [2013], Loss: 0.3237, Error: 0.0938\n",
      "Iteration [2014], Loss: 0.3925, Error: 0.0781\n",
      "Iteration [2015], Loss: 0.5539, Error: 0.1250\n",
      "Iteration [2016], Loss: 0.2910, Error: 0.0312\n",
      "Iteration [2017], Loss: 0.4704, Error: 0.1094\n",
      "Iteration [2018], Loss: 0.4698, Error: 0.0938\n",
      "Iteration [2019], Loss: 0.2740, Error: 0.0938\n",
      "Iteration [2020], Loss: 0.4476, Error: 0.1406\n",
      "Iteration [2021], Loss: 0.2986, Error: 0.0469\n",
      "Iteration [2022], Loss: 0.2035, Error: 0.0312\n",
      "Iteration [2023], Loss: 0.3775, Error: 0.1250\n",
      "Iteration [2024], Loss: 0.4425, Error: 0.1250\n",
      "Iteration [2025], Loss: 0.4275, Error: 0.1250\n",
      "Iteration [2026], Loss: 0.5165, Error: 0.1250\n",
      "Iteration [2027], Loss: 0.3412, Error: 0.0781\n",
      "Iteration [2028], Loss: 0.3466, Error: 0.1094\n",
      "Iteration [2029], Loss: 0.2768, Error: 0.0781\n",
      "Iteration [2030], Loss: 0.3333, Error: 0.0469\n",
      "Iteration [2031], Loss: 0.4813, Error: 0.1094\n",
      "Iteration [2032], Loss: 0.3141, Error: 0.0938\n",
      "Iteration [2033], Loss: 0.2712, Error: 0.0312\n",
      "Iteration [2034], Loss: 0.3116, Error: 0.0625\n",
      "Iteration [2035], Loss: 0.5135, Error: 0.1562\n",
      "Iteration [2036], Loss: 0.4277, Error: 0.1094\n",
      "Iteration [2037], Loss: 0.4717, Error: 0.1250\n",
      "Iteration [2038], Loss: 0.5521, Error: 0.1562\n",
      "Iteration [2039], Loss: 0.1983, Error: 0.0312\n",
      "Iteration [2040], Loss: 0.3918, Error: 0.1094\n",
      "Iteration [2041], Loss: 0.5480, Error: 0.1719\n",
      "Iteration [2042], Loss: 0.4605, Error: 0.1250\n",
      "Iteration [2043], Loss: 0.3340, Error: 0.0938\n",
      "Iteration [2044], Loss: 0.2234, Error: 0.0469\n",
      "Iteration [2045], Loss: 0.3936, Error: 0.0781\n",
      "Iteration [2046], Loss: 0.2882, Error: 0.0625\n",
      "Iteration [2047], Loss: 0.4526, Error: 0.1250\n",
      "Iteration [2048], Loss: 0.2259, Error: 0.0625\n",
      "Iteration [2049], Loss: 0.5707, Error: 0.1875\n",
      "Iteration [2050], Loss: 0.2149, Error: 0.0469\n",
      "Iteration [2051], Loss: 0.3835, Error: 0.1094\n",
      "Iteration [2052], Loss: 0.3556, Error: 0.0938\n",
      "Iteration [2053], Loss: 0.2801, Error: 0.0469\n",
      "Iteration [2054], Loss: 0.2785, Error: 0.0781\n",
      "Iteration [2055], Loss: 0.5705, Error: 0.1875\n",
      "Iteration [2056], Loss: 0.3195, Error: 0.0781\n",
      "Iteration [2057], Loss: 0.6909, Error: 0.1875\n",
      "Iteration [2058], Loss: 0.4656, Error: 0.1250\n",
      "Iteration [2059], Loss: 0.4828, Error: 0.1094\n",
      "Iteration [2060], Loss: 0.4065, Error: 0.1406\n",
      "Iteration [2061], Loss: 0.5569, Error: 0.1406\n",
      "Iteration [2062], Loss: 0.3459, Error: 0.0938\n",
      "Iteration [2063], Loss: 0.2446, Error: 0.0469\n",
      "Iteration [2064], Loss: 0.4377, Error: 0.1250\n",
      "Iteration [2065], Loss: 0.4261, Error: 0.1875\n",
      "Iteration [2066], Loss: 0.2164, Error: 0.0469\n",
      "Iteration [2067], Loss: 0.2310, Error: 0.0781\n",
      "Iteration [2068], Loss: 0.3731, Error: 0.1406\n",
      "Iteration [2069], Loss: 0.3335, Error: 0.0938\n",
      "Iteration [2070], Loss: 0.2420, Error: 0.0469\n",
      "Iteration [2071], Loss: 0.4773, Error: 0.0781\n",
      "Iteration [2072], Loss: 0.2284, Error: 0.0625\n",
      "Iteration [2073], Loss: 0.5351, Error: 0.1094\n",
      "Iteration [2074], Loss: 0.3190, Error: 0.0781\n",
      "Iteration [2075], Loss: 0.4348, Error: 0.1250\n",
      "Iteration [2076], Loss: 0.3144, Error: 0.0781\n",
      "Iteration [2077], Loss: 0.4443, Error: 0.1406\n",
      "Iteration [2078], Loss: 0.3103, Error: 0.0781\n",
      "Iteration [2079], Loss: 0.2532, Error: 0.0469\n",
      "Iteration [2080], Loss: 0.3466, Error: 0.0625\n",
      "Iteration [2081], Loss: 0.2206, Error: 0.0938\n",
      "Iteration [2082], Loss: 0.3266, Error: 0.0938\n",
      "Iteration [2083], Loss: 0.3565, Error: 0.0781\n",
      "Iteration [2084], Loss: 0.3949, Error: 0.1094\n",
      "Iteration [2085], Loss: 0.2969, Error: 0.0938\n",
      "Iteration [2086], Loss: 0.3516, Error: 0.0625\n",
      "Iteration [2087], Loss: 0.4647, Error: 0.1094\n",
      "Iteration [2088], Loss: 0.2796, Error: 0.0469\n",
      "Iteration [2089], Loss: 0.3280, Error: 0.0781\n",
      "Iteration [2090], Loss: 0.2330, Error: 0.0781\n",
      "Iteration [2091], Loss: 0.2280, Error: 0.0469\n",
      "Iteration [2092], Loss: 0.2759, Error: 0.0625\n",
      "Iteration [2093], Loss: 0.2862, Error: 0.0938\n",
      "Iteration [2094], Loss: 0.5085, Error: 0.1562\n",
      "Iteration [2095], Loss: 0.3221, Error: 0.0938\n",
      "Iteration [2096], Loss: 0.4123, Error: 0.1406\n",
      "Iteration [2097], Loss: 0.3101, Error: 0.0625\n",
      "Iteration [2098], Loss: 0.2889, Error: 0.0938\n",
      "Iteration [2099], Loss: 0.2990, Error: 0.1250\n",
      "Iteration [2100], Loss: 0.3924, Error: 0.1406\n",
      "Iteration [2101], Loss: 0.3765, Error: 0.0938\n",
      "Iteration [2102], Loss: 0.4749, Error: 0.0781\n",
      "Iteration [2103], Loss: 0.2832, Error: 0.0781\n",
      "Iteration [2104], Loss: 0.3330, Error: 0.0938\n",
      "Iteration [2105], Loss: 0.2844, Error: 0.0781\n",
      "Iteration [2106], Loss: 0.1986, Error: 0.0469\n",
      "Iteration [2107], Loss: 0.3342, Error: 0.0781\n",
      "Iteration [2108], Loss: 0.3819, Error: 0.1094\n",
      "Iteration [2109], Loss: 0.2433, Error: 0.0625\n",
      "Iteration [2110], Loss: 0.2948, Error: 0.0938\n",
      "Iteration [2111], Loss: 0.4608, Error: 0.1406\n",
      "Iteration [2112], Loss: 0.3823, Error: 0.0781\n",
      "Iteration [2113], Loss: 0.4173, Error: 0.1250\n",
      "Iteration [2114], Loss: 0.2863, Error: 0.0938\n",
      "Iteration [2115], Loss: 0.2185, Error: 0.0781\n",
      "Iteration [2116], Loss: 0.4080, Error: 0.1562\n",
      "Iteration [2117], Loss: 0.4013, Error: 0.0781\n",
      "Iteration [2118], Loss: 0.4885, Error: 0.1719\n",
      "Iteration [2119], Loss: 0.3268, Error: 0.0938\n",
      "Iteration [2120], Loss: 0.1442, Error: 0.0312\n",
      "Iteration [2121], Loss: 0.4730, Error: 0.1250\n",
      "Iteration [2122], Loss: 0.3983, Error: 0.1094\n",
      "Iteration [2123], Loss: 0.2833, Error: 0.0938\n",
      "Iteration [2124], Loss: 0.3709, Error: 0.1094\n",
      "Iteration [2125], Loss: 0.2758, Error: 0.0781\n",
      "Iteration [2126], Loss: 0.3960, Error: 0.1250\n",
      "Iteration [2127], Loss: 0.2803, Error: 0.0781\n",
      "Iteration [2128], Loss: 0.4045, Error: 0.0938\n",
      "Iteration [2129], Loss: 0.5583, Error: 0.1562\n",
      "Iteration [2130], Loss: 0.3458, Error: 0.0938\n",
      "Iteration [2131], Loss: 0.2895, Error: 0.0625\n",
      "Iteration [2132], Loss: 0.3894, Error: 0.1094\n",
      "Iteration [2133], Loss: 0.3472, Error: 0.0469\n",
      "Iteration [2134], Loss: 0.2619, Error: 0.0469\n",
      "Iteration [2135], Loss: 0.4230, Error: 0.0938\n",
      "Iteration [2136], Loss: 0.2785, Error: 0.0781\n",
      "Iteration [2137], Loss: 0.6460, Error: 0.2188\n",
      "Iteration [2138], Loss: 0.3870, Error: 0.0938\n",
      "Iteration [2139], Loss: 0.2985, Error: 0.0781\n",
      "Iteration [2140], Loss: 0.3444, Error: 0.0938\n",
      "Iteration [2141], Loss: 0.2820, Error: 0.0938\n",
      "Iteration [2142], Loss: 0.3594, Error: 0.0781\n",
      "Iteration [2143], Loss: 0.3552, Error: 0.1094\n",
      "Iteration [2144], Loss: 0.2645, Error: 0.1094\n",
      "Iteration [2145], Loss: 0.2749, Error: 0.1094\n",
      "Iteration [2146], Loss: 0.3191, Error: 0.0469\n",
      "Iteration [2147], Loss: 0.3069, Error: 0.0781\n",
      "Iteration [2148], Loss: 0.0981, Error: 0.0312\n",
      "Iteration [2149], Loss: 0.4690, Error: 0.1094\n",
      "Iteration [2150], Loss: 0.3062, Error: 0.0938\n",
      "Iteration [2151], Loss: 0.1529, Error: 0.0156\n",
      "Iteration [2152], Loss: 0.5161, Error: 0.1719\n",
      "Iteration [2153], Loss: 0.3721, Error: 0.1406\n",
      "Iteration [2154], Loss: 0.2182, Error: 0.0625\n",
      "Iteration [2155], Loss: 0.3339, Error: 0.1094\n",
      "Iteration [2156], Loss: 0.3936, Error: 0.1094\n",
      "Iteration [2157], Loss: 0.2628, Error: 0.0469\n",
      "Iteration [2158], Loss: 0.1713, Error: 0.0469\n",
      "Iteration [2159], Loss: 0.3616, Error: 0.0625\n",
      "Iteration [2160], Loss: 0.2451, Error: 0.0312\n",
      "Iteration [2161], Loss: 0.2760, Error: 0.0781\n",
      "Iteration [2162], Loss: 0.3648, Error: 0.1250\n",
      "Iteration [2163], Loss: 0.2453, Error: 0.0781\n",
      "Iteration [2164], Loss: 0.3948, Error: 0.1094\n",
      "Iteration [2165], Loss: 0.3437, Error: 0.0938\n",
      "Iteration [2166], Loss: 0.3516, Error: 0.1094\n",
      "Iteration [2167], Loss: 0.2552, Error: 0.0625\n",
      "Iteration [2168], Loss: 0.5081, Error: 0.1562\n",
      "Iteration [2169], Loss: 0.2987, Error: 0.0781\n",
      "Iteration [2170], Loss: 0.3775, Error: 0.1250\n",
      "Iteration [2171], Loss: 0.2802, Error: 0.0781\n",
      "Iteration [2172], Loss: 0.2284, Error: 0.0781\n",
      "Iteration [2173], Loss: 0.3222, Error: 0.0938\n",
      "Iteration [2174], Loss: 0.4116, Error: 0.1094\n",
      "Iteration [2175], Loss: 0.4185, Error: 0.1250\n",
      "Iteration [2176], Loss: 0.3000, Error: 0.1094\n",
      "Iteration [2177], Loss: 0.5468, Error: 0.1094\n",
      "Iteration [2178], Loss: 0.3361, Error: 0.0938\n",
      "Iteration [2179], Loss: 0.1977, Error: 0.0625\n",
      "Iteration [2180], Loss: 0.4755, Error: 0.1406\n",
      "Iteration [2181], Loss: 0.2967, Error: 0.0625\n",
      "Iteration [2182], Loss: 0.2501, Error: 0.0781\n",
      "Iteration [2183], Loss: 0.4195, Error: 0.0781\n",
      "Iteration [2184], Loss: 0.2780, Error: 0.0938\n",
      "Iteration [2185], Loss: 0.2752, Error: 0.0625\n",
      "Iteration [2186], Loss: 0.1927, Error: 0.0156\n",
      "Iteration [2187], Loss: 0.2377, Error: 0.0469\n",
      "Iteration [2188], Loss: 0.4052, Error: 0.1250\n",
      "Iteration [2189], Loss: 0.4054, Error: 0.1406\n",
      "Iteration [2190], Loss: 0.3359, Error: 0.1094\n",
      "Iteration [2191], Loss: 0.2517, Error: 0.0469\n",
      "Iteration [2192], Loss: 0.2481, Error: 0.0781\n",
      "Iteration [2193], Loss: 0.4251, Error: 0.1406\n",
      "Iteration [2194], Loss: 0.2070, Error: 0.0781\n",
      "Iteration [2195], Loss: 0.2119, Error: 0.0625\n",
      "Iteration [2196], Loss: 0.4466, Error: 0.1250\n",
      "Iteration [2197], Loss: 0.5341, Error: 0.1406\n",
      "Iteration [2198], Loss: 0.3854, Error: 0.0781\n",
      "Iteration [2199], Loss: 0.2819, Error: 0.0625\n",
      "Iteration [2200], Loss: 0.2209, Error: 0.1094\n",
      "Iteration [2201], Loss: 0.2501, Error: 0.0469\n",
      "Iteration [2202], Loss: 0.2480, Error: 0.0781\n",
      "Iteration [2203], Loss: 0.2417, Error: 0.0625\n",
      "Iteration [2204], Loss: 0.4649, Error: 0.1406\n",
      "Iteration [2205], Loss: 0.2773, Error: 0.0625\n",
      "Iteration [2206], Loss: 0.2741, Error: 0.0469\n",
      "Iteration [2207], Loss: 0.4621, Error: 0.1719\n",
      "Iteration [2208], Loss: 0.3036, Error: 0.0469\n",
      "Iteration [2209], Loss: 0.5286, Error: 0.1406\n",
      "Iteration [2210], Loss: 0.4682, Error: 0.1094\n",
      "Iteration [2211], Loss: 0.3396, Error: 0.0625\n",
      "Iteration [2212], Loss: 0.2798, Error: 0.0938\n",
      "Iteration [2213], Loss: 0.3564, Error: 0.0781\n",
      "Iteration [2214], Loss: 0.2595, Error: 0.0625\n",
      "Iteration [2215], Loss: 0.4722, Error: 0.1406\n",
      "Iteration [2216], Loss: 0.2550, Error: 0.0625\n",
      "Iteration [2217], Loss: 0.3067, Error: 0.1094\n",
      "Iteration [2218], Loss: 0.4478, Error: 0.1406\n",
      "Iteration [2219], Loss: 0.4059, Error: 0.1094\n",
      "Iteration [2220], Loss: 0.4013, Error: 0.0625\n",
      "Iteration [2221], Loss: 0.3687, Error: 0.1094\n",
      "Iteration [2222], Loss: 0.2810, Error: 0.0938\n",
      "Iteration [2223], Loss: 0.4713, Error: 0.1406\n",
      "Iteration [2224], Loss: 0.4115, Error: 0.0938\n",
      "Iteration [2225], Loss: 0.3337, Error: 0.0625\n",
      "Iteration [2226], Loss: 0.2625, Error: 0.0469\n",
      "Iteration [2227], Loss: 0.3052, Error: 0.1094\n",
      "Iteration [2228], Loss: 0.2330, Error: 0.0625\n",
      "Iteration [2229], Loss: 0.3187, Error: 0.0625\n",
      "Iteration [2230], Loss: 0.5702, Error: 0.1562\n",
      "Iteration [2231], Loss: 0.4819, Error: 0.1250\n",
      "Iteration [2232], Loss: 0.2494, Error: 0.0625\n",
      "Iteration [2233], Loss: 0.3968, Error: 0.1250\n",
      "Iteration [2234], Loss: 0.3140, Error: 0.0938\n",
      "Iteration [2235], Loss: 0.3063, Error: 0.0938\n",
      "Iteration [2236], Loss: 0.3266, Error: 0.1250\n",
      "Iteration [2237], Loss: 0.2881, Error: 0.1094\n",
      "Iteration [2238], Loss: 0.5309, Error: 0.1562\n",
      "Iteration [2239], Loss: 0.4753, Error: 0.1719\n",
      "Iteration [2240], Loss: 0.2746, Error: 0.0781\n",
      "Iteration [2241], Loss: 0.2663, Error: 0.0625\n",
      "Iteration [2242], Loss: 0.3547, Error: 0.1094\n",
      "Iteration [2243], Loss: 0.2672, Error: 0.0625\n",
      "Iteration [2244], Loss: 0.2454, Error: 0.0469\n",
      "Iteration [2245], Loss: 0.3221, Error: 0.0781\n",
      "Iteration [2246], Loss: 0.3858, Error: 0.0781\n",
      "Iteration [2247], Loss: 0.2439, Error: 0.0781\n",
      "Iteration [2248], Loss: 0.2593, Error: 0.0938\n",
      "Iteration [2249], Loss: 0.3670, Error: 0.0781\n",
      "Iteration [2250], Loss: 0.3157, Error: 0.1250\n",
      "Iteration [2251], Loss: 0.4658, Error: 0.1250\n",
      "Iteration [2252], Loss: 0.3470, Error: 0.1250\n",
      "Iteration [2253], Loss: 0.1815, Error: 0.0312\n",
      "Iteration [2254], Loss: 0.3647, Error: 0.1094\n",
      "Iteration [2255], Loss: 0.3321, Error: 0.0938\n",
      "Iteration [2256], Loss: 0.2954, Error: 0.0938\n",
      "Iteration [2257], Loss: 0.4475, Error: 0.1406\n",
      "Iteration [2258], Loss: 0.2730, Error: 0.0781\n",
      "Iteration [2259], Loss: 0.3034, Error: 0.0938\n",
      "Iteration [2260], Loss: 0.3410, Error: 0.0781\n",
      "Iteration [2261], Loss: 0.2721, Error: 0.0781\n",
      "Iteration [2262], Loss: 0.1728, Error: 0.0625\n",
      "Iteration [2263], Loss: 0.3216, Error: 0.0781\n",
      "Iteration [2264], Loss: 0.3066, Error: 0.1094\n",
      "Iteration [2265], Loss: 0.2201, Error: 0.0312\n",
      "Iteration [2266], Loss: 0.3070, Error: 0.0781\n",
      "Iteration [2267], Loss: 0.2400, Error: 0.0469\n",
      "Iteration [2268], Loss: 0.3445, Error: 0.1094\n",
      "Iteration [2269], Loss: 0.2753, Error: 0.0781\n",
      "Iteration [2270], Loss: 0.3686, Error: 0.1094\n",
      "Iteration [2271], Loss: 0.3340, Error: 0.1094\n",
      "Iteration [2272], Loss: 0.3039, Error: 0.0938\n",
      "Iteration [2273], Loss: 0.1474, Error: 0.0312\n",
      "Iteration [2274], Loss: 0.1984, Error: 0.0312\n",
      "Iteration [2275], Loss: 0.2224, Error: 0.0469\n",
      "Iteration [2276], Loss: 0.2568, Error: 0.0625\n",
      "Iteration [2277], Loss: 0.3684, Error: 0.0938\n",
      "Iteration [2278], Loss: 0.3632, Error: 0.1250\n",
      "Iteration [2279], Loss: 0.3839, Error: 0.1250\n",
      "Iteration [2280], Loss: 0.3415, Error: 0.0781\n",
      "Iteration [2281], Loss: 0.3246, Error: 0.0938\n",
      "Iteration [2282], Loss: 0.5055, Error: 0.1562\n",
      "Iteration [2283], Loss: 0.3215, Error: 0.0781\n",
      "Iteration [2284], Loss: 0.2052, Error: 0.0469\n",
      "Iteration [2285], Loss: 0.2208, Error: 0.0469\n",
      "Iteration [2286], Loss: 0.4081, Error: 0.1094\n",
      "Iteration [2287], Loss: 0.2928, Error: 0.0469\n",
      "Iteration [2288], Loss: 0.1465, Error: 0.0312\n",
      "Iteration [2289], Loss: 0.5407, Error: 0.1562\n",
      "Iteration [2290], Loss: 0.5053, Error: 0.1250\n",
      "Iteration [2291], Loss: 0.3138, Error: 0.0781\n",
      "Iteration [2292], Loss: 0.2750, Error: 0.0625\n",
      "Iteration [2293], Loss: 0.1854, Error: 0.0469\n",
      "Iteration [2294], Loss: 0.4337, Error: 0.0938\n",
      "Iteration [2295], Loss: 0.5452, Error: 0.1875\n",
      "Iteration [2296], Loss: 0.2165, Error: 0.0469\n",
      "Iteration [2297], Loss: 0.1512, Error: 0.0469\n",
      "Iteration [2298], Loss: 0.2059, Error: 0.0469\n",
      "Iteration [2299], Loss: 0.3837, Error: 0.0938\n",
      "Iteration [2300], Loss: 0.5509, Error: 0.1562\n",
      "Iteration [2301], Loss: 0.3373, Error: 0.1250\n",
      "Iteration [2302], Loss: 0.4077, Error: 0.1094\n",
      "Iteration [2303], Loss: 0.5168, Error: 0.1250\n",
      "Iteration [2304], Loss: 0.1931, Error: 0.0625\n",
      "Iteration [2305], Loss: 0.3847, Error: 0.0625\n",
      "Iteration [2306], Loss: 0.2390, Error: 0.0781\n",
      "Iteration [2307], Loss: 0.4048, Error: 0.0938\n",
      "Iteration [2308], Loss: 0.1795, Error: 0.0469\n",
      "Iteration [2309], Loss: 0.1519, Error: 0.0156\n",
      "Iteration [2310], Loss: 0.2697, Error: 0.0781\n",
      "Iteration [2311], Loss: 0.2272, Error: 0.0781\n",
      "Iteration [2312], Loss: 0.2456, Error: 0.0625\n",
      "Iteration [2313], Loss: 0.4452, Error: 0.1250\n",
      "Iteration [2314], Loss: 0.4865, Error: 0.1719\n",
      "Iteration [2315], Loss: 0.3457, Error: 0.1250\n",
      "Iteration [2316], Loss: 0.2327, Error: 0.0469\n",
      "Iteration [2317], Loss: 0.3834, Error: 0.1250\n",
      "Iteration [2318], Loss: 0.3148, Error: 0.0938\n",
      "Iteration [2319], Loss: 0.2832, Error: 0.0938\n",
      "Iteration [2320], Loss: 0.4071, Error: 0.1094\n",
      "Iteration [2321], Loss: 0.2290, Error: 0.0625\n",
      "Iteration [2322], Loss: 0.2719, Error: 0.0938\n",
      "Iteration [2323], Loss: 0.2218, Error: 0.0781\n",
      "Iteration [2324], Loss: 0.3738, Error: 0.1094\n",
      "Iteration [2325], Loss: 0.4257, Error: 0.1250\n",
      "Iteration [2326], Loss: 0.4405, Error: 0.1250\n",
      "Iteration [2327], Loss: 0.5009, Error: 0.1562\n",
      "Iteration [2328], Loss: 0.4038, Error: 0.1094\n",
      "Iteration [2329], Loss: 0.1982, Error: 0.0469\n",
      "Iteration [2330], Loss: 0.3094, Error: 0.0938\n",
      "Iteration [2331], Loss: 0.3983, Error: 0.1094\n",
      "Iteration [2332], Loss: 0.3741, Error: 0.1094\n",
      "Iteration [2333], Loss: 0.1512, Error: 0.0312\n",
      "Iteration [2334], Loss: 0.2329, Error: 0.0625\n",
      "Iteration [2335], Loss: 0.1912, Error: 0.0781\n",
      "Iteration [2336], Loss: 0.2885, Error: 0.0781\n",
      "Iteration [2337], Loss: 0.2725, Error: 0.0938\n",
      "Iteration [2338], Loss: 0.3784, Error: 0.1094\n",
      "Iteration [2339], Loss: 0.4876, Error: 0.0938\n",
      "Iteration [2340], Loss: 0.3841, Error: 0.1250\n",
      "Iteration [2341], Loss: 0.4182, Error: 0.1562\n",
      "Iteration [2342], Loss: 0.3541, Error: 0.1094\n",
      "Iteration [2343], Loss: 0.4991, Error: 0.1719\n",
      "Iteration [2344], Loss: 0.1847, Error: 0.0417\n",
      "Iteration [2345], Loss: 0.4305, Error: 0.1094\n",
      "Iteration [2346], Loss: 0.1082, Error: 0.0156\n",
      "Iteration [2347], Loss: 0.2781, Error: 0.0625\n",
      "Iteration [2348], Loss: 0.4304, Error: 0.0781\n",
      "Iteration [2349], Loss: 0.3387, Error: 0.1250\n",
      "Iteration [2350], Loss: 0.2745, Error: 0.0625\n",
      "Iteration [2351], Loss: 0.3514, Error: 0.0938\n",
      "Iteration [2352], Loss: 0.2380, Error: 0.0469\n",
      "Iteration [2353], Loss: 0.2283, Error: 0.0625\n",
      "Iteration [2354], Loss: 0.2485, Error: 0.0781\n",
      "Iteration [2355], Loss: 0.2718, Error: 0.0781\n",
      "Iteration [2356], Loss: 0.2296, Error: 0.0312\n",
      "Iteration [2357], Loss: 0.5482, Error: 0.1562\n",
      "Iteration [2358], Loss: 0.2348, Error: 0.0781\n",
      "Iteration [2359], Loss: 0.3432, Error: 0.1094\n",
      "Iteration [2360], Loss: 0.2316, Error: 0.0469\n",
      "Iteration [2361], Loss: 0.2187, Error: 0.0625\n",
      "Iteration [2362], Loss: 0.3344, Error: 0.0625\n",
      "Iteration [2363], Loss: 0.3793, Error: 0.0625\n",
      "Iteration [2364], Loss: 0.1248, Error: 0.0000\n",
      "Iteration [2365], Loss: 0.3707, Error: 0.1094\n",
      "Iteration [2366], Loss: 0.1758, Error: 0.0625\n",
      "Iteration [2367], Loss: 0.3922, Error: 0.0625\n",
      "Iteration [2368], Loss: 0.2880, Error: 0.0625\n",
      "Iteration [2369], Loss: 0.3117, Error: 0.1250\n",
      "Iteration [2370], Loss: 0.3594, Error: 0.0938\n",
      "Iteration [2371], Loss: 0.3563, Error: 0.0781\n",
      "Iteration [2372], Loss: 0.2438, Error: 0.0469\n",
      "Iteration [2373], Loss: 0.2263, Error: 0.0625\n",
      "Iteration [2374], Loss: 0.5489, Error: 0.1250\n",
      "Iteration [2375], Loss: 0.2511, Error: 0.0781\n",
      "Iteration [2376], Loss: 0.3359, Error: 0.1094\n",
      "Iteration [2377], Loss: 0.2528, Error: 0.0625\n",
      "Iteration [2378], Loss: 0.3444, Error: 0.0938\n",
      "Iteration [2379], Loss: 0.2204, Error: 0.0469\n",
      "Iteration [2380], Loss: 0.2368, Error: 0.0625\n",
      "Iteration [2381], Loss: 0.5515, Error: 0.1094\n",
      "Iteration [2382], Loss: 0.2202, Error: 0.0469\n",
      "Iteration [2383], Loss: 0.2903, Error: 0.0781\n",
      "Iteration [2384], Loss: 0.4517, Error: 0.1250\n",
      "Iteration [2385], Loss: 0.2587, Error: 0.0625\n",
      "Iteration [2386], Loss: 0.2950, Error: 0.0469\n",
      "Iteration [2387], Loss: 0.2372, Error: 0.0625\n",
      "Iteration [2388], Loss: 0.3988, Error: 0.0781\n",
      "Iteration [2389], Loss: 0.2380, Error: 0.0312\n",
      "Iteration [2390], Loss: 0.2754, Error: 0.0469\n",
      "Iteration [2391], Loss: 0.2864, Error: 0.1094\n",
      "Iteration [2392], Loss: 0.1561, Error: 0.0469\n",
      "Iteration [2393], Loss: 0.2671, Error: 0.0781\n",
      "Iteration [2394], Loss: 0.3780, Error: 0.0938\n",
      "Iteration [2395], Loss: 0.1142, Error: 0.0312\n",
      "Iteration [2396], Loss: 0.3520, Error: 0.0781\n",
      "Iteration [2397], Loss: 0.4768, Error: 0.1719\n",
      "Iteration [2398], Loss: 0.2406, Error: 0.0781\n",
      "Iteration [2399], Loss: 0.2462, Error: 0.0938\n",
      "Iteration [2400], Loss: 0.3095, Error: 0.0781\n",
      "Iteration [2401], Loss: 0.3024, Error: 0.0938\n",
      "Iteration [2402], Loss: 0.3998, Error: 0.0938\n",
      "Iteration [2403], Loss: 0.2668, Error: 0.0781\n",
      "Iteration [2404], Loss: 0.1609, Error: 0.0312\n",
      "Iteration [2405], Loss: 0.3517, Error: 0.0625\n",
      "Iteration [2406], Loss: 0.4742, Error: 0.1094\n",
      "Iteration [2407], Loss: 0.4199, Error: 0.1250\n",
      "Iteration [2408], Loss: 0.2609, Error: 0.0312\n",
      "Iteration [2409], Loss: 0.1688, Error: 0.0469\n",
      "Iteration [2410], Loss: 0.2864, Error: 0.0625\n",
      "Iteration [2411], Loss: 0.3236, Error: 0.1250\n",
      "Iteration [2412], Loss: 0.1647, Error: 0.0469\n",
      "Iteration [2413], Loss: 0.2231, Error: 0.0625\n",
      "Iteration [2414], Loss: 0.4181, Error: 0.1094\n",
      "Iteration [2415], Loss: 0.1647, Error: 0.0469\n",
      "Iteration [2416], Loss: 0.4257, Error: 0.1250\n",
      "Iteration [2417], Loss: 0.2695, Error: 0.0781\n",
      "Iteration [2418], Loss: 0.3284, Error: 0.0625\n",
      "Iteration [2419], Loss: 0.4397, Error: 0.1406\n",
      "Iteration [2420], Loss: 0.3700, Error: 0.0938\n",
      "Iteration [2421], Loss: 0.3498, Error: 0.1094\n",
      "Iteration [2422], Loss: 0.3410, Error: 0.0469\n",
      "Iteration [2423], Loss: 0.4855, Error: 0.1094\n",
      "Iteration [2424], Loss: 0.1577, Error: 0.0312\n",
      "Iteration [2425], Loss: 0.3846, Error: 0.1094\n",
      "Iteration [2426], Loss: 0.2904, Error: 0.0625\n",
      "Iteration [2427], Loss: 0.4242, Error: 0.1094\n",
      "Iteration [2428], Loss: 0.3091, Error: 0.0469\n",
      "Iteration [2429], Loss: 0.3784, Error: 0.1094\n",
      "Iteration [2430], Loss: 0.3177, Error: 0.0781\n",
      "Iteration [2431], Loss: 0.2937, Error: 0.0781\n",
      "Iteration [2432], Loss: 0.4170, Error: 0.1250\n",
      "Iteration [2433], Loss: 0.2213, Error: 0.0469\n",
      "Iteration [2434], Loss: 0.3600, Error: 0.1094\n",
      "Iteration [2435], Loss: 0.2141, Error: 0.0625\n",
      "Iteration [2436], Loss: 0.4301, Error: 0.0625\n",
      "Iteration [2437], Loss: 0.2739, Error: 0.0938\n",
      "Iteration [2438], Loss: 0.2264, Error: 0.0469\n",
      "Iteration [2439], Loss: 0.2826, Error: 0.0469\n",
      "Iteration [2440], Loss: 0.2628, Error: 0.0781\n",
      "Iteration [2441], Loss: 0.4553, Error: 0.1719\n",
      "Iteration [2442], Loss: 0.5382, Error: 0.1250\n",
      "Iteration [2443], Loss: 0.3686, Error: 0.1094\n",
      "Iteration [2444], Loss: 0.4178, Error: 0.1250\n",
      "Iteration [2445], Loss: 0.5177, Error: 0.1719\n",
      "Iteration [2446], Loss: 0.2475, Error: 0.0781\n",
      "Iteration [2447], Loss: 0.3370, Error: 0.0781\n",
      "Iteration [2448], Loss: 0.5153, Error: 0.1562\n",
      "Iteration [2449], Loss: 0.1718, Error: 0.0156\n",
      "Iteration [2450], Loss: 0.4857, Error: 0.1406\n",
      "Iteration [2451], Loss: 0.3261, Error: 0.1094\n",
      "Iteration [2452], Loss: 0.1783, Error: 0.0312\n",
      "Iteration [2453], Loss: 0.2178, Error: 0.0938\n",
      "Iteration [2454], Loss: 0.1105, Error: 0.0156\n",
      "Iteration [2455], Loss: 0.3170, Error: 0.1094\n",
      "Iteration [2456], Loss: 0.3522, Error: 0.1250\n",
      "Iteration [2457], Loss: 0.4045, Error: 0.1094\n",
      "Iteration [2458], Loss: 0.4120, Error: 0.1406\n",
      "Iteration [2459], Loss: 0.3054, Error: 0.0938\n",
      "Iteration [2460], Loss: 0.2228, Error: 0.0469\n",
      "Iteration [2461], Loss: 0.2526, Error: 0.0625\n",
      "Iteration [2462], Loss: 0.3057, Error: 0.0938\n",
      "Iteration [2463], Loss: 0.4196, Error: 0.0938\n",
      "Iteration [2464], Loss: 0.3987, Error: 0.1094\n",
      "Iteration [2465], Loss: 0.1859, Error: 0.0469\n",
      "Iteration [2466], Loss: 0.1864, Error: 0.0469\n",
      "Iteration [2467], Loss: 0.2461, Error: 0.0781\n",
      "Iteration [2468], Loss: 0.2465, Error: 0.0625\n",
      "Iteration [2469], Loss: 0.3439, Error: 0.0781\n",
      "Iteration [2470], Loss: 0.4231, Error: 0.1719\n",
      "Iteration [2471], Loss: 0.3460, Error: 0.0938\n",
      "Iteration [2472], Loss: 0.3792, Error: 0.0938\n",
      "Iteration [2473], Loss: 0.2372, Error: 0.0781\n",
      "Iteration [2474], Loss: 0.3378, Error: 0.0781\n",
      "Iteration [2475], Loss: 0.2861, Error: 0.0781\n",
      "Iteration [2476], Loss: 0.2436, Error: 0.0469\n",
      "Iteration [2477], Loss: 0.3252, Error: 0.1094\n",
      "Iteration [2478], Loss: 0.2424, Error: 0.0938\n",
      "Iteration [2479], Loss: 0.4316, Error: 0.1875\n",
      "Iteration [2480], Loss: 0.3569, Error: 0.1406\n",
      "Iteration [2481], Loss: 0.3277, Error: 0.0625\n",
      "Iteration [2482], Loss: 0.4525, Error: 0.1250\n",
      "Iteration [2483], Loss: 0.3007, Error: 0.0781\n",
      "Iteration [2484], Loss: 0.3903, Error: 0.1719\n",
      "Iteration [2485], Loss: 0.1983, Error: 0.0625\n",
      "Iteration [2486], Loss: 0.4313, Error: 0.1094\n",
      "Iteration [2487], Loss: 0.2567, Error: 0.0781\n",
      "Iteration [2488], Loss: 0.3736, Error: 0.0938\n",
      "Iteration [2489], Loss: 0.3996, Error: 0.0625\n",
      "Iteration [2490], Loss: 0.1542, Error: 0.0469\n",
      "Iteration [2491], Loss: 0.2753, Error: 0.0938\n",
      "Iteration [2492], Loss: 0.2740, Error: 0.1094\n",
      "Iteration [2493], Loss: 0.1782, Error: 0.0469\n",
      "Iteration [2494], Loss: 0.2943, Error: 0.0938\n",
      "Iteration [2495], Loss: 0.4294, Error: 0.1094\n",
      "Iteration [2496], Loss: 0.3371, Error: 0.0938\n",
      "Iteration [2497], Loss: 0.3329, Error: 0.1094\n",
      "Iteration [2498], Loss: 0.3112, Error: 0.0781\n",
      "Iteration [2499], Loss: 0.2887, Error: 0.0469\n",
      "Iteration [2500], Loss: 0.2517, Error: 0.0781\n",
      "Iteration [2501], Loss: 0.1698, Error: 0.0312\n",
      "Iteration [2502], Loss: 0.2425, Error: 0.0469\n",
      "Iteration [2503], Loss: 0.2280, Error: 0.0469\n",
      "Iteration [2504], Loss: 0.4296, Error: 0.1250\n",
      "Iteration [2505], Loss: 0.5111, Error: 0.1719\n",
      "Iteration [2506], Loss: 0.3476, Error: 0.0781\n",
      "Iteration [2507], Loss: 0.2903, Error: 0.0781\n",
      "Iteration [2508], Loss: 0.3848, Error: 0.1406\n",
      "Iteration [2509], Loss: 0.2538, Error: 0.0781\n",
      "Iteration [2510], Loss: 0.3485, Error: 0.1250\n",
      "Iteration [2511], Loss: 0.4417, Error: 0.1094\n",
      "Iteration [2512], Loss: 0.3333, Error: 0.0938\n",
      "Iteration [2513], Loss: 0.3084, Error: 0.0625\n",
      "Iteration [2514], Loss: 0.1936, Error: 0.0469\n",
      "Iteration [2515], Loss: 0.2789, Error: 0.0781\n",
      "Iteration [2516], Loss: 0.4199, Error: 0.1094\n",
      "Iteration [2517], Loss: 0.2999, Error: 0.1094\n",
      "Iteration [2518], Loss: 0.3505, Error: 0.0781\n",
      "Iteration [2519], Loss: 0.2504, Error: 0.1094\n",
      "Iteration [2520], Loss: 0.2470, Error: 0.0469\n",
      "Iteration [2521], Loss: 0.3520, Error: 0.0781\n",
      "Iteration [2522], Loss: 0.2135, Error: 0.0781\n",
      "Iteration [2523], Loss: 0.1944, Error: 0.0625\n",
      "Iteration [2524], Loss: 0.2521, Error: 0.0938\n",
      "Iteration [2525], Loss: 0.3596, Error: 0.1094\n",
      "Iteration [2526], Loss: 0.3315, Error: 0.1094\n",
      "Iteration [2527], Loss: 0.5688, Error: 0.1250\n",
      "Iteration [2528], Loss: 0.4295, Error: 0.0938\n",
      "Iteration [2529], Loss: 0.3685, Error: 0.0938\n",
      "Iteration [2530], Loss: 0.1511, Error: 0.0156\n",
      "Iteration [2531], Loss: 0.3594, Error: 0.1562\n",
      "Iteration [2532], Loss: 0.3124, Error: 0.0781\n",
      "Iteration [2533], Loss: 0.3318, Error: 0.1094\n",
      "Iteration [2534], Loss: 0.5600, Error: 0.1094\n",
      "Iteration [2535], Loss: 0.2573, Error: 0.0781\n",
      "Iteration [2536], Loss: 0.1722, Error: 0.0312\n",
      "Iteration [2537], Loss: 0.3091, Error: 0.0625\n",
      "Iteration [2538], Loss: 0.3364, Error: 0.1094\n",
      "Iteration [2539], Loss: 0.3270, Error: 0.0938\n",
      "Iteration [2540], Loss: 0.3362, Error: 0.0938\n",
      "Iteration [2541], Loss: 0.4136, Error: 0.0938\n",
      "Iteration [2542], Loss: 0.2456, Error: 0.0625\n",
      "Iteration [2543], Loss: 0.2013, Error: 0.0625\n",
      "Iteration [2544], Loss: 0.2668, Error: 0.0781\n",
      "Iteration [2545], Loss: 0.3448, Error: 0.0938\n",
      "Iteration [2546], Loss: 0.3220, Error: 0.0938\n",
      "Iteration [2547], Loss: 0.1457, Error: 0.0156\n",
      "Iteration [2548], Loss: 0.1942, Error: 0.0625\n",
      "Iteration [2549], Loss: 0.3778, Error: 0.1094\n",
      "Iteration [2550], Loss: 0.4151, Error: 0.1719\n",
      "Iteration [2551], Loss: 0.3503, Error: 0.0938\n",
      "Iteration [2552], Loss: 0.1840, Error: 0.0312\n",
      "Iteration [2553], Loss: 0.3403, Error: 0.1250\n",
      "Iteration [2554], Loss: 0.3068, Error: 0.0938\n",
      "Iteration [2555], Loss: 0.2651, Error: 0.0625\n",
      "Iteration [2556], Loss: 0.1830, Error: 0.0625\n",
      "Iteration [2557], Loss: 0.3643, Error: 0.0938\n",
      "Iteration [2558], Loss: 0.2449, Error: 0.0625\n",
      "Iteration [2559], Loss: 0.2906, Error: 0.0938\n",
      "Iteration [2560], Loss: 0.3911, Error: 0.1406\n",
      "Iteration [2561], Loss: 0.3431, Error: 0.0625\n",
      "Iteration [2562], Loss: 0.2782, Error: 0.0781\n",
      "Iteration [2563], Loss: 0.3315, Error: 0.1094\n",
      "Iteration [2564], Loss: 0.2754, Error: 0.0625\n",
      "Iteration [2565], Loss: 0.2365, Error: 0.0781\n",
      "Iteration [2566], Loss: 0.2738, Error: 0.0469\n",
      "Iteration [2567], Loss: 0.4421, Error: 0.1094\n",
      "Iteration [2568], Loss: 0.4970, Error: 0.0938\n",
      "Iteration [2569], Loss: 0.3109, Error: 0.1250\n",
      "Iteration [2570], Loss: 0.2686, Error: 0.0625\n",
      "Iteration [2571], Loss: 0.4390, Error: 0.1250\n",
      "Iteration [2572], Loss: 0.2687, Error: 0.0781\n",
      "Iteration [2573], Loss: 0.3998, Error: 0.1094\n",
      "Iteration [2574], Loss: 0.1759, Error: 0.0469\n",
      "Iteration [2575], Loss: 0.2882, Error: 0.1250\n",
      "Iteration [2576], Loss: 0.2766, Error: 0.0781\n",
      "Iteration [2577], Loss: 0.2920, Error: 0.0938\n",
      "Iteration [2578], Loss: 0.1815, Error: 0.0625\n",
      "Iteration [2579], Loss: 0.2031, Error: 0.0469\n",
      "Iteration [2580], Loss: 0.4829, Error: 0.1250\n",
      "Iteration [2581], Loss: 0.1331, Error: 0.0156\n",
      "Iteration [2582], Loss: 0.3729, Error: 0.0781\n",
      "Iteration [2583], Loss: 0.2442, Error: 0.0625\n",
      "Iteration [2584], Loss: 0.2475, Error: 0.0781\n",
      "Iteration [2585], Loss: 0.3671, Error: 0.0938\n",
      "Iteration [2586], Loss: 0.4235, Error: 0.0781\n",
      "Iteration [2587], Loss: 0.2340, Error: 0.0781\n",
      "Iteration [2588], Loss: 0.2465, Error: 0.0625\n",
      "Iteration [2589], Loss: 0.2196, Error: 0.0469\n",
      "Iteration [2590], Loss: 0.2636, Error: 0.0781\n",
      "Iteration [2591], Loss: 0.3896, Error: 0.1094\n",
      "Iteration [2592], Loss: 0.1590, Error: 0.0469\n",
      "Iteration [2593], Loss: 0.2310, Error: 0.0625\n",
      "Iteration [2594], Loss: 0.2717, Error: 0.1094\n",
      "Iteration [2595], Loss: 0.4588, Error: 0.1406\n",
      "Iteration [2596], Loss: 0.2244, Error: 0.0469\n",
      "Iteration [2597], Loss: 0.3733, Error: 0.1094\n",
      "Iteration [2598], Loss: 0.2374, Error: 0.0781\n",
      "Iteration [2599], Loss: 0.4426, Error: 0.1094\n",
      "Iteration [2600], Loss: 0.3966, Error: 0.1406\n",
      "Iteration [2601], Loss: 0.6605, Error: 0.1562\n",
      "Iteration [2602], Loss: 0.3219, Error: 0.1094\n",
      "Iteration [2603], Loss: 0.2335, Error: 0.0781\n",
      "Iteration [2604], Loss: 0.3201, Error: 0.0781\n",
      "Iteration [2605], Loss: 0.3961, Error: 0.1094\n",
      "Iteration [2606], Loss: 0.4143, Error: 0.0938\n",
      "Iteration [2607], Loss: 0.3549, Error: 0.0625\n",
      "Iteration [2608], Loss: 0.4648, Error: 0.1406\n",
      "Iteration [2609], Loss: 0.2271, Error: 0.0781\n",
      "Iteration [2610], Loss: 0.1640, Error: 0.0469\n",
      "Iteration [2611], Loss: 0.1854, Error: 0.0469\n",
      "Iteration [2612], Loss: 0.3234, Error: 0.0938\n",
      "Iteration [2613], Loss: 0.2734, Error: 0.0469\n",
      "Iteration [2614], Loss: 0.3530, Error: 0.0938\n",
      "Iteration [2615], Loss: 0.2779, Error: 0.0469\n",
      "Iteration [2616], Loss: 0.2461, Error: 0.0781\n",
      "Iteration [2617], Loss: 0.4214, Error: 0.1250\n",
      "Iteration [2618], Loss: 0.5557, Error: 0.1250\n",
      "Iteration [2619], Loss: 0.2468, Error: 0.0625\n",
      "Iteration [2620], Loss: 0.2346, Error: 0.0781\n",
      "Iteration [2621], Loss: 0.3389, Error: 0.0625\n",
      "Iteration [2622], Loss: 0.6583, Error: 0.2031\n",
      "Iteration [2623], Loss: 0.3257, Error: 0.0938\n",
      "Iteration [2624], Loss: 0.2231, Error: 0.0781\n",
      "Iteration [2625], Loss: 0.3591, Error: 0.0938\n",
      "Iteration [2626], Loss: 0.5122, Error: 0.1406\n",
      "Iteration [2627], Loss: 0.4475, Error: 0.1562\n",
      "Iteration [2628], Loss: 0.3569, Error: 0.0938\n",
      "Iteration [2629], Loss: 0.2309, Error: 0.0781\n",
      "Iteration [2630], Loss: 0.1880, Error: 0.0469\n",
      "Iteration [2631], Loss: 0.3997, Error: 0.1406\n",
      "Iteration [2632], Loss: 0.3057, Error: 0.0781\n",
      "Iteration [2633], Loss: 0.2209, Error: 0.0469\n",
      "Iteration [2634], Loss: 0.2707, Error: 0.0625\n",
      "Iteration [2635], Loss: 0.2238, Error: 0.0781\n",
      "Iteration [2636], Loss: 0.2392, Error: 0.0781\n",
      "Iteration [2637], Loss: 0.3603, Error: 0.0781\n",
      "Iteration [2638], Loss: 0.1629, Error: 0.0312\n",
      "Iteration [2639], Loss: 0.4498, Error: 0.1250\n",
      "Iteration [2640], Loss: 0.3660, Error: 0.1250\n",
      "Iteration [2641], Loss: 0.1402, Error: 0.0000\n",
      "Iteration [2642], Loss: 0.2732, Error: 0.0938\n",
      "Iteration [2643], Loss: 0.5086, Error: 0.1250\n",
      "Iteration [2644], Loss: 0.2791, Error: 0.1094\n",
      "Iteration [2645], Loss: 0.2414, Error: 0.0781\n",
      "Iteration [2646], Loss: 0.4448, Error: 0.1250\n",
      "Iteration [2647], Loss: 0.2915, Error: 0.0781\n",
      "Iteration [2648], Loss: 0.4050, Error: 0.1562\n",
      "Iteration [2649], Loss: 0.1987, Error: 0.0469\n",
      "Iteration [2650], Loss: 0.2380, Error: 0.0625\n",
      "Iteration [2651], Loss: 0.4028, Error: 0.1094\n",
      "Iteration [2652], Loss: 0.2321, Error: 0.0625\n",
      "Iteration [2653], Loss: 0.4007, Error: 0.1094\n",
      "Iteration [2654], Loss: 0.3745, Error: 0.0625\n",
      "Iteration [2655], Loss: 0.2152, Error: 0.0625\n",
      "Iteration [2656], Loss: 0.3413, Error: 0.0469\n",
      "Iteration [2657], Loss: 0.3202, Error: 0.0781\n",
      "Iteration [2658], Loss: 0.3508, Error: 0.1406\n",
      "Iteration [2659], Loss: 0.5883, Error: 0.1250\n",
      "Iteration [2660], Loss: 0.1569, Error: 0.0312\n",
      "Iteration [2661], Loss: 0.2607, Error: 0.0781\n",
      "Iteration [2662], Loss: 0.3266, Error: 0.0781\n",
      "Iteration [2663], Loss: 0.3971, Error: 0.0938\n",
      "Iteration [2664], Loss: 0.2990, Error: 0.1094\n",
      "Iteration [2665], Loss: 0.3260, Error: 0.0781\n",
      "Iteration [2666], Loss: 0.3230, Error: 0.0781\n",
      "Iteration [2667], Loss: 0.4215, Error: 0.1250\n",
      "Iteration [2668], Loss: 0.5237, Error: 0.1719\n",
      "Iteration [2669], Loss: 0.3278, Error: 0.1719\n",
      "Iteration [2670], Loss: 0.2111, Error: 0.0469\n",
      "Iteration [2671], Loss: 0.4078, Error: 0.0781\n",
      "Iteration [2672], Loss: 0.3031, Error: 0.1094\n",
      "Iteration [2673], Loss: 0.2353, Error: 0.0469\n",
      "Iteration [2674], Loss: 0.2523, Error: 0.0781\n",
      "Iteration [2675], Loss: 0.2804, Error: 0.0312\n",
      "Iteration [2676], Loss: 0.2084, Error: 0.0781\n",
      "Iteration [2677], Loss: 0.2874, Error: 0.0938\n",
      "Iteration [2678], Loss: 0.2190, Error: 0.0156\n",
      "Iteration [2679], Loss: 0.1833, Error: 0.0469\n",
      "Iteration [2680], Loss: 0.3170, Error: 0.1250\n",
      "Iteration [2681], Loss: 0.2115, Error: 0.0625\n",
      "Iteration [2682], Loss: 0.5016, Error: 0.1250\n",
      "Iteration [2683], Loss: 0.2908, Error: 0.1094\n",
      "Iteration [2684], Loss: 0.1708, Error: 0.0469\n",
      "Iteration [2685], Loss: 0.2677, Error: 0.0625\n",
      "Iteration [2686], Loss: 0.2603, Error: 0.0781\n",
      "Iteration [2687], Loss: 0.1890, Error: 0.0469\n",
      "Iteration [2688], Loss: 0.4937, Error: 0.1406\n",
      "Iteration [2689], Loss: 0.2642, Error: 0.0938\n",
      "Iteration [2690], Loss: 0.3007, Error: 0.1094\n",
      "Iteration [2691], Loss: 0.2201, Error: 0.0469\n",
      "Iteration [2692], Loss: 0.2789, Error: 0.0938\n",
      "Iteration [2693], Loss: 0.2230, Error: 0.0625\n",
      "Iteration [2694], Loss: 0.3156, Error: 0.0625\n",
      "Iteration [2695], Loss: 0.1529, Error: 0.0156\n",
      "Iteration [2696], Loss: 0.1746, Error: 0.0312\n",
      "Iteration [2697], Loss: 0.1976, Error: 0.0469\n",
      "Iteration [2698], Loss: 0.3471, Error: 0.0938\n",
      "Iteration [2699], Loss: 0.3253, Error: 0.0938\n",
      "Iteration [2700], Loss: 0.2334, Error: 0.0781\n",
      "Iteration [2701], Loss: 0.2600, Error: 0.0781\n",
      "Iteration [2702], Loss: 0.2462, Error: 0.0938\n",
      "Iteration [2703], Loss: 0.4828, Error: 0.1094\n",
      "Iteration [2704], Loss: 0.2355, Error: 0.0781\n",
      "Iteration [2705], Loss: 0.3776, Error: 0.1094\n",
      "Iteration [2706], Loss: 0.3697, Error: 0.1250\n",
      "Iteration [2707], Loss: 0.4390, Error: 0.1562\n",
      "Iteration [2708], Loss: 0.2769, Error: 0.0938\n",
      "Iteration [2709], Loss: 0.2475, Error: 0.0625\n",
      "Iteration [2710], Loss: 0.4923, Error: 0.1094\n",
      "Iteration [2711], Loss: 0.2777, Error: 0.1094\n",
      "Iteration [2712], Loss: 0.2286, Error: 0.0469\n",
      "Iteration [2713], Loss: 0.3950, Error: 0.0938\n",
      "Iteration [2714], Loss: 0.2481, Error: 0.0625\n",
      "Iteration [2715], Loss: 0.1584, Error: 0.0312\n",
      "Iteration [2716], Loss: 0.2662, Error: 0.0781\n",
      "Iteration [2717], Loss: 0.3340, Error: 0.0781\n",
      "Iteration [2718], Loss: 0.4516, Error: 0.0938\n",
      "Iteration [2719], Loss: 0.3655, Error: 0.1094\n",
      "Iteration [2720], Loss: 0.2754, Error: 0.0625\n",
      "Iteration [2721], Loss: 0.4774, Error: 0.1406\n",
      "Iteration [2722], Loss: 0.2936, Error: 0.1250\n",
      "Iteration [2723], Loss: 0.2804, Error: 0.0625\n",
      "Iteration [2724], Loss: 0.2681, Error: 0.0938\n",
      "Iteration [2725], Loss: 0.5028, Error: 0.1094\n",
      "Iteration [2726], Loss: 0.3332, Error: 0.0938\n",
      "Iteration [2727], Loss: 0.3837, Error: 0.1250\n",
      "Iteration [2728], Loss: 0.2837, Error: 0.0781\n",
      "Iteration [2729], Loss: 0.2396, Error: 0.0312\n",
      "Iteration [2730], Loss: 0.3757, Error: 0.0781\n",
      "Iteration [2731], Loss: 0.2333, Error: 0.0781\n",
      "Iteration [2732], Loss: 0.4551, Error: 0.1406\n",
      "Iteration [2733], Loss: 0.4749, Error: 0.1719\n",
      "Iteration [2734], Loss: 0.1877, Error: 0.0312\n",
      "Iteration [2735], Loss: 0.2117, Error: 0.0312\n",
      "Iteration [2736], Loss: 0.3741, Error: 0.1094\n",
      "Iteration [2737], Loss: 0.2764, Error: 0.0469\n",
      "Iteration [2738], Loss: 0.2482, Error: 0.0781\n",
      "Iteration [2739], Loss: 0.3653, Error: 0.1094\n",
      "Iteration [2740], Loss: 0.3645, Error: 0.1250\n",
      "Iteration [2741], Loss: 0.5750, Error: 0.1094\n",
      "Iteration [2742], Loss: 0.4558, Error: 0.1562\n",
      "Iteration [2743], Loss: 0.2709, Error: 0.0625\n",
      "Iteration [2744], Loss: 0.3708, Error: 0.0625\n",
      "Iteration [2745], Loss: 0.2148, Error: 0.0312\n",
      "Iteration [2746], Loss: 0.3649, Error: 0.1406\n",
      "Iteration [2747], Loss: 0.2445, Error: 0.0625\n",
      "Iteration [2748], Loss: 0.3554, Error: 0.0938\n",
      "Iteration [2749], Loss: 0.4432, Error: 0.1250\n",
      "Iteration [2750], Loss: 0.2517, Error: 0.1406\n",
      "Iteration [2751], Loss: 0.4845, Error: 0.1406\n",
      "Iteration [2752], Loss: 0.5083, Error: 0.1250\n",
      "Iteration [2753], Loss: 0.3054, Error: 0.0625\n",
      "Iteration [2754], Loss: 0.3287, Error: 0.1094\n",
      "Iteration [2755], Loss: 0.4566, Error: 0.1406\n",
      "Iteration [2756], Loss: 0.1479, Error: 0.0156\n",
      "Iteration [2757], Loss: 0.4183, Error: 0.0938\n",
      "Iteration [2758], Loss: 0.1707, Error: 0.0469\n",
      "Iteration [2759], Loss: 0.4943, Error: 0.0938\n",
      "Iteration [2760], Loss: 0.6103, Error: 0.1094\n",
      "Iteration [2761], Loss: 0.3114, Error: 0.0938\n",
      "Iteration [2762], Loss: 0.3984, Error: 0.1250\n",
      "Iteration [2763], Loss: 0.4347, Error: 0.1406\n",
      "Iteration [2764], Loss: 0.2429, Error: 0.0625\n",
      "Iteration [2765], Loss: 0.1754, Error: 0.0625\n",
      "Iteration [2766], Loss: 0.2841, Error: 0.0312\n",
      "Iteration [2767], Loss: 0.3598, Error: 0.0938\n",
      "Iteration [2768], Loss: 0.2719, Error: 0.0938\n",
      "Iteration [2769], Loss: 0.1802, Error: 0.0312\n",
      "Iteration [2770], Loss: 0.2255, Error: 0.0781\n",
      "Iteration [2771], Loss: 0.4385, Error: 0.1094\n",
      "Iteration [2772], Loss: 0.4030, Error: 0.1406\n",
      "Iteration [2773], Loss: 0.3171, Error: 0.1094\n",
      "Iteration [2774], Loss: 0.0977, Error: 0.0156\n",
      "Iteration [2775], Loss: 0.3133, Error: 0.0938\n",
      "Iteration [2776], Loss: 0.3033, Error: 0.0938\n",
      "Iteration [2777], Loss: 0.1424, Error: 0.0156\n",
      "Iteration [2778], Loss: 0.2241, Error: 0.0625\n",
      "Iteration [2779], Loss: 0.3359, Error: 0.1250\n",
      "Iteration [2780], Loss: 0.5899, Error: 0.1406\n",
      "Iteration [2781], Loss: 0.2163, Error: 0.0469\n",
      "Iteration [2782], Loss: 0.2652, Error: 0.0781\n",
      "Iteration [2783], Loss: 0.2434, Error: 0.0781\n",
      "Iteration [2784], Loss: 0.2680, Error: 0.0625\n",
      "Iteration [2785], Loss: 0.2717, Error: 0.1250\n",
      "Iteration [2786], Loss: 0.2419, Error: 0.0625\n",
      "Iteration [2787], Loss: 0.4060, Error: 0.1094\n",
      "Iteration [2788], Loss: 0.4058, Error: 0.1406\n",
      "Iteration [2789], Loss: 0.3864, Error: 0.0938\n",
      "Iteration [2790], Loss: 0.4734, Error: 0.1094\n",
      "Iteration [2791], Loss: 0.2467, Error: 0.0625\n",
      "Iteration [2792], Loss: 0.4266, Error: 0.1094\n",
      "Iteration [2793], Loss: 0.3677, Error: 0.1094\n",
      "Iteration [2794], Loss: 0.3143, Error: 0.0781\n",
      "Iteration [2795], Loss: 0.2530, Error: 0.0469\n",
      "Iteration [2796], Loss: 0.2933, Error: 0.0781\n",
      "Iteration [2797], Loss: 0.3102, Error: 0.0938\n",
      "Iteration [2798], Loss: 0.2961, Error: 0.0781\n",
      "Iteration [2799], Loss: 0.0986, Error: 0.0156\n",
      "Iteration [2800], Loss: 0.2607, Error: 0.0469\n",
      "Iteration [2801], Loss: 0.5618, Error: 0.2031\n",
      "Iteration [2802], Loss: 0.2071, Error: 0.0625\n",
      "Iteration [2803], Loss: 0.3219, Error: 0.0938\n",
      "Iteration [2804], Loss: 0.3064, Error: 0.1250\n",
      "Iteration [2805], Loss: 0.2110, Error: 0.0469\n",
      "Iteration [2806], Loss: 0.1997, Error: 0.0781\n",
      "Iteration [2807], Loss: 0.2407, Error: 0.0781\n",
      "Iteration [2808], Loss: 0.3486, Error: 0.0938\n",
      "Iteration [2809], Loss: 0.3619, Error: 0.0781\n",
      "Iteration [2810], Loss: 0.4135, Error: 0.1094\n",
      "Iteration [2811], Loss: 0.3424, Error: 0.0938\n",
      "Iteration [2812], Loss: 0.3055, Error: 0.1094\n",
      "Iteration [2813], Loss: 0.3506, Error: 0.0833\n",
      "Iteration [2814], Loss: 0.3342, Error: 0.0781\n",
      "Iteration [2815], Loss: 0.2051, Error: 0.0312\n",
      "Iteration [2816], Loss: 0.2702, Error: 0.0781\n",
      "Iteration [2817], Loss: 0.2889, Error: 0.0938\n",
      "Iteration [2818], Loss: 0.2847, Error: 0.0312\n",
      "Iteration [2819], Loss: 0.2965, Error: 0.0625\n",
      "Iteration [2820], Loss: 0.3292, Error: 0.1094\n",
      "Iteration [2821], Loss: 0.3702, Error: 0.1094\n",
      "Iteration [2822], Loss: 0.3118, Error: 0.0938\n",
      "Iteration [2823], Loss: 0.3139, Error: 0.1250\n",
      "Iteration [2824], Loss: 0.3447, Error: 0.0938\n",
      "Iteration [2825], Loss: 0.3529, Error: 0.0938\n",
      "Iteration [2826], Loss: 0.3768, Error: 0.1094\n",
      "Iteration [2827], Loss: 0.2485, Error: 0.0625\n",
      "Iteration [2828], Loss: 0.2664, Error: 0.1094\n",
      "Iteration [2829], Loss: 0.2595, Error: 0.0781\n",
      "Iteration [2830], Loss: 0.2619, Error: 0.0625\n",
      "Iteration [2831], Loss: 0.3000, Error: 0.0625\n",
      "Iteration [2832], Loss: 0.2886, Error: 0.0625\n",
      "Iteration [2833], Loss: 0.4771, Error: 0.1562\n",
      "Iteration [2834], Loss: 0.3370, Error: 0.0625\n",
      "Iteration [2835], Loss: 0.3828, Error: 0.1094\n",
      "Iteration [2836], Loss: 0.2771, Error: 0.0938\n",
      "Iteration [2837], Loss: 0.4159, Error: 0.1094\n",
      "Iteration [2838], Loss: 0.2100, Error: 0.0312\n",
      "Iteration [2839], Loss: 0.2248, Error: 0.0938\n",
      "Iteration [2840], Loss: 0.3048, Error: 0.0938\n",
      "Iteration [2841], Loss: 0.2285, Error: 0.0625\n",
      "Iteration [2842], Loss: 0.3006, Error: 0.0938\n",
      "Iteration [2843], Loss: 0.2263, Error: 0.0938\n",
      "Iteration [2844], Loss: 0.1839, Error: 0.0312\n",
      "Iteration [2845], Loss: 0.2467, Error: 0.0625\n",
      "Iteration [2846], Loss: 0.1727, Error: 0.0469\n",
      "Iteration [2847], Loss: 0.2733, Error: 0.0781\n",
      "Iteration [2848], Loss: 0.2700, Error: 0.0938\n",
      "Iteration [2849], Loss: 0.2869, Error: 0.1094\n",
      "Iteration [2850], Loss: 0.4689, Error: 0.0938\n",
      "Iteration [2851], Loss: 0.2420, Error: 0.1094\n",
      "Iteration [2852], Loss: 0.3570, Error: 0.0938\n",
      "Iteration [2853], Loss: 0.5466, Error: 0.1250\n",
      "Iteration [2854], Loss: 0.5136, Error: 0.1719\n",
      "Iteration [2855], Loss: 0.5022, Error: 0.1719\n",
      "Iteration [2856], Loss: 0.1592, Error: 0.0625\n",
      "Iteration [2857], Loss: 0.2381, Error: 0.0625\n",
      "Iteration [2858], Loss: 0.4149, Error: 0.1250\n",
      "Iteration [2859], Loss: 0.4008, Error: 0.1250\n",
      "Iteration [2860], Loss: 0.2634, Error: 0.0938\n",
      "Iteration [2861], Loss: 0.2282, Error: 0.0469\n",
      "Iteration [2862], Loss: 0.3209, Error: 0.1250\n",
      "Iteration [2863], Loss: 0.4343, Error: 0.1250\n",
      "Iteration [2864], Loss: 0.3899, Error: 0.0938\n",
      "Iteration [2865], Loss: 0.4083, Error: 0.0938\n",
      "Iteration [2866], Loss: 0.2244, Error: 0.0625\n",
      "Iteration [2867], Loss: 0.4010, Error: 0.1250\n",
      "Iteration [2868], Loss: 0.2878, Error: 0.1094\n",
      "Iteration [2869], Loss: 0.3855, Error: 0.1094\n",
      "Iteration [2870], Loss: 0.2272, Error: 0.0625\n",
      "Iteration [2871], Loss: 0.1790, Error: 0.0625\n",
      "Iteration [2872], Loss: 0.2595, Error: 0.0781\n",
      "Iteration [2873], Loss: 0.4058, Error: 0.0781\n",
      "Iteration [2874], Loss: 0.1739, Error: 0.0469\n",
      "Iteration [2875], Loss: 0.3897, Error: 0.1250\n",
      "Iteration [2876], Loss: 0.2815, Error: 0.0781\n",
      "Iteration [2877], Loss: 0.3768, Error: 0.1250\n",
      "Iteration [2878], Loss: 0.2919, Error: 0.0625\n",
      "Iteration [2879], Loss: 0.3925, Error: 0.1562\n",
      "Iteration [2880], Loss: 0.4458, Error: 0.1719\n",
      "Iteration [2881], Loss: 0.4099, Error: 0.1250\n",
      "Iteration [2882], Loss: 0.2932, Error: 0.0938\n",
      "Iteration [2883], Loss: 0.4056, Error: 0.0938\n",
      "Iteration [2884], Loss: 0.3265, Error: 0.0469\n",
      "Iteration [2885], Loss: 0.2428, Error: 0.0625\n",
      "Iteration [2886], Loss: 0.3090, Error: 0.0781\n",
      "Iteration [2887], Loss: 0.2815, Error: 0.0781\n",
      "Iteration [2888], Loss: 0.3362, Error: 0.0938\n",
      "Iteration [2889], Loss: 0.4374, Error: 0.0938\n",
      "Iteration [2890], Loss: 0.2373, Error: 0.0625\n",
      "Iteration [2891], Loss: 0.0883, Error: 0.0156\n",
      "Iteration [2892], Loss: 0.1780, Error: 0.0156\n",
      "Iteration [2893], Loss: 0.2014, Error: 0.0625\n",
      "Iteration [2894], Loss: 0.2373, Error: 0.0469\n",
      "Iteration [2895], Loss: 0.6921, Error: 0.1562\n",
      "Iteration [2896], Loss: 0.4444, Error: 0.1250\n",
      "Iteration [2897], Loss: 0.3047, Error: 0.0781\n",
      "Iteration [2898], Loss: 0.2285, Error: 0.0312\n",
      "Iteration [2899], Loss: 0.4183, Error: 0.0938\n",
      "Iteration [2900], Loss: 0.3181, Error: 0.0625\n",
      "Iteration [2901], Loss: 0.3510, Error: 0.1250\n",
      "Iteration [2902], Loss: 0.2553, Error: 0.0625\n",
      "Iteration [2903], Loss: 0.3462, Error: 0.0781\n",
      "Iteration [2904], Loss: 0.2658, Error: 0.0938\n",
      "Iteration [2905], Loss: 0.2108, Error: 0.0469\n",
      "Iteration [2906], Loss: 0.4138, Error: 0.0938\n",
      "Iteration [2907], Loss: 0.2972, Error: 0.0781\n",
      "Iteration [2908], Loss: 0.3014, Error: 0.0938\n",
      "Iteration [2909], Loss: 0.4090, Error: 0.0781\n",
      "Iteration [2910], Loss: 0.3356, Error: 0.1094\n",
      "Iteration [2911], Loss: 0.2172, Error: 0.0781\n",
      "Iteration [2912], Loss: 0.2688, Error: 0.0781\n",
      "Iteration [2913], Loss: 0.2614, Error: 0.1250\n",
      "Iteration [2914], Loss: 0.3886, Error: 0.1406\n",
      "Iteration [2915], Loss: 0.1909, Error: 0.0156\n",
      "Iteration [2916], Loss: 0.2309, Error: 0.0625\n",
      "Iteration [2917], Loss: 0.2350, Error: 0.0781\n",
      "Iteration [2918], Loss: 0.3331, Error: 0.0938\n",
      "Iteration [2919], Loss: 0.3141, Error: 0.0312\n",
      "Iteration [2920], Loss: 0.2442, Error: 0.0469\n",
      "Iteration [2921], Loss: 0.2653, Error: 0.0625\n",
      "Iteration [2922], Loss: 0.1682, Error: 0.0469\n",
      "Iteration [2923], Loss: 0.2066, Error: 0.0625\n",
      "Iteration [2924], Loss: 0.3345, Error: 0.0469\n",
      "Iteration [2925], Loss: 0.2573, Error: 0.0781\n",
      "Iteration [2926], Loss: 0.2569, Error: 0.0625\n",
      "Iteration [2927], Loss: 0.4196, Error: 0.0781\n",
      "Iteration [2928], Loss: 0.2156, Error: 0.0469\n",
      "Iteration [2929], Loss: 0.4048, Error: 0.1406\n",
      "Iteration [2930], Loss: 0.2399, Error: 0.0625\n",
      "Iteration [2931], Loss: 0.2105, Error: 0.0312\n",
      "Iteration [2932], Loss: 0.2965, Error: 0.0938\n",
      "Iteration [2933], Loss: 0.2043, Error: 0.0625\n",
      "Iteration [2934], Loss: 0.1944, Error: 0.0312\n",
      "Iteration [2935], Loss: 0.1928, Error: 0.0469\n",
      "Iteration [2936], Loss: 0.2925, Error: 0.0469\n",
      "Iteration [2937], Loss: 0.2699, Error: 0.0781\n",
      "Iteration [2938], Loss: 0.4147, Error: 0.0938\n",
      "Iteration [2939], Loss: 0.2729, Error: 0.0781\n",
      "Iteration [2940], Loss: 0.2474, Error: 0.0781\n",
      "Iteration [2941], Loss: 0.2275, Error: 0.0469\n",
      "Iteration [2942], Loss: 0.1967, Error: 0.0625\n",
      "Iteration [2943], Loss: 0.2518, Error: 0.1094\n",
      "Iteration [2944], Loss: 0.2597, Error: 0.1094\n",
      "Iteration [2945], Loss: 0.2261, Error: 0.0625\n",
      "Iteration [2946], Loss: 0.2148, Error: 0.0625\n",
      "Iteration [2947], Loss: 0.2232, Error: 0.0625\n",
      "Iteration [2948], Loss: 0.2599, Error: 0.0625\n",
      "Iteration [2949], Loss: 0.2449, Error: 0.0781\n",
      "Iteration [2950], Loss: 0.4734, Error: 0.1562\n",
      "Iteration [2951], Loss: 0.1387, Error: 0.0312\n",
      "Iteration [2952], Loss: 0.2883, Error: 0.0625\n",
      "Iteration [2953], Loss: 0.4482, Error: 0.1562\n",
      "Iteration [2954], Loss: 0.2322, Error: 0.0781\n",
      "Iteration [2955], Loss: 0.4008, Error: 0.1094\n",
      "Iteration [2956], Loss: 0.3128, Error: 0.0781\n",
      "Iteration [2957], Loss: 0.2772, Error: 0.0938\n",
      "Iteration [2958], Loss: 0.2624, Error: 0.0625\n",
      "Iteration [2959], Loss: 0.4969, Error: 0.1250\n",
      "Iteration [2960], Loss: 0.2061, Error: 0.0625\n",
      "Iteration [2961], Loss: 0.3358, Error: 0.0938\n",
      "Iteration [2962], Loss: 0.1281, Error: 0.0156\n",
      "Iteration [2963], Loss: 0.3958, Error: 0.0938\n",
      "Iteration [2964], Loss: 0.3477, Error: 0.0938\n",
      "Iteration [2965], Loss: 0.3875, Error: 0.1250\n",
      "Iteration [2966], Loss: 0.2918, Error: 0.0469\n",
      "Iteration [2967], Loss: 0.3126, Error: 0.0938\n",
      "Iteration [2968], Loss: 0.4151, Error: 0.2031\n",
      "Iteration [2969], Loss: 0.2589, Error: 0.0781\n",
      "Iteration [2970], Loss: 0.1598, Error: 0.0469\n",
      "Iteration [2971], Loss: 0.2810, Error: 0.0938\n",
      "Iteration [2972], Loss: 0.2694, Error: 0.1094\n",
      "Iteration [2973], Loss: 0.3329, Error: 0.0938\n",
      "Iteration [2974], Loss: 0.2355, Error: 0.0469\n",
      "Iteration [2975], Loss: 0.4816, Error: 0.1094\n",
      "Iteration [2976], Loss: 0.2140, Error: 0.0781\n",
      "Iteration [2977], Loss: 0.3351, Error: 0.1094\n",
      "Iteration [2978], Loss: 0.1922, Error: 0.0625\n",
      "Iteration [2979], Loss: 0.1705, Error: 0.0469\n",
      "Iteration [2980], Loss: 0.2778, Error: 0.0938\n",
      "Iteration [2981], Loss: 0.1255, Error: 0.0312\n",
      "Iteration [2982], Loss: 0.3192, Error: 0.0625\n",
      "Iteration [2983], Loss: 0.3751, Error: 0.1250\n",
      "Iteration [2984], Loss: 0.2321, Error: 0.0625\n",
      "Iteration [2985], Loss: 0.3473, Error: 0.1094\n",
      "Iteration [2986], Loss: 0.3845, Error: 0.1250\n",
      "Iteration [2987], Loss: 0.2668, Error: 0.0781\n",
      "Iteration [2988], Loss: 0.4130, Error: 0.1250\n",
      "Iteration [2989], Loss: 0.3303, Error: 0.0625\n",
      "Iteration [2990], Loss: 0.2515, Error: 0.0469\n",
      "Iteration [2991], Loss: 0.2917, Error: 0.0938\n",
      "Iteration [2992], Loss: 0.5599, Error: 0.2188\n",
      "Iteration [2993], Loss: 0.3884, Error: 0.1406\n",
      "Iteration [2994], Loss: 0.2618, Error: 0.0625\n",
      "Iteration [2995], Loss: 0.3470, Error: 0.0938\n",
      "Iteration [2996], Loss: 0.2824, Error: 0.0469\n",
      "Iteration [2997], Loss: 0.2095, Error: 0.0625\n",
      "Iteration [2998], Loss: 0.5234, Error: 0.1406\n",
      "Iteration [2999], Loss: 0.2925, Error: 0.0625\n",
      "Iteration [3000], Loss: 0.2179, Error: 0.0625\n",
      "Iteration [3001], Loss: 0.2530, Error: 0.0938\n",
      "Iteration [3002], Loss: 0.4361, Error: 0.1094\n",
      "Iteration [3003], Loss: 0.2603, Error: 0.0312\n",
      "Iteration [3004], Loss: 0.2470, Error: 0.0781\n",
      "Iteration [3005], Loss: 0.3465, Error: 0.0938\n",
      "Iteration [3006], Loss: 0.3094, Error: 0.1094\n",
      "Iteration [3007], Loss: 0.2125, Error: 0.0781\n",
      "Iteration [3008], Loss: 0.3970, Error: 0.1094\n",
      "Iteration [3009], Loss: 0.2129, Error: 0.0625\n",
      "Iteration [3010], Loss: 0.2697, Error: 0.0469\n",
      "Iteration [3011], Loss: 0.3661, Error: 0.0938\n",
      "Iteration [3012], Loss: 0.1582, Error: 0.0469\n",
      "Iteration [3013], Loss: 0.4680, Error: 0.1406\n",
      "Iteration [3014], Loss: 0.2098, Error: 0.0312\n",
      "Iteration [3015], Loss: 0.1661, Error: 0.0312\n",
      "Iteration [3016], Loss: 0.2425, Error: 0.0625\n",
      "Iteration [3017], Loss: 0.3069, Error: 0.0625\n",
      "Iteration [3018], Loss: 0.4092, Error: 0.1406\n",
      "Iteration [3019], Loss: 0.2890, Error: 0.0781\n",
      "Iteration [3020], Loss: 0.2156, Error: 0.0625\n",
      "Iteration [3021], Loss: 0.3425, Error: 0.0625\n",
      "Iteration [3022], Loss: 0.4012, Error: 0.1250\n",
      "Iteration [3023], Loss: 0.2637, Error: 0.0625\n",
      "Iteration [3024], Loss: 0.2148, Error: 0.0781\n",
      "Iteration [3025], Loss: 0.4845, Error: 0.0938\n",
      "Iteration [3026], Loss: 0.1224, Error: 0.0156\n",
      "Iteration [3027], Loss: 0.2496, Error: 0.0938\n",
      "Iteration [3028], Loss: 0.2395, Error: 0.0781\n",
      "Iteration [3029], Loss: 0.2541, Error: 0.0781\n",
      "Iteration [3030], Loss: 0.2420, Error: 0.0938\n",
      "Iteration [3031], Loss: 0.1162, Error: 0.0156\n",
      "Iteration [3032], Loss: 0.5659, Error: 0.1406\n",
      "Iteration [3033], Loss: 0.2838, Error: 0.1094\n",
      "Iteration [3034], Loss: 0.3060, Error: 0.1406\n",
      "Iteration [3035], Loss: 0.2168, Error: 0.0625\n",
      "Iteration [3036], Loss: 0.3925, Error: 0.1250\n",
      "Iteration [3037], Loss: 0.5282, Error: 0.1562\n",
      "Iteration [3038], Loss: 0.3097, Error: 0.1250\n",
      "Iteration [3039], Loss: 0.3193, Error: 0.0625\n",
      "Iteration [3040], Loss: 0.5325, Error: 0.1250\n",
      "Iteration [3041], Loss: 0.1815, Error: 0.0625\n",
      "Iteration [3042], Loss: 0.1850, Error: 0.0312\n",
      "Iteration [3043], Loss: 0.3184, Error: 0.0938\n",
      "Iteration [3044], Loss: 0.2312, Error: 0.0312\n",
      "Iteration [3045], Loss: 0.3109, Error: 0.1719\n",
      "Iteration [3046], Loss: 0.3149, Error: 0.0938\n",
      "Iteration [3047], Loss: 0.3835, Error: 0.0938\n",
      "Iteration [3048], Loss: 0.2543, Error: 0.0625\n",
      "Iteration [3049], Loss: 0.2634, Error: 0.0781\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss_list, train_error_list, val_loss_list, val_error_list = train_pytorch_nn(net, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68a94b20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f0df4ff5c355881b80525571747654b",
     "grade": true,
     "grade_id": "cell-7ed268bdb3a5cc92",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test_train_function is running correctly\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "\n",
    "def test_train_function_runs(train_loss_list, train_error_list, val_loss_list, val_error_list):\n",
    "    \n",
    "    assert len(train_loss_list) > 0, \"Train loss list is empty!\"\n",
    "    assert len(train_error_list) > 0, \"Train error list is empty!\"\n",
    "    assert len(val_loss_list) > 0, \"Validation loss list is empty!\"\n",
    "    assert len(val_error_list) > 0, \"Validation error list is empty!\"\n",
    "    \n",
    "test_train_function_runs(train_loss_list, train_error_list, val_loss_list, val_error_list)\n",
    "print(\"The test_train_function is running correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1af6f42a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2daf4bfb9f2606f7984ceca9b795bccb",
     "grade": true,
     "grade_id": "cell-6b934ef854bca710",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training and validation loss decreases with learning\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "def test_training_loss_decrease(train_loss_list):\n",
    "    \n",
    "    assert train_loss_list[0] > train_loss_list[-1], (\n",
    "        f\"Expected training loss to decrease, but got initial loss {train_loss_list[0]} and final loss {train_loss_list[-1]}\"\n",
    "    )\n",
    "test_training_loss_decrease(train_loss_list)\n",
    "print(\"The training and validation loss decreases with learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c01cc5f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ad26c3fd22a3c61c7ee3aa2a3152a82",
     "grade": true,
     "grade_id": "cell-a0dee2247a2d58e1",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test validation loss is non-negative\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "def test_validation_loss_non_negative(net, val_dataloader):\n",
    "\n",
    "    avg_val_loss, _ = validate_pytorch_NN(net, val_dataloader)\n",
    "    \n",
    "    assert avg_val_loss >= 0, f\"Expected non-negative validation loss, but got {avg_val_loss}\"\n",
    "    \n",
    "test_validation_loss_non_negative(net, val_dataloader)\n",
    "print(\"The Test validation loss is non-negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1125cb95",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6599d26408e670e7acc03371b0c7b239",
     "grade": false,
     "grade_id": "cell-e09fb6fcf54cecbc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def plot_losses(train_loss_list, val_loss_list, val_interval=1000):\n",
    "    \"\"\"\n",
    "    Plot separate training and validation losses.\n",
    "\n",
    "    train_loss_list: List of training losses\n",
    "    val_loss_list: List of validation losses\n",
    "    val_interval: Interval of iterations when validation is calculated\n",
    "    \"\"\"\n",
    "    val_iters = [i for i in range(0, len(train_loss_list), val_interval)]\n",
    "\n",
    "    # Plot Training Loss\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(len(train_loss_list)), train_loss_list)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Training Loss vs Iterations')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    # Plot Validation Loss\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(val_iters, val_loss_list, marker='o')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.title('Validation Loss vs Iterations')\n",
    "    plt.show()\n",
    "\n",
    "# # Example usage (assuming you have train_loss_list and val_loss_list):\n",
    "# train_loss_list, train_error_list, val_loss_list, val_error_list = train_pytorch_nn(net, train_dataloader, val_dataloader)\n",
    "\n",
    "# Plot the losses\n",
    "plot_losses(train_loss_list, val_loss_list, val_interval=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e3e868",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24171868daeaf1d09d66181a960c7a67",
     "grade": false,
     "grade_id": "cell-88735cc12433b8a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Final Takeaways\n",
    "Congratulations on finishing up the assignment. Here are some final points:\n",
    "\n",
    "1. The graph we plotted at the very end shows that the training loss and validation loss both decrease with iterations performed. This is the model learning itself. \n",
    "\n",
    "2. We have not discussed many details about the neural network developed in this assignment. It is a simple linear single layer network used for learning purposes. However, how to construct a neural network to improve its performance is a whole topic in itself. \n",
    "\n",
    "3. You may have noticed that we used 3050 iterations for training. We are performing the validation step after every 1000 iterations. To clearly show three steps of validation learning, we used a value slightly above 3000. \n",
    "\n",
    "4. We generally train models 10 times more than what we did in this assignment. However, we have limited the iterations as a trade off for faster training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcf0e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
