{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc9489b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b599cdc4d67ef286c3a84c9d82faf39",
     "grade": false,
     "grade_id": "cell-assignment-1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 2: Neural Network in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2895bf9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1fe0b9ca78c693c567e6cb1a3761d1f1",
     "grade": false,
     "grade_id": "cell-169290abf3c740f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 1 Background and Instructions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1631246",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0199e21f9387e8ce9245411436bc3e8c",
     "grade": false,
     "grade_id": "cell-6764e64724eee683",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Background\n",
    "\n",
    "Neural Networks are a class of algorithms, loosely modeled after human biological neural networks in the brain. They are instrumental in extracting patterns out of the dataset provided. Most of the present day advancements in Artificial Intelligence are made possible by the different variants of neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612a244",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d7add9c8086df49fda9e34d3010da51",
     "grade": false,
     "grade_id": "cell-4a662a8e4dabeb68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment\n",
    "In this assignment, we will develop a Neural Network and write the entire training loop of that Neural Network using a library called Pytorch. Pytorch, released by Meta (also known as Facebook) is perhaps today the most popular package used to develop a neural network. We are going to implement the backpropagation algorithm in Pytorch. Backpropagation was first postulated roughly 40 years ago by Geoffrey Hinton - one of the authority figures in Deep Learning today. He was awarded the Turing Prize for his contributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8849b0c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f5870e379ee42df13811d794e0678e4",
     "grade": false,
     "grade_id": "cell-6cde643839b5d137",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Instructions\n",
    "\n",
    "Complete the starter code methods provided without changing their signatures or return values. \n",
    "\n",
    "### Note:\n",
    "You are expected to write code where you see **your code here**.  \n",
    "Make sure you delete the lines with **raise NotImplementedError** or your code may not run correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4f461",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f71de58398884072f936a4574d95055e",
     "grade": false,
     "grade_id": "cell-212345b5e873db74",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Import Libraries\n",
    "\n",
    "The code cell below contains the basic python packages you will need to complete this assignment. Do not update\n",
    "the python package or add new ones. Doing so will may cause the autograder to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455da6d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3599aaa94ad1331206d5448dd65ef310",
     "grade": false,
     "grade_id": "cell-0ec972f6dbfa4bf8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Basic libraries for functionalities\n",
    "import numpy as np              #numpy library stands for numerical python - they create a datatype called arrays which speeds up calculation\n",
    "import matplotlib.pyplot as plt #matplotlib is a library used for drawing graphs and plotting\n",
    "import random                   #used to create random datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f177f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e7f511f308a5cf92d0bcecb33a29342",
     "grade": false,
     "grade_id": "cell-3d3b59daba42ad30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## About Pytorch\n",
    "Building a Deep Learning model is a customizable process; for example, someone may want to build a Deep Learning model to extract large features from an image, whereas someone else may only want to extract small features from it. In order to address this, packages were developed for creating Deep Learning models, such as tensorflow, keras and Pytorch. Pytorch is the perhaps the package most widely used by developers. This framework provides Deep Learning model building blocks that developers can assemble for their needs.\n",
    "\n",
    "## PyTorch Blitz\n",
    "Here is a quick introduction to PyTorch(60 minutes) provided by the developers of PyTorch itself. Please watch it if you are new to PyTorch. \n",
    "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b77ec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25671756383fc625dfc989272f2514f6",
     "grade": false,
     "grade_id": "cell-3db39efe970ac68d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#More libraries for implementing our neural network\n",
    "import torch                                        #pytorch library \n",
    "from torch.utils.data import DataLoader, Dataset    #importing the dataset and dotaloader class which we will use to input the data\n",
    "import torch.optim as optim                         #optimizer for pytorch\n",
    "import torch.nn as nn                               #neural network module to write the network architecture\n",
    "import torch.nn.functional as F                     #neural network module to write the network architecture\n",
    "import torchvision as thv                           #torch vision to work on images in the dataset\n",
    "from torchvision.datasets import MNIST              #library to import/download the MNIST datset\n",
    "import torchvision.transforms as T                  #torchvision module to do data transformation\n",
    "from copy import deepcopy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce22428",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5489b6b2d7ee72d11a0ccaaf7db10a3e",
     "grade": false,
     "grade_id": "cell-73023a2099b7e873",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 2: Data and it's Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1c970c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef5491e37fa182a1eca26b2e48b2cb62",
     "grade": false,
     "grade_id": "cell-b15342dfc6cde03c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Get Data \n",
    "\n",
    "We provide the dataset in the local MNIST directory. Run the following function `get_data()`to return the training and validation datasets as `train` and `val`. The training images and labels are coupled together and will be called `train` in the `get_data()` method. In the same way, the validation (testing) images and labels are coupled together and will be called `val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a68fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \n",
    "    \"\"\"\n",
    "    return - the training and validation datasets along with their lables. (output type: torchvision dataset)\n",
    "    \"\"\"\n",
    "    train = MNIST('./', download=False, train=True)\n",
    "    val = MNIST('./', download=False, train=False)\n",
    "\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b77dd19",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b9aba3a480b6f045af36358b5b5a286",
     "grade": false,
     "grade_id": "cell-0baaa2712d9270e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mnist_train, mnist_val = get_data()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6225efc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c1a56b1908a76aec0771c5e788f2039",
     "grade": false,
     "grade_id": "cell-37f23030b4333e16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Subsampling of Dataset\n",
    "The training dataset contains 60,000 images, and we will select only 30,000 of them. The testing dataset contains 10,000 images, and we will select 5,000 of them. The function `subsample` takes the input of how many samples we want. \n",
    "\n",
    "Along with subsampling, we will also normalize our dataset by dividing the feature data by 255 (maximum value) because it provides stability while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af17ac65",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5552694dfa738aca89689fd04dc6cd48",
     "grade": false,
     "grade_id": "cell-349ef7c33dcaa008",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def subsample(X, Y, num_samples, num_classes):\n",
    "    \"\"\"\n",
    "    X: Feature set, as a 2D numpy array\n",
    "    Y: Labels, as a 1D numpy array\n",
    "    num_samples: Total number of samples required after subsampling\n",
    "    num_classes: Number of unique classes in the dataset\n",
    "    \n",
    "    Return: subsampled_X, subsampled_Y subsampled dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    ### Uncomment each line of code and complete as needed\n",
    "\n",
    "    # Flatten X and change type\n",
    "    # X = X._________(X._________[0], -1).astype(np.float32)                         #use the reshape and the shape function to flatten X data          \n",
    "    \n",
    "    # Normalize X\n",
    "    # X = X / ________                                                               #Normalize with the value of 255\n",
    "    # Calculate the final size of each class\n",
    "    # class_size = num_samples // __________                                         #Each class size is given by dividing the total samples by the number of classes\n",
    "    # indices = []\n",
    "\n",
    "    # for label in range(num_classes):  \n",
    "    #     label_indices = np.argwhere(Y == _______)[:class_size].flatten()           # Find indices for the given label and select the first class_size elements           \n",
    "    #     indices.extend(________)                                                   # Keep on extending the list based upon label indices\n",
    "    # indices = np.array(indices)\n",
    "    # subsampled_X = X[_________]                                                    # Get the subsampled values\n",
    "    # subsampled_Y = Y[_________]                                                    # Get the subsampled values\n",
    "\n",
    "    # return subsampled_X, subsampled_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b7bd0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a51c2e7e07c142e5bc7c5c99e404e4e",
     "grade": false,
     "grade_id": "cell-9044c3985e8af55d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Calling the subsample two times \n",
    "Notice that we have called the `subsample` function twice, once to create the train dataset and once to create the validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209216cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f210a9d4be6d4f76d2591f59b43dd195",
     "grade": false,
     "grade_id": "cell-7522d4207e3a4d0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "trainX, trainY = subsample(X=mnist_train.data.numpy(), Y=mnist_train.targets.numpy(), num_samples=30000, num_classes=10)\n",
    "valX, valY = subsample(X=mnist_val.data.numpy(), Y=mnist_val.targets.numpy(), num_samples=5000, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b916efa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61d8614a257fce122c6cd55c4f892a93",
     "grade": true,
     "grade_id": "cell-e810e00d59b4594d",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "\n",
    "def test_subsample_output_shape(X, Y, num_samples, num_classes):\n",
    "\n",
    "    subsampled_X, subsampled_Y = subsample(X, Y, num_samples, num_classes)\n",
    "\n",
    "    assert subsampled_X.shape[0] == num_samples, f\"Expected {num_samples} samples, but got {subsampled_X.shape[0]}\"\n",
    "    assert len(subsampled_Y) == num_samples, f\"Expected {num_samples} labels, but got {len(subsampled_Y)}\"\n",
    "    \n",
    "test_subsample_output_shape(X=mnist_train.data.numpy(), Y=mnist_train.targets.numpy(), num_samples=30000, num_classes=10)\n",
    "print(\"Output Shape Test Passed for training data\")\n",
    "\n",
    "test_subsample_output_shape(X=mnist_val.data.numpy(), Y=mnist_val.targets.numpy(), num_samples=5000, num_classes=10)\n",
    "print(\"Output Shape Test Passed for Validation data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130545e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94a7d81200515299a7350bc2e7c2c518",
     "grade": true,
     "grade_id": "cell-17d2e9d8c63ab8fc",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "def test_subsample_class_distribution(X, Y, num_samples, num_classes):\n",
    "\n",
    "    subsampled_X, subsampled_Y = subsample(X, Y, num_samples, num_classes)\n",
    "    \n",
    "    unique, counts = np.unique(subsampled_Y, return_counts=True)\n",
    "    expected_class_size = num_samples // num_classes\n",
    "\n",
    "    for i, label in enumerate(range(num_classes)):\n",
    "        assert counts[i] == expected_class_size, (\n",
    "            f\"Expected {expected_class_size} samples for class {label}, \"\n",
    "            f\"but got {counts[i]}\")\n",
    "        \n",
    "test_subsample_output_shape(X=mnist_train.data.numpy(), Y=mnist_train.targets.numpy(), num_samples=30000, num_classes=10)\n",
    "print(\"Subsample Class Distribution Passed for training data\")\n",
    "\n",
    "test_subsample_output_shape(X=mnist_val.data.numpy(), Y=mnist_val.targets.numpy(), num_samples=5000, num_classes=10)\n",
    "print(\"Subsample Class Distribution Passed for Validation data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f7e92",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "551b9161ec4b4f2860b68a01d372e41c",
     "grade": true,
     "grade_id": "cell-2a08077e310bfdc1",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "def test_subsample_normalization(X, Y, num_samples, num_classes):\n",
    "\n",
    "    subsampled_X, _ = subsample(X, Y, num_samples, num_classes)\n",
    "    \n",
    "    assert np.all(subsampled_X <= 1.0) and np.all(subsampled_X >= 0.0), (\n",
    "        \"Normalization check failed: Features are not in the range [0, 1]\")\n",
    "    \n",
    "test_subsample_output_shape(X=mnist_train.data.numpy(), Y=mnist_train.targets.numpy(), num_samples=30000, num_classes=10)\n",
    "print(\"Normalization test is passed for training data\")\n",
    "\n",
    "test_subsample_output_shape(X=mnist_val.data.numpy(), Y=mnist_val.targets.numpy(), num_samples=5000, num_classes=10)\n",
    "print(\"Normalizaiton test passed for testing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e823a70c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2dd6584df718d96ba52e447ec4544ae",
     "grade": false,
     "grade_id": "cell-4b046a7ceec35bcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## About MNIST dataset\n",
    "In this assignment, we are going to work with the MNIST digit dataset. It’s a classic dataset with images of handwritten digits from 0 to 9. It's great for testing and teaching image recognition. Each image is labeled with the correct digit, so it’s a great starting point for experimenting with how well models can recognize and classify numbers. Here we are going to apply this on the neural network we are going to develop using PyTorch. \n",
    "\n",
    "We will create a custom dataset using the `Dataset` class in PyTorch. It helps us create the dataset which can later be used with the the `DataLoader` class of PyTorch to train and test our models. In Machine Learning (ML) and Deep Learning (DL), the majority of the time is spent working on preparing the data to be used by the model. That involves cleaning the dataset, organizing it, removing missing items, performing some transforms and many other activities. In order to make that job simpler, PyTorch introduced the Dataset class for our purpose - which helps us in creating and transforming our dataset in a customizable way. After the dataset has been created, the next task is to provide the model with small batches of data from the dataset - here in comes the use of DataLoader class which helps us in doing this action quite effectively. \n",
    "\n",
    "These elements are introduced in the PyTorch Blitz video linked earlier in the notebook; however, for more in-depth reading, see these two tutorials: \n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/basics/data_tutorial.html. \n",
    "\n",
    "* https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4 - In this playlist, see video 09."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169dccd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1c9e6a7420f613d9142c87ee3698f00",
     "grade": false,
     "grade_id": "cell-5036eb0d8be2c50d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "    -----------\n",
    "    x_data : numpy-array of subsampled Image Input\n",
    "    y_data : numpy-array of subsampled Labels Input\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    __len__():Returns the number of samples.\n",
    "\n",
    "    __getitem__(idx):Retrieves the image and label at the given index (Tensor).\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    ### Uncomment each line of code and complete as needed\n",
    "    \n",
    "#     def __init__(self, x_data, y_data):\n",
    "#         self.x_data = ______                                                 # In the constructor define the images and the labels\n",
    "#         self.y_data = ________\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.x_data)                                              #The length of the total dataset\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         x = self.x_data[______]                                              #This would ensure the retrieval of new datapoints  \n",
    "#         y = self.y_data[______]\n",
    "#         return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d17f45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d474260688fd369eda66898fcceba3e",
     "grade": false,
     "grade_id": "cell-1f1ffb83f58b0369",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = MNISTDataset(trainX, trainY)  \n",
    "val_dataset = MNISTDataset(valX, valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf285b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d6261c0eb5a3742e050c55253668992",
     "grade": true,
     "grade_id": "cell-b2a00ba7ccb8eef8",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "\n",
    "def test_length(x_data, y_data, expected_length):\n",
    "    dataset = MNISTDataset(x_data, y_data)\n",
    "\n",
    "    dataset_length = len(dataset)\n",
    "    assert dataset_length == expected_length, f\"Expected dataset length {expected_length}, but got {dataset_length}\"\n",
    "    \n",
    "test_length(x_data= trainX, y_data=trainY, expected_length = 30000 )\n",
    "print(\"The length of the Train dataset is correct\")\n",
    "\n",
    "test_length(x_data= valX, y_data=valY, expected_length = 5000 )\n",
    "print(\"The length of the Val dataset is correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9171d1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b20c2fdf3accc07ed953a4fe86478513",
     "grade": true,
     "grade_id": "cell-52a5c522fc0c934b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "def test_getitem(x_data, y_data):\n",
    "    dataset = MNISTDataset(x_data, y_data)\n",
    "\n",
    "    idx = 1\n",
    "    expected_x = x_data[idx]\n",
    "    expected_y = y_data[idx]\n",
    "    x, y = dataset[idx]\n",
    "\n",
    "    assert np.array_equal(x, expected_x), f\"Expected x at index {idx} to be {expected_x}, but got {x}\"\n",
    "    assert y == expected_y, f\"Expected y at index {idx} to be {expected_y}, but got {y}\"\n",
    "    \n",
    "    \n",
    "test_getitem(x_data = trainX, y_data=trainY)\n",
    "print(\"Data Retrieval of Train dataset is Passed\")\n",
    "test_getitem(x_data = valX, y_data=valY)\n",
    "print(\"Data Retrieval of Val dataset is Passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9de48e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae16d29c837ddb738629b2b5d783bc9c",
     "grade": false,
     "grade_id": "cell-21ed73ac4c4458b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 3 Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b884ccbb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d424000daeeb90791a4fd7c321e6be03",
     "grade": false,
     "grade_id": "cell-8ebe6308c7af7391",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Define Neural Network in Pytorch\n",
    "Neural networks in PyTorch are created using the `torch.nn` module. The `__init__` method specifies which layers are present in our network. You will notice that in the `__init__` method the linear layer has parameters 784 and 10. This is because we are taking our input dataset, composed of MNIST images of the size 28X28, and converting them to a linear vector of 784. The value 10 represents the number of possible outputs of the MNIST dataset. \n",
    "\n",
    "\n",
    "The `forward` method determines how the data will flow through the network. It shows which elements are applied upon the input data `x` and in what order to perform the forward pass of the datset. In the forward method we are applying `ReLU` upon our dataset after the fully connected layer `fc1`. This is the application of the activation function on our data to bring non-linearity into our model. Non-Linearity is at the heart of what makes neural networks capable of extracting features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba25c3e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b006c84b8e19514959b2f1111533b51",
     "grade": false,
     "grade_id": "cell-d04445af0a20d126",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module): \n",
    "    \"\"\"\n",
    "    A neural network with a single fully connected layer.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    fc1 :Fully connected layer with input size 784 (28X28) and output size 10 (number of samples).\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    forward(x):Applies the forward pass using ReLU activation on the fully connected layer.\n",
    "    \n",
    "    Output:\n",
    "    --------\n",
    "    out: output of the application of the simple neural network on the dataset\n",
    "    \"\"\" \n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    ### Uncomment each line of code and get the answer\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(_____, _____)                                       #Define the sizes of the layer of the NN corresponding to the dataset\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = F.______(self.fc1(_____))                                           # write out how the data flows through the model\n",
    "#         return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1b6a0d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df321224f21ed9bbf08851e9bdd1dc5f",
     "grade": false,
     "grade_id": "cell-eafc45687fca3252",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "net = Net() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e249a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa6d5a1d045e80de4688d5bfe0aef2f5",
     "grade": true,
     "grade_id": "cell-5fdc86c5be80b511",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "def test_initialization(net):\n",
    "    assert isinstance(net.fc1, nn.Linear), \"fc1 should be an instance of nn.Linear\"\n",
    "    assert net.fc1.in_features == 784, \"fc1 input features should be 784\"\n",
    "    assert net.fc1.out_features == 10, \"fc1 output features should be 10\"\n",
    "    print(\"Initialization test passed.\")\n",
    "    \n",
    "test_initialization(net)\n",
    "print(\"The Model is correctly initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf3402",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05c260246ad991621819e37342d53007",
     "grade": true,
     "grade_id": "cell-372f27cd8dc6ab68",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "def test_forward_pass(net):\n",
    "    dummy_input = torch.randn(1, 784)\n",
    "    output = net(dummy_input)\n",
    "    assert output.shape == (1, 10), \"Output shape should be (1, 10)\"\n",
    "    assert output.dtype == torch.float32, \"Output dtype should be float32\"\n",
    "    print(\"Forward pass test passed.\")\n",
    "    \n",
    "    \n",
    "test_forward_pass(net)\n",
    "print(\"Forward pass is correctly initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195451af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b390a1a7edcb3c0b2554433823431fd",
     "grade": false,
     "grade_id": "cell-727132540cd416ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 4 Training and Evaluation of the Neural Network\n",
    "After defining our dataset and our model above, we train and evaluate our model. If you look at the code in this section, you will notice that the validation method, `validate_pytorch_NN`, is defined first and then the training loop, `train_pytorch_nn`. This has been done purposefully, because we are calling the validation function from the training loop. A recommendation is that you start coding from the training loop and then circle back to validation loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa77a94",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "668aeacd4edb0e6dd3019635a73141e9",
     "grade": false,
     "grade_id": "cell-d957321df715a3b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Validation of Pytorch Neural Network\n",
    "Here we have written the `validate_pytorch_NN` method. It takes the trained model, implements it on the testing (validation) dataset, calculates the running loss, and finally outputs the average validation loss and average validation error. Here the key metric is the running loss. During the training and the evaluation (validation) of a neural network, it is run iteratively. The performance is tracked at each iteration, and the record keeping of that is called running loss. We will later plot the loss and the accuracy curves of the model. Those curves are characteristic of the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fc99f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae8ed7b269c68cebaf033d5f4670d4e6",
     "grade": false,
     "grade_id": "cell-0ff4deb962733383",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def validate_pytorch_NN(net, val_dataloader):\n",
    "    \"\"\"\n",
    "    Validate the PyTorch neural network with the given validation dataloader.\n",
    "    \n",
    "    net: The neural network model\n",
    "    criterion: The loss function\n",
    "    val_dataloader: DataLoader for the validation data\n",
    "    \n",
    "    Return: average validation loss and error\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    ### Uncomment each line of code and complete as needed\n",
    "\n",
    "    # Initialize total validation loss and error\n",
    "    # val_loss, val_error = 0, 0\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Iterate over all mini-batches from the validation dataset\n",
    "    # for x_val, y_val in val_dataloader:\n",
    "    #     x_val = torch.Tensor(_______)                      # Convert the data to tensor\n",
    "    #     y_val = torch.Tensor(________)                     # Convert the data to tensor\n",
    "    #     \n",
    "    #     # Compute forward pass and error\n",
    "    #     with torch.no_grad():\n",
    "    #         val_out = net(______)                                                 #Calculate the output\n",
    "    #         val_loss += criterion(________, _________).item()                     #Calculate the running loss\n",
    "    #         _, val_pred = torch.max(_______, 1)                                   # Make the prediction\n",
    "    #         val_error += np.sum(np.array(_____.cpu()) != _________.cpu().numpy()) / ______.shape[0]  #Calculate the running error \n",
    "    # \n",
    "    # # Compute average validation loss and error\n",
    "    # avg_val_loss = _______ / len(_______)                                         #Calculate the average val loss and val error\n",
    "    # avg_val_error = __________ / len(_________)\n",
    "    # \n",
    "    # return avg_val_loss, avg_val_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b30d88e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66f543e524fb95b154ff78cd8d74c3e0",
     "grade": false,
     "grade_id": "cell-391b0c7230ed68c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Train Pytorch Neural Network\n",
    "\n",
    "The training loop in PyTorch teaches your model to make better predictions. Start by passing your data through the model to get predictions. Then calculate how far off these predictions are from the actual values using a loss function. Next, update the model’s parameters to reduce this error, and repeat this process many times. Essentially, the training loop helps the model learn from its mistakes and improve over time.\n",
    "\n",
    "### Steps of the Training Loop\n",
    "1. Perform the forward pass on the data where the `model` is applied on the training dataset to generate an ouptut.\n",
    "2. Perform the backpropagation where the gradients are set to zero using `optimizer.zero_grad()` and calculate the backward loss using `loss.backward()`\n",
    "3. Update the model parameters using the `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917fbb2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e86c4f62b7eaf23a606bd70ba84d9e83",
     "grade": false,
     "grade_id": "cell-572bc733a3053ecd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_pytorch_nn(net, train_dataloader, val_dataloader):\n",
    "    \"\"\"\n",
    "    Train a PyTorch neural network with the given parameters.\n",
    "    \n",
    "    train_dataloader: DataLoader for the training data\n",
    "    val_dataloader: DataLoader for the validation data\n",
    "    lr: Learning rate for the optimizer\n",
    "    \n",
    "    Return: Lists of training and validation losses and errors\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    ### Uncomment each line of code and complete as needed\n",
    "\n",
    "    # Setup (e.g., seed initialization, if any) - Do not change these values\n",
    "#     torch.manual_seed(20)\n",
    "#     random.seed(20)\n",
    "#     np.random.seed(20)\n",
    "#     optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#     train_error_list = []\n",
    "#     train_loss_list = []\n",
    "#     val_error_list = []\n",
    "#     val_loss_list = []\n",
    "\n",
    "#     # Calculate the total number of batches needed\n",
    "#     num_iterations = 3050\n",
    "\n",
    "#     iteration = 0\n",
    "\n",
    "#     while iteration < ___________:                                                          #write the loop breaking condition such that we can it performs 30,000 iterations\n",
    "#         for x, y in train_dataloader:\n",
    "#             if iteration >= num_iterations:\n",
    "#                 break\n",
    "\n",
    "#             x = x\n",
    "#             y = y\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = net(____)                                            #network is applied on input data\n",
    "#             loss = criterion(______, _____)                                # running loss is calculated\n",
    "#             _, predicted = torch.max(______, 1)                            # Model class predictions are taken\n",
    "#             err = np.sum(_______.cpu().numpy() != _________.cpu().numpy()) / x.shape[0]   #Running error of the model is calculated\n",
    "\n",
    "#             # Backward pass and optimization\n",
    "#             optimizer._______grad()                                                       # Zero gradients\n",
    "#             loss.___________()                                                            #calculate the loss values\n",
    "#             optimizer.__________()                                                        #take the next optimization step\n",
    "\n",
    "#             # Logging\n",
    "#             print(f\"Iteration [{iteration}], \"\n",
    "#             f\"Loss: {loss.item():.4f}, \"\n",
    "#             f\"Error: {err:.4f}, \")\n",
    "#             train_loss_list.append(________.item())                                       # log the loss value\n",
    "#             train_error_list.append(______)                                               # log the error value\n",
    "\n",
    "#             # Validation step\n",
    "#             if iteration % 1000 == 0:                                                      #validation step at every 500 iteration\n",
    "#                 val_loss, val_err = validate_pytorch_NN(net, val_dataloader) \n",
    "#                 val_loss_list.append(______)                                              # log the validaiton loss value\n",
    "#                 val_error_list.append(__________)                                         # log the validation error value\n",
    "\n",
    "#             iteration += ___                                                              #increment iteration\n",
    "\n",
    "#     return train_loss_list, train_error_list, val_loss_list, val_error_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33ea92c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6af5d983cf765ecf0d7d7cf313b6e0b2",
     "grade": false,
     "grade_id": "cell-d53ca14d3be40490",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = MNISTDataset(trainX, trainY) \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = MNISTDataset(valX, valY)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522b4ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dec6b1b1ecd137e26817b4f6c71253af",
     "grade": false,
     "grade_id": "cell-7db9ffa85c87e271",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loss_list, train_error_list, val_loss_list, val_error_list = train_pytorch_nn(net, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a94b20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f0df4ff5c355881b80525571747654b",
     "grade": true,
     "grade_id": "cell-7ed268bdb3a5cc92",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "\n",
    "def test_train_function_runs(train_loss_list, train_error_list, val_loss_list, val_error_list):\n",
    "    \n",
    "    assert len(train_loss_list) > 0, \"Train loss list is empty!\"\n",
    "    assert len(train_error_list) > 0, \"Train error list is empty!\"\n",
    "    assert len(val_loss_list) > 0, \"Validation loss list is empty!\"\n",
    "    assert len(val_error_list) > 0, \"Validation error list is empty!\"\n",
    "    \n",
    "test_train_function_runs(train_loss_list, train_error_list, val_loss_list, val_error_list)\n",
    "print(\"The test_train_function is running correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6f42a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2daf4bfb9f2606f7984ceca9b795bccb",
     "grade": true,
     "grade_id": "cell-6b934ef854bca710",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "def test_training_loss_decrease(train_loss_list):\n",
    "    \n",
    "    assert train_loss_list[0] > train_loss_list[-1], (\n",
    "        f\"Expected training loss to decrease, but got initial loss {train_loss_list[0]} and final loss {train_loss_list[-1]}\"\n",
    "    )\n",
    "test_training_loss_decrease(train_loss_list)\n",
    "print(\"The training and validation loss decreases with learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c01cc5f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ad26c3fd22a3c61c7ee3aa2a3152a82",
     "grade": true,
     "grade_id": "cell-a0dee2247a2d58e1",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "##### TEST YOUR CODE HERE #####\n",
    "###############################\n",
    "def test_validation_loss_non_negative(net, val_dataloader):\n",
    "\n",
    "    avg_val_loss, _ = validate_pytorch_NN(net, val_dataloader)\n",
    "    \n",
    "    assert avg_val_loss >= 0, f\"Expected non-negative validation loss, but got {avg_val_loss}\"\n",
    "    \n",
    "test_validation_loss_non_negative(net, val_dataloader)\n",
    "print(\"The Test validation loss is non-negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125cb95",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6599d26408e670e7acc03371b0c7b239",
     "grade": false,
     "grade_id": "cell-e09fb6fcf54cecbc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_losses(train_loss_list, val_loss_list, val_interval=1000):\n",
    "    \"\"\"\n",
    "    Plot separate training and validation losses.\n",
    "\n",
    "    train_loss_list: List of training losses\n",
    "    val_loss_list: List of validation losses\n",
    "    val_interval: Interval of iterations when validation is calculated\n",
    "    \"\"\"\n",
    "    val_iters = [i for i in range(0, len(train_loss_list), val_interval)]\n",
    "\n",
    "    # Plot Training Loss\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(len(train_loss_list)), train_loss_list)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Training Loss vs Iterations')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    # Plot Validation Loss\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(val_iters, val_loss_list, marker='o')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.title('Validation Loss vs Iterations')\n",
    "    plt.show()\n",
    "\n",
    "# # Example usage (assuming you have train_loss_list and val_loss_list):\n",
    "# train_loss_list, train_error_list, val_loss_list, val_error_list = train_pytorch_nn(net, train_dataloader, val_dataloader)\n",
    "\n",
    "# Plot the losses\n",
    "plot_losses(train_loss_list, val_loss_list, val_interval=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e3e868",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24171868daeaf1d09d66181a960c7a67",
     "grade": false,
     "grade_id": "cell-88735cc12433b8a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Final Takeaways\n",
    "Congratulations on finishing up the assignment. Here are some final points:\n",
    "\n",
    "1. The graph we plotted at the very end shows that the training loss and validation loss both decrease with iterations performed. This is the model learning itself. \n",
    "\n",
    "2. We have not discussed many details about the neural network developed in this assignment. It is a simple linear single layer network used for learning purposes. However, how to construct a neural network to improve its performance is a whole topic in itself. \n",
    "\n",
    "3. You may have noticed that we used 3050 iterations for training. We are performing the validation step after every 1000 iterations. To clearly show three steps of validation learning, we used a value slightly above 3000. \n",
    "\n",
    "4. We generally train models 10 times more than what we did in this assignment. However, we have limited the iterations as a trade off for faster training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcf0e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
